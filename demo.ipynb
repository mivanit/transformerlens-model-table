{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/miv/projects/tools/transformerlens-model-table/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/miv/projects/tools/transformerlens-model-table/get_model_table.py:44: UserWarning: Failed to get Hugging Face token -- info about certain models will be limited\n",
      "Invalid Hugging Face token\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import transformer_lens\n",
    "from get_model_table import get_model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TransformerLens version: 2.15.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_385709/2128677661.py:2: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
      "  import pkg_resources\n"
     ]
    }
   ],
   "source": [
    "# this notebook is for demo purposes and not automatically re-run -- see the live page for up-to-date version\n",
    "import pkg_resources\n",
    "print('TransformerLens version:', pkg_resources.get_distribution('transformer_lens').version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running in parallel with n_processes = 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model info:  41%|████      | 86/211 [00:17<00:14,  8.48it/s]WARNING:root:llama-7b-hf tokenizer not loaded. Please load manually.\n",
      "Loading model info:  42%|████▏     | 89/211 [00:18<00:12, 10.08it/s]WARNING:root:llama-13b-hf tokenizer not loaded. Please load manually.\n",
      "WARNING:root:llama-30b-hf tokenizer not loaded. Please load manually.\n",
      "Loading model info:  43%|████▎     | 91/211 [00:18<00:14,  8.36it/s]WARNING:root:llama-65b-hf tokenizer not loaded. Please load manually.\n",
      "Loading model info:  66%|██████▋   | 140/211 [00:29<00:11,  6.28it/s]/home/miv/projects/tools/transformerlens-model-table/get_model_table.py:170: UserWarning: Failed to get activation cache for 'Mixtral-8x7B-Instruct-v0.1':\n",
      "The register_meta function for torch.nonzero() raises unimplemented by default, as a correct data-independent implementation does not exist. This implementation returns a fake value, assuming all elements of the tensor are non-zero. To enable this registration, please set 'torch.fx.experimental._config.meta_nonzero_assume_all_nonzero' to True.\n",
      "  warnings.warn(msg)\n",
      "/home/miv/projects/tools/transformerlens-model-table/get_model_table.py:170: UserWarning: Failed to get activation cache for 'Mixtral-8x7B-v0.1':\n",
      "The register_meta function for torch.nonzero() raises unimplemented by default, as a correct data-independent implementation does not exist. This implementation returns a fake value, assuming all elements of the tensor are non-zero. To enable this registration, please set 'torch.fx.experimental._config.meta_nonzero_assume_all_nonzero' to True.\n",
      "  warnings.warn(msg)\n",
      "Loading model info:  69%|██████▉   | 146/211 [00:32<00:20,  3.15it/s]WARNING:root:Loading model bigcode/santacoder requires setting trust_remote_code=True\n",
      "Loading model info:  70%|███████   | 148/211 [00:32<00:17,  3.60it/s]WARNING:root:Loading model Qwen/Qwen-1_8B requires setting trust_remote_code=True\n",
      "Loading model info:  71%|███████   | 149/211 [00:33<00:16,  3.81it/s]WARNING:root:Loading model Qwen/Qwen-7B requires setting trust_remote_code=True\n",
      "WARNING:root:Loading model Qwen/Qwen-14B requires setting trust_remote_code=True\n",
      "WARNING:root:Loading model Qwen/Qwen-1_8B-Chat requires setting trust_remote_code=True\n",
      "Loading model info:  71%|███████   | 150/211 [00:34<00:27,  2.21it/s]WARNING:root:Loading model Qwen/Qwen-7B-Chat requires setting trust_remote_code=True\n",
      "Loading model info:  72%|███████▏  | 151/211 [00:35<00:28,  2.09it/s]WARNING:root:Loading model Qwen/Qwen-14B-Chat requires setting trust_remote_code=True\n",
      "Loading model info:  88%|████████▊ | 186/211 [00:46<00:13,  1.92it/s]WARNING:root:Loading model microsoft/phi-2 requires setting trust_remote_code=True\n",
      "Loading model info:  89%|████████▊ | 187/211 [00:47<00:11,  2.04it/s]WARNING:root:Loading model microsoft/Phi-3-mini-4k-instruct requires setting trust_remote_code=True\n",
      "WARNING:root:Loading model microsoft/phi-4 requires setting trust_remote_code=True\n",
      "Loading model info:  96%|█████████▌| 202/211 [00:54<00:05,  1.68it/s]/home/miv/projects/tools/transformerlens-model-table/get_model_table.py:170: UserWarning: Failed to get activation cache for 't5-small':\n",
      "Invalid positional_embedding_type passed in relative_positional_bias\n",
      "  warnings.warn(msg)\n",
      "/home/miv/projects/tools/transformerlens-model-table/get_model_table.py:170: UserWarning: Failed to get activation cache for 't5-base':\n",
      "Invalid positional_embedding_type passed in relative_positional_bias\n",
      "  warnings.warn(msg)\n",
      "Loading model info:  98%|█████████▊| 207/211 [00:55<00:01,  2.62it/s]/home/miv/projects/tools/transformerlens-model-table/get_model_table.py:170: UserWarning: Failed to get activation cache for 't5-large':\n",
      "Invalid positional_embedding_type passed in relative_positional_bias\n",
      "  warnings.warn(msg)\n",
      "Loading model info: 100%|██████████| 211/211 [00:55<00:00,  3.79it/s]\n"
     ]
    }
   ],
   "source": [
    "TABLE = get_model_table(\n",
    "\tverbose=True,\n",
    "\tforce_reload=True,\n",
    "\tparallelize=4,\n",
    "\tdo_write=True,\n",
    "\tallow_except=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name.default_alias</th>\n",
       "      <th>name.huggingface</th>\n",
       "      <th>name.aliases</th>\n",
       "      <th>name.is_gated</th>\n",
       "      <th>name.model_type</th>\n",
       "      <th>name.from_cfg</th>\n",
       "      <th>n_params.as_str</th>\n",
       "      <th>n_params.as_int</th>\n",
       "      <th>n_params.from_name</th>\n",
       "      <th>cfg.n_params</th>\n",
       "      <th>...</th>\n",
       "      <th>config</th>\n",
       "      <th>tokenizer.name</th>\n",
       "      <th>tokenizer.vocab_size</th>\n",
       "      <th>tokenizer.max_len</th>\n",
       "      <th>tokenizer.class</th>\n",
       "      <th>tokenizer.vocab_hash</th>\n",
       "      <th>tensor_shapes.state_dict</th>\n",
       "      <th>tensor_shapes.state_dict.raw__</th>\n",
       "      <th>tensor_shapes.activation_cache</th>\n",
       "      <th>tensor_shapes.activation_cache.raw__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>non_hf</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>85M</td>\n",
       "      <td>84934656</td>\n",
       "      <td>None</td>\n",
       "      <td>84934656</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 12\\nd_model: 768\\nn_ctx: 1024\\nd_hea...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>50257.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>v8xfIj5kwZX5RwgLU66lZNZUlE4=</td>\n",
       "      <td>embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...</td>\n",
       "      <td>blocks:\\n  '[0-11]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-11]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td></td>\n",
       "      <td>non_hf</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>302M</td>\n",
       "      <td>301989888</td>\n",
       "      <td>None</td>\n",
       "      <td>301989888</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 24\\nd_model: 1024\\nn_ctx: 1024\\nd_he...</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>50257.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>v8xfIj5kwZX5RwgLU66lZNZUlE4=</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1024)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed...</td>\n",
       "      <td>blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-23]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td></td>\n",
       "      <td>non_hf</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>708M</td>\n",
       "      <td>707788800</td>\n",
       "      <td>None</td>\n",
       "      <td>707788800</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 36\\nd_model: 1280\\nn_ctx: 1024\\nd_he...</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>50257.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>v8xfIj5kwZX5RwgLU66lZNZUlE4=</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1280)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 1280)'}, 'pos_embed...</td>\n",
       "      <td>blocks:\\n  '[0-35]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-35]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td></td>\n",
       "      <td>non_hf</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1.5B</td>\n",
       "      <td>1474560000</td>\n",
       "      <td>None</td>\n",
       "      <td>1474560000</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 48\\nd_model: 1600\\nn_ctx: 1024\\nd_he...</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>50257.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>v8xfIj5kwZX5RwgLU66lZNZUlE4=</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1600)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 1600)'}, 'pos_embed...</td>\n",
       "      <td>blocks:\\n  '[0-47]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-47]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distillgpt2</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>distillgpt2, distill-gpt2, distil-gpt2, gpt2-xs</td>\n",
       "      <td>non_hf</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>42M</td>\n",
       "      <td>42467328</td>\n",
       "      <td>None</td>\n",
       "      <td>42467328</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 6\\nd_model: 768\\nn_ctx: 1024\\nd_head...</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>50257.0</td>\n",
       "      <td>1024</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>v8xfIj5kwZX5RwgLU66lZNZUlE4=</td>\n",
       "      <td>embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...</td>\n",
       "      <td>blocks:\\n  '[0-5]':\\n    ln1:\\n      hook_scal...</td>\n",
       "      <td>{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>206</th>\n",
       "      <td>yi-34b-chat</td>\n",
       "      <td>01-ai/Yi-34B-Chat</td>\n",
       "      <td>yi-34b-chat, Yi-34B-Chat</td>\n",
       "      <td>non_hf</td>\n",
       "      <td>yi</td>\n",
       "      <td>Yi-34B-Chat</td>\n",
       "      <td>39B</td>\n",
       "      <td>38755368960</td>\n",
       "      <td>34b</td>\n",
       "      <td>38755368960</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 60\\nd_model: 7168\\nn_ctx: 4096\\nd_he...</td>\n",
       "      <td>01-ai/Yi-34B-Chat</td>\n",
       "      <td>63992.0</td>\n",
       "      <td>4096</td>\n",
       "      <td>LlamaTokenizerFast</td>\n",
       "      <td>VGXAFrTzytwGdUlX6AWH0NacncM=</td>\n",
       "      <td>embed:\\n  W_E: (64000, 7168)\\nblocks:\\n  '[0-5...</td>\n",
       "      <td>{'embed': {'W_E': '(64000, 7168)'}, 'blocks': ...</td>\n",
       "      <td>blocks:\\n  '[0-59]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-59]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>google-t5/t5-small</td>\n",
       "      <td>t5-small</td>\n",
       "      <td>non_hf</td>\n",
       "      <td>t5</td>\n",
       "      <td>t5-small</td>\n",
       "      <td>19M</td>\n",
       "      <td>18874368</td>\n",
       "      <td>None</td>\n",
       "      <td>18874368</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 6\\nd_model: 512\\nn_ctx: 20\\nd_head: ...</td>\n",
       "      <td>google-t5/t5-small</td>\n",
       "      <td>32100.0</td>\n",
       "      <td>512</td>\n",
       "      <td>T5TokenizerFast</td>\n",
       "      <td>jQeywCyCMVL_vza2wKfpuwjNVys=</td>\n",
       "      <td>embed:\\n  W_E: (32128, 512)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(32128, 512)'}, 'pos_embed'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>208</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>google-t5/t5-base</td>\n",
       "      <td>t5-base</td>\n",
       "      <td>non_hf</td>\n",
       "      <td>t5</td>\n",
       "      <td>t5-base</td>\n",
       "      <td>85M</td>\n",
       "      <td>84934656</td>\n",
       "      <td>None</td>\n",
       "      <td>84934656</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 12\\nd_model: 768\\nn_ctx: 20\\nd_head:...</td>\n",
       "      <td>google-t5/t5-base</td>\n",
       "      <td>32100.0</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>T5TokenizerFast</td>\n",
       "      <td>jQeywCyCMVL_vza2wKfpuwjNVys=</td>\n",
       "      <td>embed:\\n  W_E: (32128, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(32128, 768)'}, 'pos_embed'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>209</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>google-t5/t5-large</td>\n",
       "      <td>t5-large</td>\n",
       "      <td>non_hf</td>\n",
       "      <td>t5</td>\n",
       "      <td>t5-large</td>\n",
       "      <td>302M</td>\n",
       "      <td>301989888</td>\n",
       "      <td>None</td>\n",
       "      <td>301989888</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 24\\nd_model: 1024\\nn_ctx: 20\\nd_head...</td>\n",
       "      <td>google-t5/t5-large</td>\n",
       "      <td>32100.0</td>\n",
       "      <td>1000000000000000019884624838656</td>\n",
       "      <td>T5TokenizerFast</td>\n",
       "      <td>jQeywCyCMVL_vza2wKfpuwjNVys=</td>\n",
       "      <td>embed:\\n  W_E: (32128, 1024)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(32128, 1024)'}, 'pos_embed...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>mGPT</td>\n",
       "      <td>ai-forever/mGPT</td>\n",
       "      <td>mGPT</td>\n",
       "      <td>non_hf</td>\n",
       "      <td>None</td>\n",
       "      <td>mGPT</td>\n",
       "      <td>1.2B</td>\n",
       "      <td>1207959552</td>\n",
       "      <td>None</td>\n",
       "      <td>1207959552</td>\n",
       "      <td>...</td>\n",
       "      <td>n_layers: 24\\nd_model: 2048\\nn_ctx: 2048\\nd_he...</td>\n",
       "      <td>ai-forever/mGPT</td>\n",
       "      <td>100000.0</td>\n",
       "      <td>2048</td>\n",
       "      <td>GPT2TokenizerFast</td>\n",
       "      <td>8j6CU_p3zgyeEBZ1Z3lu358tiy0=</td>\n",
       "      <td>embed:\\n  W_E: (100000, 2048)\\npos_embed:\\n  W...</td>\n",
       "      <td>{'embed': {'W_E': '(100000, 2048)'}, 'pos_embe...</td>\n",
       "      <td>blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-23]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>211 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name.default_alias    name.huggingface  \\\n",
       "0           gpt2-small                gpt2   \n",
       "1          gpt2-medium         gpt2-medium   \n",
       "2           gpt2-large          gpt2-large   \n",
       "3              gpt2-xl             gpt2-xl   \n",
       "4          distillgpt2          distilgpt2   \n",
       "..                 ...                 ...   \n",
       "206        yi-34b-chat   01-ai/Yi-34B-Chat   \n",
       "207           t5-small  google-t5/t5-small   \n",
       "208            t5-base   google-t5/t5-base   \n",
       "209           t5-large  google-t5/t5-large   \n",
       "210               mGPT     ai-forever/mGPT   \n",
       "\n",
       "                                        name.aliases name.is_gated  \\\n",
       "0                                         gpt2-small        non_hf   \n",
       "1                                                           non_hf   \n",
       "2                                                           non_hf   \n",
       "3                                                           non_hf   \n",
       "4    distillgpt2, distill-gpt2, distil-gpt2, gpt2-xs        non_hf   \n",
       "..                                               ...           ...   \n",
       "206                         yi-34b-chat, Yi-34B-Chat        non_hf   \n",
       "207                                         t5-small        non_hf   \n",
       "208                                          t5-base        non_hf   \n",
       "209                                         t5-large        non_hf   \n",
       "210                                             mGPT        non_hf   \n",
       "\n",
       "    name.model_type name.from_cfg n_params.as_str  n_params.as_int  \\\n",
       "0              gpt2          gpt2             85M         84934656   \n",
       "1              gpt2   gpt2-medium            302M        301989888   \n",
       "2              gpt2    gpt2-large            708M        707788800   \n",
       "3              gpt2       gpt2-xl            1.5B       1474560000   \n",
       "4              gpt2    distilgpt2             42M         42467328   \n",
       "..              ...           ...             ...              ...   \n",
       "206              yi   Yi-34B-Chat             39B      38755368960   \n",
       "207              t5      t5-small             19M         18874368   \n",
       "208              t5       t5-base             85M         84934656   \n",
       "209              t5      t5-large            302M        301989888   \n",
       "210            None          mGPT            1.2B       1207959552   \n",
       "\n",
       "    n_params.from_name  cfg.n_params  ...  \\\n",
       "0                 None      84934656  ...   \n",
       "1                 None     301989888  ...   \n",
       "2                 None     707788800  ...   \n",
       "3                 None    1474560000  ...   \n",
       "4                 None      42467328  ...   \n",
       "..                 ...           ...  ...   \n",
       "206                34b   38755368960  ...   \n",
       "207               None      18874368  ...   \n",
       "208               None      84934656  ...   \n",
       "209               None     301989888  ...   \n",
       "210               None    1207959552  ...   \n",
       "\n",
       "                                                config      tokenizer.name  \\\n",
       "0    n_layers: 12\\nd_model: 768\\nn_ctx: 1024\\nd_hea...                gpt2   \n",
       "1    n_layers: 24\\nd_model: 1024\\nn_ctx: 1024\\nd_he...         gpt2-medium   \n",
       "2    n_layers: 36\\nd_model: 1280\\nn_ctx: 1024\\nd_he...          gpt2-large   \n",
       "3    n_layers: 48\\nd_model: 1600\\nn_ctx: 1024\\nd_he...             gpt2-xl   \n",
       "4    n_layers: 6\\nd_model: 768\\nn_ctx: 1024\\nd_head...          distilgpt2   \n",
       "..                                                 ...                 ...   \n",
       "206  n_layers: 60\\nd_model: 7168\\nn_ctx: 4096\\nd_he...   01-ai/Yi-34B-Chat   \n",
       "207  n_layers: 6\\nd_model: 512\\nn_ctx: 20\\nd_head: ...  google-t5/t5-small   \n",
       "208  n_layers: 12\\nd_model: 768\\nn_ctx: 20\\nd_head:...   google-t5/t5-base   \n",
       "209  n_layers: 24\\nd_model: 1024\\nn_ctx: 20\\nd_head...  google-t5/t5-large   \n",
       "210  n_layers: 24\\nd_model: 2048\\nn_ctx: 2048\\nd_he...     ai-forever/mGPT   \n",
       "\n",
       "     tokenizer.vocab_size                tokenizer.max_len  \\\n",
       "0                 50257.0                             1024   \n",
       "1                 50257.0                             1024   \n",
       "2                 50257.0                             1024   \n",
       "3                 50257.0                             1024   \n",
       "4                 50257.0                             1024   \n",
       "..                    ...                              ...   \n",
       "206               63992.0                             4096   \n",
       "207               32100.0                              512   \n",
       "208               32100.0  1000000000000000019884624838656   \n",
       "209               32100.0  1000000000000000019884624838656   \n",
       "210              100000.0                             2048   \n",
       "\n",
       "        tokenizer.class          tokenizer.vocab_hash  \\\n",
       "0     GPT2TokenizerFast  v8xfIj5kwZX5RwgLU66lZNZUlE4=   \n",
       "1     GPT2TokenizerFast  v8xfIj5kwZX5RwgLU66lZNZUlE4=   \n",
       "2     GPT2TokenizerFast  v8xfIj5kwZX5RwgLU66lZNZUlE4=   \n",
       "3     GPT2TokenizerFast  v8xfIj5kwZX5RwgLU66lZNZUlE4=   \n",
       "4     GPT2TokenizerFast  v8xfIj5kwZX5RwgLU66lZNZUlE4=   \n",
       "..                  ...                           ...   \n",
       "206  LlamaTokenizerFast  VGXAFrTzytwGdUlX6AWH0NacncM=   \n",
       "207     T5TokenizerFast  jQeywCyCMVL_vza2wKfpuwjNVys=   \n",
       "208     T5TokenizerFast  jQeywCyCMVL_vza2wKfpuwjNVys=   \n",
       "209     T5TokenizerFast  jQeywCyCMVL_vza2wKfpuwjNVys=   \n",
       "210   GPT2TokenizerFast  8j6CU_p3zgyeEBZ1Z3lu358tiy0=   \n",
       "\n",
       "                              tensor_shapes.state_dict  \\\n",
       "0    embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...   \n",
       "1    embed:\\n  W_E: (50257, 1024)\\npos_embed:\\n  W_...   \n",
       "2    embed:\\n  W_E: (50257, 1280)\\npos_embed:\\n  W_...   \n",
       "3    embed:\\n  W_E: (50257, 1600)\\npos_embed:\\n  W_...   \n",
       "4    embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...   \n",
       "..                                                 ...   \n",
       "206  embed:\\n  W_E: (64000, 7168)\\nblocks:\\n  '[0-5...   \n",
       "207  embed:\\n  W_E: (32128, 512)\\npos_embed:\\n  W_p...   \n",
       "208  embed:\\n  W_E: (32128, 768)\\npos_embed:\\n  W_p...   \n",
       "209  embed:\\n  W_E: (32128, 1024)\\npos_embed:\\n  W_...   \n",
       "210  embed:\\n  W_E: (100000, 2048)\\npos_embed:\\n  W...   \n",
       "\n",
       "                        tensor_shapes.state_dict.raw__  \\\n",
       "0    {'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...   \n",
       "1    {'embed': {'W_E': '(50257, 1024)'}, 'pos_embed...   \n",
       "2    {'embed': {'W_E': '(50257, 1280)'}, 'pos_embed...   \n",
       "3    {'embed': {'W_E': '(50257, 1600)'}, 'pos_embed...   \n",
       "4    {'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...   \n",
       "..                                                 ...   \n",
       "206  {'embed': {'W_E': '(64000, 7168)'}, 'blocks': ...   \n",
       "207  {'embed': {'W_E': '(32128, 512)'}, 'pos_embed'...   \n",
       "208  {'embed': {'W_E': '(32128, 768)'}, 'pos_embed'...   \n",
       "209  {'embed': {'W_E': '(32128, 1024)'}, 'pos_embed...   \n",
       "210  {'embed': {'W_E': '(100000, 2048)'}, 'pos_embe...   \n",
       "\n",
       "                        tensor_shapes.activation_cache  \\\n",
       "0    blocks:\\n  '[0-11]':\\n    ln1:\\n      hook_sca...   \n",
       "1    blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...   \n",
       "2    blocks:\\n  '[0-35]':\\n    ln1:\\n      hook_sca...   \n",
       "3    blocks:\\n  '[0-47]':\\n    ln1:\\n      hook_sca...   \n",
       "4    blocks:\\n  '[0-5]':\\n    ln1:\\n      hook_scal...   \n",
       "..                                                 ...   \n",
       "206  blocks:\\n  '[0-59]':\\n    ln1:\\n      hook_sca...   \n",
       "207                                                NaN   \n",
       "208                                                NaN   \n",
       "209                                                NaN   \n",
       "210  blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...   \n",
       "\n",
       "                  tensor_shapes.activation_cache.raw__  \n",
       "0    {'blocks': {'[0-11]': {'ln1': {'hook_scale': '...  \n",
       "1    {'blocks': {'[0-23]': {'ln1': {'hook_scale': '...  \n",
       "2    {'blocks': {'[0-35]': {'ln1': {'hook_scale': '...  \n",
       "3    {'blocks': {'[0-47]': {'ln1': {'hook_scale': '...  \n",
       "4    {'blocks': {'[0-5]': {'ln1': {'hook_scale': '(...  \n",
       "..                                                 ...  \n",
       "206  {'blocks': {'[0-59]': {'ln1': {'hook_scale': '...  \n",
       "207                                                NaN  \n",
       "208                                                NaN  \n",
       "209                                                NaN  \n",
       "210  {'blocks': {'[0-23]': {'ln1': {'hook_scale': '...  \n",
       "\n",
       "[211 rows x 30 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(TABLE)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
