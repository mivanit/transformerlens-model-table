{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from get_model_table import get_model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model info:  12%|█▏        | 16/136 [00:13<01:49,  1.10it/s, model: gpt-neox-20b]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  20%|█▉        | 27/136 [00:20<01:17,  1.40it/s, model: pythia-14m]            Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  21%|██        | 28/136 [00:21<01:09,  1.56it/s, model: pythia-31m]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  21%|██▏       | 29/136 [00:21<01:04,  1.66it/s, model: pythia-70m]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  22%|██▏       | 30/136 [00:22<01:01,  1.73it/s, model: pythia-160m]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  23%|██▎       | 31/136 [00:23<01:01,  1.70it/s, model: pythia-410m]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  24%|██▎       | 32/136 [00:23<01:10,  1.48it/s, model: pythia-1b]  Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  24%|██▍       | 33/136 [00:24<01:08,  1.51it/s, model: pythia-1.4b]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  25%|██▌       | 34/136 [00:25<01:18,  1.30it/s, model: pythia-2.8b]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  26%|██▌       | 35/136 [00:26<01:23,  1.21it/s, model: pythia-6.9b]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  26%|██▋       | 36/136 [00:27<01:30,  1.11it/s, model: pythia-12b] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  27%|██▋       | 37/136 [00:28<01:31,  1.08it/s, model: pythia-70m-deduped]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  28%|██▊       | 38/136 [00:29<01:16,  1.28it/s, model: pythia-160m-deduped]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  29%|██▊       | 39/136 [00:29<01:08,  1.43it/s, model: pythia-410m-deduped]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  29%|██▉       | 40/136 [00:30<01:10,  1.37it/s, model: pythia-1b-deduped]  Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  30%|███       | 41/136 [00:31<01:06,  1.43it/s, model: pythia-1.4b-deduped]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  31%|███       | 42/136 [00:31<01:11,  1.31it/s, model: pythia-2.8b-deduped]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  32%|███▏      | 43/136 [00:32<01:14,  1.25it/s, model: pythia-6.9b-deduped]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  32%|███▏      | 44/136 [00:33<01:14,  1.24it/s, model: pythia-12b-deduped] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  33%|███▎      | 45/136 [00:34<01:17,  1.18it/s, model: pythia-70m-v0]     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  34%|███▍      | 46/136 [00:35<01:13,  1.23it/s, model: pythia-160m-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  35%|███▍      | 47/136 [00:36<01:17,  1.15it/s, model: pythia-410m-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  35%|███▌      | 48/136 [00:37<01:11,  1.22it/s, model: pythia-1b-v0]  Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  36%|███▌      | 49/136 [00:37<01:05,  1.32it/s, model: pythia-1.4b-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  37%|███▋      | 50/136 [00:38<01:09,  1.24it/s, model: pythia-2.8b-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  38%|███▊      | 51/136 [00:39<01:10,  1.21it/s, model: pythia-6.9b-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  38%|███▊      | 52/136 [00:40<01:21,  1.03it/s, model: pythia-12b-v0] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  39%|███▉      | 53/136 [00:41<01:19,  1.04it/s, model: pythia-70m-deduped-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  40%|███▉      | 54/136 [00:42<01:12,  1.14it/s, model: pythia-160m-deduped-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  40%|████      | 55/136 [00:42<01:03,  1.27it/s, model: pythia-410m-deduped-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  41%|████      | 56/136 [00:43<01:07,  1.19it/s, model: pythia-1b-deduped-v0]  Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  42%|████▏     | 57/136 [00:44<01:03,  1.25it/s, model: pythia-1.4b-deduped-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  43%|████▎     | 58/136 [00:45<01:00,  1.29it/s, model: pythia-2.8b-deduped-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  43%|████▎     | 59/136 [00:46<01:06,  1.15it/s, model: pythia-6.9b-deduped-v0]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  44%|████▍     | 60/136 [00:47<01:08,  1.11it/s, model: pythia-12b-deduped-v0] Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  45%|████▍     | 61/136 [00:48<01:11,  1.04it/s, model: pythia-160m-seed1]    Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  46%|████▌     | 62/136 [00:49<01:03,  1.17it/s, model: pythia-160m-seed2]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  46%|████▋     | 63/136 [00:49<00:59,  1.23it/s, model: pythia-160m-seed3]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  47%|████▋     | 64/136 [00:50<00:58,  1.24it/s, model: solu-1l-pile]     Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  48%|████▊     | 65/136 [00:50<00:48,  1.47it/s, model: solu-2l-pile]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  49%|████▊     | 66/136 [00:51<00:43,  1.61it/s, model: solu-4l-pile]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  49%|████▉     | 67/136 [00:51<00:38,  1.78it/s, model: solu-6l-pile]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  50%|█████     | 68/136 [00:52<00:36,  1.87it/s, model: solu-8l-pile]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  51%|█████     | 69/136 [00:52<00:34,  1.92it/s, model: solu-10l-pile]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  51%|█████▏    | 70/136 [00:53<00:33,  1.96it/s, model: solu-12l-pile]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  64%|██████▍   | 87/136 [01:00<00:18,  2.63it/s, model: attn-only-2l-demo]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  67%|██████▋   | 91/136 [01:02<00:19,  2.29it/s, model: llama-7b]         WARNING:root:llama-7b-hf tokenizer not loaded. Please load manually.\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model llama-7b: 'HookedTransformer' object has no attribute 'tokenizer'\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  68%|██████▊   | 92/136 [01:02<00:15,  2.91it/s, model: llama-13b]WARNING:root:llama-13b-hf tokenizer not loaded. Please load manually.\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model llama-13b: 'HookedTransformer' object has no attribute 'tokenizer'\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  68%|██████▊   | 93/136 [01:02<00:12,  3.45it/s, model: llama-30b]WARNING:root:llama-30b-hf tokenizer not loaded. Please load manually.\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model llama-30b: 'HookedTransformer' object has no attribute 'tokenizer'\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  69%|██████▉   | 94/136 [01:03<00:13,  3.21it/s, model: llama-65b]WARNING:root:llama-65b-hf tokenizer not loaded. Please load manually.\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model llama-65b: 'HookedTransformer' object has no attribute 'tokenizer'\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  70%|██████▉   | 95/136 [01:03<00:12,  3.27it/s, model: Llama-2-7b]f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model Llama-2-7b: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-hf.\n",
      "401 Client Error. (Request ID: Root=1-65e0d1b7-467bb74652c46cbf1b841ccb;0b3b600c-b760-4dff-bdd9-9bbe5b338410)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-hf is gated. You must be authenticated to access it.\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  71%|███████   | 96/136 [01:03<00:11,  3.54it/s, model: Llama-2-7b-chat]f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model Llama-2-7b-chat: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-7b-chat-hf.\n",
      "401 Client Error. (Request ID: Root=1-65e0d1b7-1c0b578670fcc9a23b6f7ae1;07472732-bbde-40bd-9dd4-289378edc832)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-7b-chat-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-7b-chat-hf is gated. You must be authenticated to access it.\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  71%|███████▏  | 97/136 [01:03<00:10,  3.81it/s, model: Llama-2-13b]    f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model Llama-2-13b: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-13b-hf.\n",
      "401 Client Error. (Request ID: Root=1-65e0d1b7-33a422b72307906a2acff687;f9284046-0b93-4024-9bfb-3e6ec847d1df)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-13b-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-13b-hf is gated. You must be authenticated to access it.\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  72%|███████▏  | 98/136 [01:04<00:09,  4.06it/s, model: Llama-2-13b-chat]f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model Llama-2-13b-chat: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/meta-llama/Llama-2-13b-chat-hf.\n",
      "401 Client Error. (Request ID: Root=1-65e0d1b8-7f52adba7d0728d81d873afe;975d7ad1-98f0-4a63-9619-7104fe75be0b)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/meta-llama/Llama-2-13b-chat-hf/resolve/main/config.json.\n",
      "Repo model meta-llama/Llama-2-13b-chat-hf is gated. You must be authenticated to access it.\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  73%|███████▎  | 99/136 [01:04<00:08,  4.29it/s, model: CodeLlamallama-2-7b]f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model CodeLlamallama-2-7b: CodeLlama-7b-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  74%|███████▎  | 100/136 [01:04<00:07,  4.80it/s, model: CodeLlama-7b-python]f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model CodeLlama-7b-python: CodeLlama-7b-Python-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  74%|███████▍  | 101/136 [01:04<00:06,  5.50it/s, model: CodeLlama-7b-instruct]f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model CodeLlama-7b-instruct: CodeLlama-7b-Instruct-hf is not a local folder and is not a valid model identifier listed on 'https://huggingface.co/models'\n",
      "If this is a private repository, make sure to pass a token having permission to this repo either by logging in with `huggingface-cli login` or by passing `token=<your_token>`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  87%|████████▋ | 118/136 [01:11<00:07,  2.43it/s, model: stablelm-base-alpha-3b]      Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  88%|████████▊ | 119/136 [01:12<00:07,  2.18it/s, model: stablelm-base-alpha-7b]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  88%|████████▊ | 120/136 [01:13<00:09,  1.69it/s, model: stablelm-tuned-alpha-3b]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  89%|████████▉ | 121/136 [01:13<00:09,  1.63it/s, model: stablelm-tuned-alpha-7b]Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  90%|████████▉ | 122/136 [01:14<00:08,  1.64it/s, model: mistral-7b]             f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model mistral-7b: cannot repeat_interleave a meta tensor without output_size\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  90%|█████████ | 123/136 [01:14<00:06,  1.88it/s, model: mistral-7b-instruct]f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model mistral-7b-instruct: cannot repeat_interleave a meta tensor without output_size\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  95%|█████████▍| 129/136 [01:22<00:08,  1.28s/it, model: santacoder]         WARNING:root:Loading model bigcode/santacoder requires setting trust_remote_code=True\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "Loading model info:  96%|█████████▌| 130/136 [01:23<00:06,  1.16s/it, model: qwen-1.8b] WARNING:root:Loading model Qwen/Qwen-1_8B requires setting trust_remote_code=True\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model qwen-1.8b: This modeling file requires the following packages that were not found in your environment: tiktoken. Run `pip install tiktoken`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  96%|█████████▋| 131/136 [01:23<00:04,  1.01it/s, model: qwen-7b]  WARNING:root:Loading model Qwen/Qwen-7B requires setting trust_remote_code=True\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model qwen-7b: This modeling file requires the following packages that were not found in your environment: tiktoken. Run `pip install tiktoken`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  97%|█████████▋| 132/136 [01:24<00:03,  1.06it/s, model: qwen-14b]WARNING:root:Loading model Qwen/Qwen-14B requires setting trust_remote_code=True\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model qwen-14b: This modeling file requires the following packages that were not found in your environment: tiktoken. Run `pip install tiktoken`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  98%|█████████▊| 133/136 [01:25<00:02,  1.21it/s, model: qwen-1.8b-chat]WARNING:root:Loading model Qwen/Qwen-1_8B-Chat requires setting trust_remote_code=True\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model qwen-1.8b-chat: This modeling file requires the following packages that were not found in your environment: tiktoken. Run `pip install tiktoken`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  99%|█████████▊| 134/136 [01:25<00:01,  1.31it/s, model: qwen-7b-chat]  WARNING:root:Loading model Qwen/Qwen-7B-Chat requires setting trust_remote_code=True\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model qwen-7b-chat: This modeling file requires the following packages that were not found in your environment: tiktoken. Run `pip install tiktoken`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info:  99%|█████████▉| 135/136 [01:26<00:00,  1.33it/s, model: qwen-14b-chat]WARNING:root:Loading model Qwen/Qwen-14B-Chat requires setting trust_remote_code=True\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:116: UserWarning: Failed to get tensor shapes for model qwen-14b-chat: This modeling file requires the following packages that were not found in your environment: tiktoken. Run `pip install tiktoken`\n",
      "  warnings.warn(f\"Failed to get tensor shapes for model {model_name}: {e}\")\n",
      "Loading model info: 100%|██████████| 136/136 [01:27<00:00,  1.56it/s, model: qwen-14b-chat]\n"
     ]
    }
   ],
   "source": [
    "TABLE = get_model_table(force_reload=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_alias</th>\n",
       "      <th>official_name</th>\n",
       "      <th>model_size_info</th>\n",
       "      <th>model_type</th>\n",
       "      <th>state_dict</th>\n",
       "      <th>activation_cache</th>\n",
       "      <th>cfg_model_name</th>\n",
       "      <th>n_params_str</th>\n",
       "      <th>n_params</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>d_model</th>\n",
       "      <th>d_vocab</th>\n",
       "      <th>act_fn</th>\n",
       "      <th>positional_embedding_type</th>\n",
       "      <th>parallel_attn_mlp</th>\n",
       "      <th>original_architecture</th>\n",
       "      <th>normalization_type</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>blocks:\\n  '[0-11]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>85M</td>\n",
       "      <td>84934656</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1024)\\npos_embed:\\n  W_...</td>\n",
       "      <td>blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>302M</td>\n",
       "      <td>301989888</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1280)\\npos_embed:\\n  W_...</td>\n",
       "      <td>blocks:\\n  '[0-35]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>708M</td>\n",
       "      <td>707788800</td>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>1280</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1600)\\npos_embed:\\n  W_...</td>\n",
       "      <td>blocks:\\n  '[0-47]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1.5B</td>\n",
       "      <td>1474560000</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>1600</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distillgpt2</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>blocks:\\n  '[0-5]':\\n    ln1:\\n      hook_scal...</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>42M</td>\n",
       "      <td>42467328</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>qwen-7b</td>\n",
       "      <td>Qwen/Qwen-7B</td>\n",
       "      <td>7b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-7B</td>\n",
       "      <td>5.0B</td>\n",
       "      <td>5033164800</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4096</td>\n",
       "      <td>151936</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "      <td>RMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>qwen-14b</td>\n",
       "      <td>Qwen/Qwen-14B</td>\n",
       "      <td>14b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-14B</td>\n",
       "      <td>9.8B</td>\n",
       "      <td>9804185600</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>5120</td>\n",
       "      <td>152064</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "      <td>RMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>qwen-1.8b-chat</td>\n",
       "      <td>Qwen/Qwen-1_8B-Chat</td>\n",
       "      <td>1.8b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-1_8B-Chat</td>\n",
       "      <td>944M</td>\n",
       "      <td>943718400</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>151936</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "      <td>RMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>qwen-7b-chat</td>\n",
       "      <td>Qwen/Qwen-7B-Chat</td>\n",
       "      <td>7b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-7B-Chat</td>\n",
       "      <td>5.0B</td>\n",
       "      <td>5033164800</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4096</td>\n",
       "      <td>151936</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "      <td>RMS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>qwen-14b-chat</td>\n",
       "      <td>Qwen/Qwen-14B-Chat</td>\n",
       "      <td>14b</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-14B-Chat</td>\n",
       "      <td>9.8B</td>\n",
       "      <td>9804185600</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>5120</td>\n",
       "      <td>152064</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "      <td>RMS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 18 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_alias        official_name model_size_info model_type  \\\n",
       "0        gpt2-small                 gpt2            None       gpt2   \n",
       "1       gpt2-medium          gpt2-medium            None       gpt2   \n",
       "2        gpt2-large           gpt2-large            None       gpt2   \n",
       "3           gpt2-xl              gpt2-xl            None       gpt2   \n",
       "4       distillgpt2           distilgpt2            None       gpt2   \n",
       "..              ...                  ...             ...        ...   \n",
       "131         qwen-7b         Qwen/Qwen-7B              7b       None   \n",
       "132        qwen-14b        Qwen/Qwen-14B             14b       None   \n",
       "133  qwen-1.8b-chat  Qwen/Qwen-1_8B-Chat            1.8b       None   \n",
       "134    qwen-7b-chat    Qwen/Qwen-7B-Chat              7b       None   \n",
       "135   qwen-14b-chat   Qwen/Qwen-14B-Chat             14b       None   \n",
       "\n",
       "                                            state_dict  \\\n",
       "0    embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...   \n",
       "1    embed:\\n  W_E: (50257, 1024)\\npos_embed:\\n  W_...   \n",
       "2    embed:\\n  W_E: (50257, 1280)\\npos_embed:\\n  W_...   \n",
       "3    embed:\\n  W_E: (50257, 1600)\\npos_embed:\\n  W_...   \n",
       "4    embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...   \n",
       "..                                                 ...   \n",
       "131                                               None   \n",
       "132                                               None   \n",
       "133                                               None   \n",
       "134                                               None   \n",
       "135                                               None   \n",
       "\n",
       "                                      activation_cache  cfg_model_name  \\\n",
       "0    blocks:\\n  '[0-11]':\\n    ln1:\\n      hook_sca...            gpt2   \n",
       "1    blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...     gpt2-medium   \n",
       "2    blocks:\\n  '[0-35]':\\n    ln1:\\n      hook_sca...      gpt2-large   \n",
       "3    blocks:\\n  '[0-47]':\\n    ln1:\\n      hook_sca...         gpt2-xl   \n",
       "4    blocks:\\n  '[0-5]':\\n    ln1:\\n      hook_scal...      distilgpt2   \n",
       "..                                                 ...             ...   \n",
       "131                                               None         Qwen-7B   \n",
       "132                                               None        Qwen-14B   \n",
       "133                                               None  Qwen-1_8B-Chat   \n",
       "134                                               None    Qwen-7B-Chat   \n",
       "135                                               None   Qwen-14B-Chat   \n",
       "\n",
       "    n_params_str    n_params  n_layers  n_heads  d_model  d_vocab    act_fn  \\\n",
       "0            85M    84934656        12       12      768    50257  gelu_new   \n",
       "1           302M   301989888        24       16     1024    50257  gelu_new   \n",
       "2           708M   707788800        36       20     1280    50257  gelu_new   \n",
       "3           1.5B  1474560000        48       25     1600    50257  gelu_new   \n",
       "4            42M    42467328         6       12      768    50257  gelu_new   \n",
       "..           ...         ...       ...      ...      ...      ...       ...   \n",
       "131         5.0B  5033164800        32       32     4096   151936      silu   \n",
       "132         9.8B  9804185600        40       40     5120   152064      silu   \n",
       "133         944M   943718400        24       16     2048   151936      silu   \n",
       "134         5.0B  5033164800        32       32     4096   151936      silu   \n",
       "135         9.8B  9804185600        40       40     5120   152064      silu   \n",
       "\n",
       "    positional_embedding_type  parallel_attn_mlp original_architecture  \\\n",
       "0                    standard              False       GPT2LMHeadModel   \n",
       "1                    standard              False       GPT2LMHeadModel   \n",
       "2                    standard              False       GPT2LMHeadModel   \n",
       "3                    standard              False       GPT2LMHeadModel   \n",
       "4                    standard              False       GPT2LMHeadModel   \n",
       "..                        ...                ...                   ...   \n",
       "131                    rotary              False       QWenLMHeadModel   \n",
       "132                    rotary              False       QWenLMHeadModel   \n",
       "133                    rotary              False       QWenLMHeadModel   \n",
       "134                    rotary              False       QWenLMHeadModel   \n",
       "135                    rotary              False       QWenLMHeadModel   \n",
       "\n",
       "    normalization_type  \n",
       "0                   LN  \n",
       "1                   LN  \n",
       "2                   LN  \n",
       "3                   LN  \n",
       "4                   LN  \n",
       "..                 ...  \n",
       "131                RMS  \n",
       "132                RMS  \n",
       "133                RMS  \n",
       "134                RMS  \n",
       "135                RMS  \n",
       "\n",
       "[136 rows x 18 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='cuda'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': None,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    }
   ],
   "source": [
    "print(transformer_lens.loading.get_pretrained_model_config('gpt2'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
