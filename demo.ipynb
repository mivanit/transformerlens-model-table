{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from get_model_table import get_model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model info: 100%|██████████| 169/169 [00:32<00:00,  5.21it/s]\n",
      "f:\\projects\\tools\\transformerlens-model-table\\get_model_table.py:294: UserWarning: Failed to get model info for 4/169 models: {'mistral-7b': OSError('You are trying to access a gated repo.\\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\\n401 Client Error. (Request ID: Root=1-66694d8d-604aebb52c523c7148d2643a;ae533f1e-5f42-4a7a-a7be-e502636d9662)\\n\\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\\nAccess to model mistralai/Mistral-7B-v0.1 is restricted. You must be authenticated to access it.'), 'mistral-7b-instruct': OSError('You are trying to access a gated repo.\\nMake sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\\n401 Client Error. (Request ID: Root=1-66694d8d-5716e54f3fcd92b536fcdd29;f09ae052-93bc-444f-a677-8da7d92cdb35)\\n\\nCannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\\nAccess to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must be authenticated to access it.'), 'mixtral': OSError('You are trying to access a gated repo.\\nMake sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-v0.1.\\n401 Client Error. (Request ID: Root=1-66694d8d-40289e0324a89f6c25fc1f56;f569c169-e48e-477d-b935-a2f029cda4d4)\\n\\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/config.json.\\nAccess to model mistralai/Mixtral-8x7B-v0.1 is restricted. You must be authenticated to access it.'), 'mixtral-instruct': OSError('You are trying to access a gated repo.\\nMake sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1.\\n401 Client Error. (Request ID: Root=1-66694d8d-7cb1f5a14df010e8474468a2;072d99f8-0610-4212-a77d-0d48273551d9)\\n\\nCannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\\nAccess to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it.')}\n",
      "\tmistral-7b: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-v0.1.\n",
      "401 Client Error. (Request ID: Root=1-66694d8d-604aebb52c523c7148d2643a;ae533f1e-5f42-4a7a-a7be-e502636d9662)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-v0.1 is restricted. You must be authenticated to access it.\n",
      "\tmistral-7b-instruct: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1.\n",
      "401 Client Error. (Request ID: Root=1-66694d8d-5716e54f3fcd92b536fcdd29;f09ae052-93bc-444f-a677-8da7d92cdb35)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mistral-7B-Instruct-v0.1 is restricted. You must be authenticated to access it.\n",
      "\tmixtral: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-v0.1.\n",
      "401 Client Error. (Request ID: Root=1-66694d8d-40289e0324a89f6c25fc1f56;f569c169-e48e-477d-b935-a2f029cda4d4)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mixtral-8x7B-v0.1 is restricted. You must be authenticated to access it.\n",
      "\tmixtral-instruct: You are trying to access a gated repo.\n",
      "Make sure to have access to it at https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1.\n",
      "401 Client Error. (Request ID: Root=1-66694d8d-7cb1f5a14df010e8474468a2;072d99f8-0610-4212-a77d-0d48273551d9)\n",
      "\n",
      "Cannot access gated repo for url https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1/resolve/main/config.json.\n",
      "Access to model mistralai/Mixtral-8x7B-Instruct-v0.1 is restricted. You must be authenticated to access it.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "WRITING PARTIAL DATA\n",
      "  warnings.warn(msg + \"\\n\\n\" + \"-\"*80 + \"\\n\\n\" + \"WRITING PARTIAL DATA\")\n"
     ]
    }
   ],
   "source": [
    "TABLE = get_model_table(\n",
    "\tverbose=True,\n",
    "\tforce_reload=True,\n",
    "\tparallelize=True,\n",
    "\tdo_write=True,\n",
    "\tallow_except=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name.default_alias</th>\n",
       "      <th>name.official</th>\n",
       "      <th>name.aliases</th>\n",
       "      <th>model_type</th>\n",
       "      <th>name.from_cfg</th>\n",
       "      <th>n_params.as_str</th>\n",
       "      <th>n_params.as_int</th>\n",
       "      <th>n_params.from_name</th>\n",
       "      <th>config.n_params</th>\n",
       "      <th>config.n_layers</th>\n",
       "      <th>...</th>\n",
       "      <th>config.positional_embedding_type</th>\n",
       "      <th>config.parallel_attn_mlp</th>\n",
       "      <th>config.original_architecture</th>\n",
       "      <th>config.normalization_type</th>\n",
       "      <th>cfg.raw__</th>\n",
       "      <th>cfg</th>\n",
       "      <th>tensor_shapes.state_dict</th>\n",
       "      <th>tensor_shapes.state_dict.raw__</th>\n",
       "      <th>tensor_shapes.activation_cache</th>\n",
       "      <th>tensor_shapes.activation_cache.raw__</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>[gpt2-small]</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>85M</td>\n",
       "      <td>84934656</td>\n",
       "      <td>None</td>\n",
       "      <td>84934656</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 12, 'd_model': 768, 'n_ctx': 1024...</td>\n",
       "      <td>n_layers: 12\\nd_model: 768\\nn_ctx: 1024\\nd_hea...</td>\n",
       "      <td>embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...</td>\n",
       "      <td>blocks:\\n  '[0-11]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-11]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>[]</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>302M</td>\n",
       "      <td>301989888</td>\n",
       "      <td>None</td>\n",
       "      <td>301989888</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 24, 'd_model': 1024, 'n_ctx': 102...</td>\n",
       "      <td>n_layers: 24\\nd_model: 1024\\nn_ctx: 1024\\nd_he...</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1024)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed...</td>\n",
       "      <td>blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-23]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>[]</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>708M</td>\n",
       "      <td>707788800</td>\n",
       "      <td>None</td>\n",
       "      <td>707788800</td>\n",
       "      <td>36</td>\n",
       "      <td>...</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 36, 'd_model': 1280, 'n_ctx': 102...</td>\n",
       "      <td>n_layers: 36\\nd_model: 1280\\nn_ctx: 1024\\nd_he...</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1280)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 1280)'}, 'pos_embed...</td>\n",
       "      <td>blocks:\\n  '[0-35]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-35]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>[]</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1.5B</td>\n",
       "      <td>1474560000</td>\n",
       "      <td>None</td>\n",
       "      <td>1474560000</td>\n",
       "      <td>48</td>\n",
       "      <td>...</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 48, 'd_model': 1600, 'n_ctx': 102...</td>\n",
       "      <td>n_layers: 48\\nd_model: 1600\\nn_ctx: 1024\\nd_he...</td>\n",
       "      <td>embed:\\n  W_E: (50257, 1600)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 1600)'}, 'pos_embed...</td>\n",
       "      <td>blocks:\\n  '[0-47]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-47]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distillgpt2</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>[distillgpt2, distill-gpt2, distil-gpt2, gpt2-xs]</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>42M</td>\n",
       "      <td>42467328</td>\n",
       "      <td>None</td>\n",
       "      <td>42467328</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 6, 'd_model': 768, 'n_ctx': 1024,...</td>\n",
       "      <td>n_layers: 6\\nd_model: 768\\nn_ctx: 1024\\nd_head...</td>\n",
       "      <td>embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...</td>\n",
       "      <td>blocks:\\n  '[0-5]':\\n    ln1:\\n      hook_scal...</td>\n",
       "      <td>{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>160</th>\n",
       "      <td>yi-34b-chat</td>\n",
       "      <td>01-ai/Yi-34B-Chat</td>\n",
       "      <td>[yi-34b-chat, Yi-34B-Chat]</td>\n",
       "      <td>None</td>\n",
       "      <td>Yi-34B-Chat</td>\n",
       "      <td>39B</td>\n",
       "      <td>38755368960</td>\n",
       "      <td>34b</td>\n",
       "      <td>38755368960</td>\n",
       "      <td>60</td>\n",
       "      <td>...</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>LlamaForCausalLM</td>\n",
       "      <td>RMS</td>\n",
       "      <td>{'n_layers': 60, 'd_model': 7168, 'n_ctx': 409...</td>\n",
       "      <td>n_layers: 60\\nd_model: 7168\\nn_ctx: 4096\\nd_he...</td>\n",
       "      <td>embed:\\n  W_E: (64000, 7168)\\nblocks:\\n  '[0-5...</td>\n",
       "      <td>{'embed': {'W_E': '(64000, 7168)'}, 'blocks': ...</td>\n",
       "      <td>blocks:\\n  '[0-59]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-59]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>161</th>\n",
       "      <td>t5-small</td>\n",
       "      <td>google-t5/t5-small</td>\n",
       "      <td>[t5-small]</td>\n",
       "      <td>None</td>\n",
       "      <td>t5-small</td>\n",
       "      <td>19M</td>\n",
       "      <td>18874368</td>\n",
       "      <td>None</td>\n",
       "      <td>18874368</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>relative_positional_bias</td>\n",
       "      <td>False</td>\n",
       "      <td>T5ForConditionalGeneration</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 6, 'd_model': 512, 'n_ctx': 20, '...</td>\n",
       "      <td>n_layers: 6\\nd_model: 512\\nn_ctx: 20\\nd_head: ...</td>\n",
       "      <td>embed:\\n  W_E: (32128, 512)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(32128, 512)'}, 'pos_embed'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>162</th>\n",
       "      <td>t5-base</td>\n",
       "      <td>google-t5/t5-base</td>\n",
       "      <td>[t5-base]</td>\n",
       "      <td>None</td>\n",
       "      <td>t5-base</td>\n",
       "      <td>85M</td>\n",
       "      <td>84934656</td>\n",
       "      <td>None</td>\n",
       "      <td>84934656</td>\n",
       "      <td>12</td>\n",
       "      <td>...</td>\n",
       "      <td>relative_positional_bias</td>\n",
       "      <td>False</td>\n",
       "      <td>T5ForConditionalGeneration</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 12, 'd_model': 768, 'n_ctx': 20, ...</td>\n",
       "      <td>n_layers: 12\\nd_model: 768\\nn_ctx: 20\\nd_head:...</td>\n",
       "      <td>embed:\\n  W_E: (32128, 768)\\npos_embed:\\n  W_p...</td>\n",
       "      <td>{'embed': {'W_E': '(32128, 768)'}, 'pos_embed'...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163</th>\n",
       "      <td>t5-large</td>\n",
       "      <td>google-t5/t5-large</td>\n",
       "      <td>[t5-large]</td>\n",
       "      <td>None</td>\n",
       "      <td>t5-large</td>\n",
       "      <td>302M</td>\n",
       "      <td>301989888</td>\n",
       "      <td>None</td>\n",
       "      <td>301989888</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>relative_positional_bias</td>\n",
       "      <td>False</td>\n",
       "      <td>T5ForConditionalGeneration</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 24, 'd_model': 1024, 'n_ctx': 20,...</td>\n",
       "      <td>n_layers: 24\\nd_model: 1024\\nn_ctx: 20\\nd_head...</td>\n",
       "      <td>embed:\\n  W_E: (32128, 1024)\\npos_embed:\\n  W_...</td>\n",
       "      <td>{'embed': {'W_E': '(32128, 1024)'}, 'pos_embed...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164</th>\n",
       "      <td>mGPT</td>\n",
       "      <td>None</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>mGPT</td>\n",
       "      <td>1.2B</td>\n",
       "      <td>1207959552</td>\n",
       "      <td>None</td>\n",
       "      <td>1207959552</td>\n",
       "      <td>24</td>\n",
       "      <td>...</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "      <td>LN</td>\n",
       "      <td>{'n_layers': 24, 'd_model': 2048, 'n_ctx': 204...</td>\n",
       "      <td>n_layers: 24\\nd_model: 2048\\nn_ctx: 2048\\nd_he...</td>\n",
       "      <td>embed:\\n  W_E: (100000, 2048)\\npos_embed:\\n  W...</td>\n",
       "      <td>{'embed': {'W_E': '(100000, 2048)'}, 'pos_embe...</td>\n",
       "      <td>blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...</td>\n",
       "      <td>{'blocks': {'[0-23]': {'ln1': {'hook_scale': '...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>165 rows × 24 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    name.default_alias       name.official  \\\n",
       "0           gpt2-small                gpt2   \n",
       "1          gpt2-medium         gpt2-medium   \n",
       "2           gpt2-large          gpt2-large   \n",
       "3              gpt2-xl             gpt2-xl   \n",
       "4          distillgpt2          distilgpt2   \n",
       "..                 ...                 ...   \n",
       "160        yi-34b-chat   01-ai/Yi-34B-Chat   \n",
       "161           t5-small  google-t5/t5-small   \n",
       "162            t5-base   google-t5/t5-base   \n",
       "163           t5-large  google-t5/t5-large   \n",
       "164               mGPT                None   \n",
       "\n",
       "                                          name.aliases model_type  \\\n",
       "0                                         [gpt2-small]       gpt2   \n",
       "1                                                   []       gpt2   \n",
       "2                                                   []       gpt2   \n",
       "3                                                   []       gpt2   \n",
       "4    [distillgpt2, distill-gpt2, distil-gpt2, gpt2-xs]       gpt2   \n",
       "..                                                 ...        ...   \n",
       "160                         [yi-34b-chat, Yi-34B-Chat]       None   \n",
       "161                                         [t5-small]       None   \n",
       "162                                          [t5-base]       None   \n",
       "163                                         [t5-large]       None   \n",
       "164                                                 []       None   \n",
       "\n",
       "    name.from_cfg n_params.as_str  n_params.as_int n_params.from_name  \\\n",
       "0            gpt2             85M         84934656               None   \n",
       "1     gpt2-medium            302M        301989888               None   \n",
       "2      gpt2-large            708M        707788800               None   \n",
       "3         gpt2-xl            1.5B       1474560000               None   \n",
       "4      distilgpt2             42M         42467328               None   \n",
       "..            ...             ...              ...                ...   \n",
       "160   Yi-34B-Chat             39B      38755368960                34b   \n",
       "161      t5-small             19M         18874368               None   \n",
       "162       t5-base             85M         84934656               None   \n",
       "163      t5-large            302M        301989888               None   \n",
       "164          mGPT            1.2B       1207959552               None   \n",
       "\n",
       "     config.n_params  config.n_layers  ...  config.positional_embedding_type  \\\n",
       "0           84934656               12  ...                          standard   \n",
       "1          301989888               24  ...                          standard   \n",
       "2          707788800               36  ...                          standard   \n",
       "3         1474560000               48  ...                          standard   \n",
       "4           42467328                6  ...                          standard   \n",
       "..               ...              ...  ...                               ...   \n",
       "160      38755368960               60  ...                            rotary   \n",
       "161         18874368                6  ...          relative_positional_bias   \n",
       "162         84934656               12  ...          relative_positional_bias   \n",
       "163        301989888               24  ...          relative_positional_bias   \n",
       "164       1207959552               24  ...                          standard   \n",
       "\n",
       "     config.parallel_attn_mlp  config.original_architecture  \\\n",
       "0                       False               GPT2LMHeadModel   \n",
       "1                       False               GPT2LMHeadModel   \n",
       "2                       False               GPT2LMHeadModel   \n",
       "3                       False               GPT2LMHeadModel   \n",
       "4                       False               GPT2LMHeadModel   \n",
       "..                        ...                           ...   \n",
       "160                     False              LlamaForCausalLM   \n",
       "161                     False    T5ForConditionalGeneration   \n",
       "162                     False    T5ForConditionalGeneration   \n",
       "163                     False    T5ForConditionalGeneration   \n",
       "164                     False               GPT2LMHeadModel   \n",
       "\n",
       "    config.normalization_type  \\\n",
       "0                          LN   \n",
       "1                          LN   \n",
       "2                          LN   \n",
       "3                          LN   \n",
       "4                          LN   \n",
       "..                        ...   \n",
       "160                       RMS   \n",
       "161                        LN   \n",
       "162                        LN   \n",
       "163                        LN   \n",
       "164                        LN   \n",
       "\n",
       "                                             cfg.raw__  \\\n",
       "0    {'n_layers': 12, 'd_model': 768, 'n_ctx': 1024...   \n",
       "1    {'n_layers': 24, 'd_model': 1024, 'n_ctx': 102...   \n",
       "2    {'n_layers': 36, 'd_model': 1280, 'n_ctx': 102...   \n",
       "3    {'n_layers': 48, 'd_model': 1600, 'n_ctx': 102...   \n",
       "4    {'n_layers': 6, 'd_model': 768, 'n_ctx': 1024,...   \n",
       "..                                                 ...   \n",
       "160  {'n_layers': 60, 'd_model': 7168, 'n_ctx': 409...   \n",
       "161  {'n_layers': 6, 'd_model': 512, 'n_ctx': 20, '...   \n",
       "162  {'n_layers': 12, 'd_model': 768, 'n_ctx': 20, ...   \n",
       "163  {'n_layers': 24, 'd_model': 1024, 'n_ctx': 20,...   \n",
       "164  {'n_layers': 24, 'd_model': 2048, 'n_ctx': 204...   \n",
       "\n",
       "                                                   cfg  \\\n",
       "0    n_layers: 12\\nd_model: 768\\nn_ctx: 1024\\nd_hea...   \n",
       "1    n_layers: 24\\nd_model: 1024\\nn_ctx: 1024\\nd_he...   \n",
       "2    n_layers: 36\\nd_model: 1280\\nn_ctx: 1024\\nd_he...   \n",
       "3    n_layers: 48\\nd_model: 1600\\nn_ctx: 1024\\nd_he...   \n",
       "4    n_layers: 6\\nd_model: 768\\nn_ctx: 1024\\nd_head...   \n",
       "..                                                 ...   \n",
       "160  n_layers: 60\\nd_model: 7168\\nn_ctx: 4096\\nd_he...   \n",
       "161  n_layers: 6\\nd_model: 512\\nn_ctx: 20\\nd_head: ...   \n",
       "162  n_layers: 12\\nd_model: 768\\nn_ctx: 20\\nd_head:...   \n",
       "163  n_layers: 24\\nd_model: 1024\\nn_ctx: 20\\nd_head...   \n",
       "164  n_layers: 24\\nd_model: 2048\\nn_ctx: 2048\\nd_he...   \n",
       "\n",
       "                              tensor_shapes.state_dict  \\\n",
       "0    embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...   \n",
       "1    embed:\\n  W_E: (50257, 1024)\\npos_embed:\\n  W_...   \n",
       "2    embed:\\n  W_E: (50257, 1280)\\npos_embed:\\n  W_...   \n",
       "3    embed:\\n  W_E: (50257, 1600)\\npos_embed:\\n  W_...   \n",
       "4    embed:\\n  W_E: (50257, 768)\\npos_embed:\\n  W_p...   \n",
       "..                                                 ...   \n",
       "160  embed:\\n  W_E: (64000, 7168)\\nblocks:\\n  '[0-5...   \n",
       "161  embed:\\n  W_E: (32128, 512)\\npos_embed:\\n  W_p...   \n",
       "162  embed:\\n  W_E: (32128, 768)\\npos_embed:\\n  W_p...   \n",
       "163  embed:\\n  W_E: (32128, 1024)\\npos_embed:\\n  W_...   \n",
       "164  embed:\\n  W_E: (100000, 2048)\\npos_embed:\\n  W...   \n",
       "\n",
       "                        tensor_shapes.state_dict.raw__  \\\n",
       "0    {'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...   \n",
       "1    {'embed': {'W_E': '(50257, 1024)'}, 'pos_embed...   \n",
       "2    {'embed': {'W_E': '(50257, 1280)'}, 'pos_embed...   \n",
       "3    {'embed': {'W_E': '(50257, 1600)'}, 'pos_embed...   \n",
       "4    {'embed': {'W_E': '(50257, 768)'}, 'pos_embed'...   \n",
       "..                                                 ...   \n",
       "160  {'embed': {'W_E': '(64000, 7168)'}, 'blocks': ...   \n",
       "161  {'embed': {'W_E': '(32128, 512)'}, 'pos_embed'...   \n",
       "162  {'embed': {'W_E': '(32128, 768)'}, 'pos_embed'...   \n",
       "163  {'embed': {'W_E': '(32128, 1024)'}, 'pos_embed...   \n",
       "164  {'embed': {'W_E': '(100000, 2048)'}, 'pos_embe...   \n",
       "\n",
       "                        tensor_shapes.activation_cache  \\\n",
       "0    blocks:\\n  '[0-11]':\\n    ln1:\\n      hook_sca...   \n",
       "1    blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...   \n",
       "2    blocks:\\n  '[0-35]':\\n    ln1:\\n      hook_sca...   \n",
       "3    blocks:\\n  '[0-47]':\\n    ln1:\\n      hook_sca...   \n",
       "4    blocks:\\n  '[0-5]':\\n    ln1:\\n      hook_scal...   \n",
       "..                                                 ...   \n",
       "160  blocks:\\n  '[0-59]':\\n    ln1:\\n      hook_sca...   \n",
       "161                                                NaN   \n",
       "162                                                NaN   \n",
       "163                                                NaN   \n",
       "164  blocks:\\n  '[0-23]':\\n    ln1:\\n      hook_sca...   \n",
       "\n",
       "                  tensor_shapes.activation_cache.raw__  \n",
       "0    {'blocks': {'[0-11]': {'ln1': {'hook_scale': '...  \n",
       "1    {'blocks': {'[0-23]': {'ln1': {'hook_scale': '...  \n",
       "2    {'blocks': {'[0-35]': {'ln1': {'hook_scale': '...  \n",
       "3    {'blocks': {'[0-47]': {'ln1': {'hook_scale': '...  \n",
       "4    {'blocks': {'[0-5]': {'ln1': {'hook_scale': '(...  \n",
       "..                                                 ...  \n",
       "160  {'blocks': {'[0-59]': {'ln1': {'hook_scale': '...  \n",
       "161                                                NaN  \n",
       "162                                                NaN  \n",
       "163                                                NaN  \n",
       "164  {'blocks': {'[0-23]': {'ln1': {'hook_scale': '...  \n",
       "\n",
       "[165 rows x 24 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(TABLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HookedTransformerConfig:\n",
      "{'act_fn': 'gelu_new',\n",
      " 'attention_dir': 'causal',\n",
      " 'attn_only': False,\n",
      " 'attn_types': None,\n",
      " 'checkpoint_index': None,\n",
      " 'checkpoint_label_type': None,\n",
      " 'checkpoint_value': None,\n",
      " 'd_head': 64,\n",
      " 'd_mlp': 3072,\n",
      " 'd_model': 768,\n",
      " 'd_vocab': 50257,\n",
      " 'd_vocab_out': 50257,\n",
      " 'decoder_start_token_id': None,\n",
      " 'default_prepend_bos': True,\n",
      " 'device': device(type='cuda'),\n",
      " 'dtype': torch.float32,\n",
      " 'eps': 1e-05,\n",
      " 'experts_per_token': None,\n",
      " 'final_rms': False,\n",
      " 'from_checkpoint': False,\n",
      " 'gated_mlp': False,\n",
      " 'init_mode': 'gpt2',\n",
      " 'init_weights': False,\n",
      " 'initializer_range': 0.02886751345948129,\n",
      " 'load_in_4bit': False,\n",
      " 'model_name': 'gpt2',\n",
      " 'n_ctx': 1024,\n",
      " 'n_devices': 1,\n",
      " 'n_heads': 12,\n",
      " 'n_key_value_heads': None,\n",
      " 'n_layers': 12,\n",
      " 'n_params': 84934656,\n",
      " 'normalization_type': 'LN',\n",
      " 'num_experts': None,\n",
      " 'original_architecture': 'GPT2LMHeadModel',\n",
      " 'parallel_attn_mlp': False,\n",
      " 'positional_embedding_type': 'standard',\n",
      " 'post_embedding_ln': False,\n",
      " 'relative_attention_max_distance': None,\n",
      " 'relative_attention_num_buckets': None,\n",
      " 'rotary_adjacent_pairs': False,\n",
      " 'rotary_base': 10000,\n",
      " 'rotary_dim': None,\n",
      " 'scale_attn_by_inverse_layer_idx': False,\n",
      " 'seed': None,\n",
      " 'tie_word_embeddings': False,\n",
      " 'tokenizer_name': 'gpt2',\n",
      " 'tokenizer_prepends_bos': None,\n",
      " 'trust_remote_code': False,\n",
      " 'use_attn_in': False,\n",
      " 'use_attn_result': False,\n",
      " 'use_attn_scale': True,\n",
      " 'use_hook_mlp_in': False,\n",
      " 'use_hook_tokens': False,\n",
      " 'use_local_attn': False,\n",
      " 'use_split_qkv_input': False,\n",
      " 'window_size': None}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python\\Python3_11\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(transformer_lens.loading.get_pretrained_model_config('gpt2'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
