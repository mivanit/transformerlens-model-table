{"name.default_alias":"gpt2-small","name.huggingface":"gpt2","name.aliases":"gpt2-small","model_type":"gpt2","name.from_cfg":"gpt2","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"gpt2","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"gpt2","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"gpt2","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"gpt2-medium","name.huggingface":"gpt2-medium","name.aliases":"","model_type":"gpt2","name.from_cfg":"gpt2-medium","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"gpt2-medium","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"gpt2-medium","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2-medium\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2-medium\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"gpt2-medium","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"gpt2-large","name.huggingface":"gpt2-large","name.aliases":"","model_type":"gpt2","name.from_cfg":"gpt2-large","n_params.as_str":"708M","n_params.as_int":707788800,"n_params.from_name":null,"cfg.n_params":707788800,"cfg.n_layers":36,"cfg.n_heads":20,"cfg.d_model":1280,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":36,"d_model":1280,"n_ctx":1024,"d_head":64,"model_name":"gpt2-large","n_heads":20,"d_mlp":5120,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"gpt2-large","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0223606798,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":707788800,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 36\nd_model: 1280\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2-large\nn_heads: 20\nd_mlp: 5120\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2-large\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.022360679774997897\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 707788800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"gpt2-large","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1280)\npos_embed:\n  W_pos: (1024, 1280)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (1280,)\n    ln2:\n      '[w, b]': (1280,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 1280, 64)\n      W_O: (20, 64, 1280)\n      '[b_Q, b_K, b_V]': (20, 64)\n      b_O: (1280,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1280, 5120)\n      b_in: (5120,)\n      W_out: (5120, 1280)\n      b_out: (1280,)\nln_final:\n  '[w, b]': (1280,)\nunembed:\n  W_U: (1280, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1280)"},"pos_embed":{"W_pos":"(1024, 1280)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(1280,)"},"ln2":{"[w, b]":"(1280,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 1280, 64)","W_O":"(20, 64, 1280)","[b_Q, b_K, b_V]":"(20, 64)","b_O":"(1280,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1280, 5120)","b_in":"(5120,)","W_out":"(5120, 1280)","b_out":"(1280,)"}}},"ln_final":{"[w, b]":"(1280,)"},"unembed":{"W_U":"(1280, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 5120)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1280)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1280)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 64)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 5120)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1280)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1280)"}}
{"name.default_alias":"gpt2-xl","name.huggingface":"gpt2-xl","name.aliases":"","model_type":"gpt2","name.from_cfg":"gpt2-xl","n_params.as_str":"1.5B","n_params.as_int":1474560000,"n_params.from_name":null,"cfg.n_params":1474560000,"cfg.n_layers":48,"cfg.n_heads":25,"cfg.d_model":1600,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":48,"d_model":1600,"n_ctx":1024,"d_head":64,"model_name":"gpt2-xl","n_heads":25,"d_mlp":6400,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"gpt2-xl","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1474560000,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 48\nd_model: 1600\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2-xl\nn_heads: 25\nd_mlp: 6400\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2-xl\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1474560000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"gpt2-xl","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1600)\npos_embed:\n  W_pos: (1024, 1600)\nblocks:\n  '[0-47]':\n    ln1:\n      '[w, b]': (1600,)\n    ln2:\n      '[w, b]': (1600,)\n    attn:\n      '[W_Q, W_K, W_V]': (25, 1600, 64)\n      W_O: (25, 64, 1600)\n      '[b_Q, b_K, b_V]': (25, 64)\n      b_O: (1600,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1600, 6400)\n      b_in: (6400,)\n      W_out: (6400, 1600)\n      b_out: (1600,)\nln_final:\n  '[w, b]': (1600,)\nunembed:\n  W_U: (1600, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1600)"},"pos_embed":{"W_pos":"(1024, 1600)"},"blocks":{"[0-47]":{"ln1":{"[w, b]":"(1600,)"},"ln2":{"[w, b]":"(1600,)"},"attn":{"[W_Q, W_K, W_V]":"(25, 1600, 64)","W_O":"(25, 64, 1600)","[b_Q, b_K, b_V]":"(25, 64)","b_O":"(1600,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1600, 6400)","b_in":"(6400,)","W_out":"(6400, 1600)","b_out":"(1600,)"}}},"ln_final":{"[w, b]":"(1600,)"},"unembed":{"W_U":"(1600, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-47]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1600)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 25, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 25, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1600)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 6400)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1600)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1600)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1600)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-47]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1600)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 25, 64)","[hook_attn_scores, hook_pattern]":"(batch, 25, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1600)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 6400)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1600)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1600)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1600)"}}
{"name.default_alias":"distillgpt2","name.huggingface":"distilgpt2","name.aliases":"distillgpt2, distill-gpt2, distil-gpt2, gpt2-xs","model_type":"gpt2","name.from_cfg":"distilgpt2","n_params.as_str":"42M","n_params.as_int":42467328,"n_params.from_name":null,"cfg.n_params":42467328,"cfg.n_layers":6,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"distilgpt2","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"distilgpt2","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":42467328,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: distilgpt2\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: distilgpt2\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 42467328\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"distilgpt2","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"opt-125m","name.huggingface":"facebook\/opt-125m","name.aliases":"opt-125m, opt-small, opt","model_type":"opt","name.from_cfg":"opt-125m","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"125m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50272,"cfg.act_fn":"relu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"OPTForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"opt-125m","n_heads":12,"d_mlp":3072,"act_fn":"relu","d_vocab":50272,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"OPTForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"facebook\/opt-125m","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50272,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: opt-125m\nn_heads: 12\nd_mlp: 3072\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-125m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"facebook\/opt-125m","tokenizer.vocab_size":50265.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"f1FIzqnRiMYzke1CU0hp8TDxq7k","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"opt-1.3b","name.huggingface":"facebook\/opt-1.3b","name.aliases":"opt-1.3b, opt-medium","model_type":"opt","name.from_cfg":"opt-1.3b","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.3b","cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":32,"cfg.d_model":2048,"cfg.d_vocab":50272,"cfg.act_fn":"relu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"OPTForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":64,"model_name":"opt-1.3b","n_heads":32,"d_mlp":8192,"act_fn":"relu","d_vocab":50272,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"OPTForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"facebook\/opt-1.3b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50272,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 64\nmodel_name: opt-1.3b\nn_heads: 32\nd_mlp: 8192\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-1.3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"facebook\/opt-1.3b","tokenizer.vocab_size":50265.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"f1FIzqnRiMYzke1CU0hp8TDxq7k","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2048, 64)\n      W_O: (32, 64, 2048)\n      '[b_Q, b_K, b_V]': (32, 64)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2048, 64)","W_O":"(32, 64, 2048)","[b_Q, b_K, b_V]":"(32, 64)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 64)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2048)"}}
{"name.default_alias":"opt-2.7b","name.huggingface":"facebook\/opt-2.7b","name.aliases":"opt-2.7b, opt-large","model_type":"opt","name.from_cfg":"opt-2.7b","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.7b","cfg.n_params":2516582400,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":2560,"cfg.d_vocab":50272,"cfg.act_fn":"relu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"OPTForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":2560,"n_ctx":2048,"d_head":80,"model_name":"opt-2.7b","n_heads":32,"d_mlp":10240,"act_fn":"relu","d_vocab":50272,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"OPTForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"facebook\/opt-2.7b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0158113883,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50272,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":2516582400,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: opt-2.7b\nn_heads: 32\nd_mlp: 10240\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-2.7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.015811388300841896\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"facebook\/opt-2.7b","tokenizer.vocab_size":50265.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"f1FIzqnRiMYzke1CU0hp8TDxq7k","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 2560)\npos_embed:\n  W_pos: (2048, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 2560)"},"pos_embed":{"W_pos":"(2048, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2560)"}}
{"name.default_alias":"opt-6.7b","name.huggingface":"facebook\/opt-6.7b","name.aliases":"opt-6.7b, opt-xl","model_type":"opt","name.from_cfg":"opt-6.7b","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.7b","cfg.n_params":6442450944,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":50272,"cfg.act_fn":"relu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"OPTForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"opt-6.7b","n_heads":32,"d_mlp":16384,"act_fn":"relu","d_vocab":50272,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"OPTForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"facebook\/opt-6.7b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50272,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":6442450944,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-6.7b\nn_heads: 32\nd_mlp: 16384\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-6.7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"facebook\/opt-6.7b","tokenizer.vocab_size":50265.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"f1FIzqnRiMYzke1CU0hp8TDxq7k","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 4096)\npos_embed:\n  W_pos: (2048, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 4096)"},"pos_embed":{"W_pos":"(2048, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 4096)"}}
{"name.default_alias":"opt-13b","name.huggingface":"facebook\/opt-13b","name.aliases":"opt-13b, opt-xxl","model_type":"opt","name.from_cfg":"opt-13b","n_params.as_str":"13B","n_params.as_int":12582912000,"n_params.from_name":"13b","cfg.n_params":12582912000,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":50272,"cfg.act_fn":"relu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"OPTForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"opt-13b","n_heads":40,"d_mlp":20480,"act_fn":"relu","d_vocab":50272,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"OPTForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"facebook\/opt-13b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50272,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912000,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-13b\nn_heads: 40\nd_mlp: 20480\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-13b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"facebook\/opt-13b","tokenizer.vocab_size":50265.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"f1FIzqnRiMYzke1CU0hp8TDxq7k","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 5120)\npos_embed:\n  W_pos: (2048, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 5120)"},"pos_embed":{"W_pos":"(2048, 5120)"},"blocks":{"[0-39]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 5120)"}}
{"name.default_alias":"opt-30b","name.huggingface":"facebook\/opt-30b","name.aliases":"opt-30b, opt-xxxl","model_type":"opt","name.from_cfg":"opt-30b","n_params.as_str":"30B","n_params.as_int":29595009024,"n_params.from_name":"30b","cfg.n_params":29595009024,"cfg.n_layers":48,"cfg.n_heads":56,"cfg.d_model":7168,"cfg.d_vocab":50272,"cfg.act_fn":"relu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"OPTForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":48,"d_model":7168,"n_ctx":2048,"d_head":128,"model_name":"opt-30b","n_heads":56,"d_mlp":28672,"act_fn":"relu","d_vocab":50272,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"OPTForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"facebook\/opt-30b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0094491118,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50272,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":29595009024,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 48\nd_model: 7168\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-30b\nn_heads: 56\nd_mlp: 28672\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-30b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.00944911182523068\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 29595009024\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"facebook\/opt-30b","tokenizer.vocab_size":50265.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"f1FIzqnRiMYzke1CU0hp8TDxq7k","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 7168)\npos_embed:\n  W_pos: (2048, 7168)\nblocks:\n  '[0-47]':\n    ln1:\n      '[w, b]': (7168,)\n    ln2:\n      '[w, b]': (7168,)\n    attn:\n      '[W_Q, W_K, W_V]': (56, 7168, 128)\n      W_O: (56, 128, 7168)\n      '[b_Q, b_K, b_V]': (56, 128)\n      b_O: (7168,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (7168, 28672)\n      b_in: (28672,)\n      W_out: (28672, 7168)\n      b_out: (7168,)\nln_final:\n  '[w, b]': (7168,)\nunembed:\n  W_U: (7168, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 7168)"},"pos_embed":{"W_pos":"(2048, 7168)"},"blocks":{"[0-47]":{"ln1":{"[w, b]":"(7168,)"},"ln2":{"[w, b]":"(7168,)"},"attn":{"[W_Q, W_K, W_V]":"(56, 7168, 128)","W_O":"(56, 128, 7168)","[b_Q, b_K, b_V]":"(56, 128)","b_O":"(7168,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(7168, 28672)","b_in":"(28672,)","W_out":"(28672, 7168)","b_out":"(7168,)"}}},"ln_final":{"[w, b]":"(7168,)"},"unembed":{"W_U":"(7168, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-47]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 56, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 56, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 28672)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 7168)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 7168)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 7168)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-47]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 56, 128)","[hook_attn_scores, hook_pattern]":"(batch, 56, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 28672)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 7168)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 7168)"}}
{"name.default_alias":"opt-66b","name.huggingface":"facebook\/opt-66b","name.aliases":"opt-66b, opt-xxxxl","model_type":"opt","name.from_cfg":"opt-66b","n_params.as_str":"65B","n_params.as_int":65229815808,"n_params.from_name":"66b","cfg.n_params":65229815808,"cfg.n_layers":64,"cfg.n_heads":72,"cfg.d_model":9216,"cfg.d_vocab":50272,"cfg.act_fn":"relu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"OPTForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":64,"d_model":9216,"n_ctx":2048,"d_head":128,"model_name":"opt-66b","n_heads":72,"d_mlp":36864,"act_fn":"relu","d_vocab":50272,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"OPTForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"facebook\/opt-66b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0083333333,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50272,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":65229815808,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 64\nd_model: 9216\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-66b\nn_heads: 72\nd_mlp: 36864\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-66b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.008333333333333333\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 65229815808\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"facebook\/opt-66b","tokenizer.vocab_size":50265.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"f1FIzqnRiMYzke1CU0hp8TDxq7k","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 9216)\npos_embed:\n  W_pos: (2048, 9216)\nblocks:\n  '[0-63]':\n    ln1:\n      '[w, b]': (9216,)\n    ln2:\n      '[w, b]': (9216,)\n    attn:\n      '[W_Q, W_K, W_V]': (72, 9216, 128)\n      W_O: (72, 128, 9216)\n      '[b_Q, b_K, b_V]': (72, 128)\n      b_O: (9216,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (9216, 36864)\n      b_in: (36864,)\n      W_out: (36864, 9216)\n      b_out: (9216,)\nln_final:\n  '[w, b]': (9216,)\nunembed:\n  W_U: (9216, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 9216)"},"pos_embed":{"W_pos":"(2048, 9216)"},"blocks":{"[0-63]":{"ln1":{"[w, b]":"(9216,)"},"ln2":{"[w, b]":"(9216,)"},"attn":{"[W_Q, W_K, W_V]":"(72, 9216, 128)","W_O":"(72, 128, 9216)","[b_Q, b_K, b_V]":"(72, 128)","b_O":"(9216,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(9216, 36864)","b_in":"(36864,)","W_out":"(36864, 9216)","b_out":"(9216,)"}}},"ln_final":{"[w, b]":"(9216,)"},"unembed":{"W_U":"(9216, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-63]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 9216)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 72, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 72, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 9216)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 36864)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 9216)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 9216)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 9216)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-63]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 9216)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 72, 128)","[hook_attn_scores, hook_pattern]":"(batch, 72, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 9216)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 36864)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 9216)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 9216)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 9216)"}}
{"name.default_alias":"gpt-neo-125M","name.huggingface":null,"name.aliases":"","model_type":"gpt-neo","name.from_cfg":"gpt-neo-125M","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"125M","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"gpt-neo-125M","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neo-125M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: gpt-neo-125M\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neo-125M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neo-125M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"gpt-neo-1.3B","name.huggingface":null,"name.aliases":"","model_type":"gpt-neo","name.from_cfg":"gpt-neo-1.3B","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.3B","cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"gpt-neo-1.3B","n_heads":16,"d_mlp":8192,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neo-1.3B","window_size":256,"attn_types":["global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: gpt-neo-1.3B\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neo-1.3B\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neo-1.3B","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2048)"}}
{"name.default_alias":"gpt-neo-2.7B","name.huggingface":null,"name.aliases":"","model_type":"gpt-neo","name.from_cfg":"gpt-neo-2.7B","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.7B","cfg.n_params":2516582400,"cfg.n_layers":32,"cfg.n_heads":20,"cfg.d_model":2560,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":2560,"n_ctx":2048,"d_head":128,"model_name":"gpt-neo-2.7B","n_heads":20,"d_mlp":10240,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neo-2.7B","window_size":256,"attn_types":["global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0158113883,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":2516582400,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 128\nmodel_name: gpt-neo-2.7B\nn_heads: 20\nd_mlp: 10240\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neo-2.7B\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.015811388300841896\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neo-2.7B","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 2560)\npos_embed:\n  W_pos: (2048, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 2560, 128)\n      W_O: (20, 128, 2560)\n      '[b_Q, b_K, b_V]': (20, 128)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 2560)"},"pos_embed":{"W_pos":"(2048, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 2560, 128)","W_O":"(20, 128, 2560)","[b_Q, b_K, b_V]":"(20, 128)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 128)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2560)"}}
{"name.default_alias":"gpt-j-6B","name.huggingface":null,"name.aliases":"","model_type":"gpt-j","name.from_cfg":"gpt-j-6B","n_params.as_str":"5.6B","n_params.as_int":5637144576,"n_params.from_name":"6B","cfg.n_params":5637144576,"cfg.n_layers":28,"cfg.n_heads":16,"cfg.d_model":4096,"cfg.d_vocab":50400,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTJForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":28,"d_model":4096,"n_ctx":2048,"d_head":256,"model_name":"gpt-j-6B","n_heads":16,"d_mlp":16384,"act_fn":"gelu_new","d_vocab":50400,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTJForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-j-6B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50400,"parallel_attn_mlp":true,"rotary_dim":64,"n_params":5637144576,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":true,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 28\nd_model: 4096\nn_ctx: 2048\nd_head: 256\nmodel_name: gpt-j-6B\nn_heads: 16\nd_mlp: 16384\nact_fn: gelu_new\nd_vocab: 50400\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTJForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-j-6B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50400\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 5637144576\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: true\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-j-6B","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"aKfp-BCA9d3W27qknxFiS0DGC5s","tensor_shapes.state_dict":"embed:\n  W_E: (50400, 4096)\nblocks:\n  '[0-27]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 4096, 256)\n      W_O: (16, 256, 4096)\n      '[b_Q, b_K, b_V]': (16, 256)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50400)\n  b_U: (50400,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50400, 4096)"},"blocks":{"[0-27]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 4096, 256)","W_O":"(16, 256, 4096)","[b_Q, b_K, b_V]":"(16, 256)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50400)","b_U":"(50400,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-27]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-27]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 256)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"gpt-neox-20b","name.huggingface":"EleutherAI\/gpt-neox-20b","name.aliases":"gpt-neox-20b, gpt-neox, neox","model_type":"gpt-neo","name.from_cfg":"gpt-neox-20b","n_params.as_str":"20B","n_params.as_int":19931332608,"n_params.from_name":"20b","cfg.n_params":19931332608,"cfg.n_layers":44,"cfg.n_heads":64,"cfg.d_model":6144,"cfg.d_vocab":50432,"cfg.act_fn":"gelu_fast","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":44,"d_model":6144,"n_ctx":2048,"d_head":96,"model_name":"gpt-neox-20b","n_heads":64,"d_mlp":24576,"act_fn":"gelu_fast","d_vocab":50432,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0102062073,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50432,"parallel_attn_mlp":true,"rotary_dim":24,"n_params":19931332608,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 44\nd_model: 6144\nn_ctx: 2048\nd_head: 96\nmodel_name: gpt-neox-20b\nn_heads: 64\nd_mlp: 24576\nact_fn: gelu_fast\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.010206207261596576\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 24\nn_params: 19931332608\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 6144)\nblocks:\n  '[0-43]':\n    ln1:\n      '[w, b]': (6144,)\n    ln2:\n      '[w, b]': (6144,)\n    attn:\n      '[W_Q, W_K, W_V]': (64, 6144, 96)\n      W_O: (64, 96, 6144)\n      '[b_Q, b_K, b_V]': (64, 96)\n      b_O: (6144,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 24)\n    mlp:\n      W_in: (6144, 24576)\n      b_in: (24576,)\n      W_out: (24576, 6144)\n      b_out: (6144,)\nln_final:\n  '[w, b]': (6144,)\nunembed:\n  W_U: (6144, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 6144)"},"blocks":{"[0-43]":{"ln1":{"[w, b]":"(6144,)"},"ln2":{"[w, b]":"(6144,)"},"attn":{"[W_Q, W_K, W_V]":"(64, 6144, 96)","W_O":"(64, 96, 6144)","[b_Q, b_K, b_V]":"(64, 96)","b_O":"(6144,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 24)"},"mlp":{"W_in":"(6144, 24576)","b_in":"(24576,)","W_out":"(24576, 6144)","b_out":"(6144,)"}}},"ln_final":{"[w, b]":"(6144,)"},"unembed":{"W_U":"(6144, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-43]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        64, 96)\n      '[hook_attn_scores, hook_pattern]': (batch, 64, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      6144)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 6144)\nhook_embed: (batch, seq_len, 6144)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-43]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 64, 96)","[hook_attn_scores, hook_pattern]":"(batch, 64, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 6144)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"hook_embed":"(batch, seq_len, 6144)"}}
{"name.default_alias":"stanford-gpt2-small-a","name.huggingface":"stanford-crfm\/alias-gpt2-small-x21","name.aliases":"stanford-gpt2-small-a, alias-gpt2-small-x21, gpt2-mistral-small-a, gpt2-stanford-small-a","model_type":"gpt2","name.from_cfg":"alias-gpt2-small-x21","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"alias-gpt2-small-x21","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/alias-gpt2-small-x21","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: alias-gpt2-small-x21\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/alias-gpt2-small-x21\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/alias-gpt2-small-x21","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-b","name.huggingface":"stanford-crfm\/battlestar-gpt2-small-x49","name.aliases":"stanford-gpt2-small-b, battlestar-gpt2-small-x49, gpt2-mistral-small-b, gpt2-mistral-small-b","model_type":"gpt2","name.from_cfg":"battlestar-gpt2-small-x49","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"battlestar-gpt2-small-x49","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/battlestar-gpt2-small-x49","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: battlestar-gpt2-small-x49\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/battlestar-gpt2-small-x49\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/battlestar-gpt2-small-x49","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-c","name.huggingface":"stanford-crfm\/caprica-gpt2-small-x81","name.aliases":"stanford-gpt2-small-c, caprica-gpt2-small-x81, gpt2-mistral-small-c, gpt2-stanford-small-c","model_type":"gpt2","name.from_cfg":"caprica-gpt2-small-x81","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"caprica-gpt2-small-x81","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/caprica-gpt2-small-x81","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: caprica-gpt2-small-x81\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/caprica-gpt2-small-x81\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/caprica-gpt2-small-x81","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-d","name.huggingface":"stanford-crfm\/darkmatter-gpt2-small-x343","name.aliases":"stanford-gpt2-small-d, darkmatter-gpt2-small-x343, gpt2-mistral-small-d, gpt2-mistral-small-d","model_type":"gpt2","name.from_cfg":"darkmatter-gpt2-small-x343","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"darkmatter-gpt2-small-x343","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/darkmatter-gpt2-small-x343","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: darkmatter-gpt2-small-x343\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/darkmatter-gpt2-small-x343\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/darkmatter-gpt2-small-x343","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-e","name.huggingface":"stanford-crfm\/expanse-gpt2-small-x777","name.aliases":"stanford-gpt2-small-e, expanse-gpt2-small-x777, gpt2-mistral-small-e, gpt2-mistral-small-e","model_type":"gpt2","name.from_cfg":"expanse-gpt2-small-x777","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"expanse-gpt2-small-x777","n_heads":12,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/expanse-gpt2-small-x777","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: expanse-gpt2-small-x777\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/expanse-gpt2-small-x777\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/expanse-gpt2-small-x777","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-medium-a","name.huggingface":"stanford-crfm\/arwen-gpt2-medium-x21","name.aliases":"stanford-gpt2-medium-a, arwen-gpt2-medium-x21, gpt2-medium-small-a, gpt2-stanford-medium-a","model_type":"gpt2","name.from_cfg":"arwen-gpt2-medium-x21","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"arwen-gpt2-medium-x21","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/arwen-gpt2-medium-x21","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: arwen-gpt2-medium-x21\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/arwen-gpt2-medium-x21\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/arwen-gpt2-medium-x21","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-b","name.huggingface":"stanford-crfm\/beren-gpt2-medium-x49","name.aliases":"stanford-gpt2-medium-b, beren-gpt2-medium-x49, gpt2-medium-small-b, gpt2-stanford-medium-b","model_type":"gpt2","name.from_cfg":"beren-gpt2-medium-x49","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"beren-gpt2-medium-x49","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/beren-gpt2-medium-x49","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: beren-gpt2-medium-x49\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/beren-gpt2-medium-x49\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/beren-gpt2-medium-x49","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-c","name.huggingface":"stanford-crfm\/celebrimbor-gpt2-medium-x81","name.aliases":"stanford-gpt2-medium-c, celebrimbor-gpt2-medium-x81, gpt2-medium-small-c, gpt2-medium-small-c","model_type":"gpt2","name.from_cfg":"celebrimbor-gpt2-medium-x81","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"celebrimbor-gpt2-medium-x81","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/celebrimbor-gpt2-medium-x81","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: celebrimbor-gpt2-medium-x81\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/celebrimbor-gpt2-medium-x81\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/celebrimbor-gpt2-medium-x81","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-d","name.huggingface":"stanford-crfm\/durin-gpt2-medium-x343","name.aliases":"stanford-gpt2-medium-d, durin-gpt2-medium-x343, gpt2-medium-small-d, gpt2-stanford-medium-d","model_type":"gpt2","name.from_cfg":"durin-gpt2-medium-x343","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"durin-gpt2-medium-x343","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/durin-gpt2-medium-x343","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: durin-gpt2-medium-x343\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/durin-gpt2-medium-x343\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/durin-gpt2-medium-x343","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-e","name.huggingface":"stanford-crfm\/eowyn-gpt2-medium-x777","name.aliases":"stanford-gpt2-medium-e, eowyn-gpt2-medium-x777, gpt2-medium-small-e, gpt2-stanford-medium-e","model_type":"gpt2","name.from_cfg":"eowyn-gpt2-medium-x777","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"eowyn-gpt2-medium-x777","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stanford-crfm\/eowyn-gpt2-medium-x777","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":true,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: eowyn-gpt2-medium-x777\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/eowyn-gpt2-medium-x777\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stanford-crfm\/eowyn-gpt2-medium-x777","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-14m","name.huggingface":"EleutherAI\/pythia-14m","name.aliases":"pythia-14m","model_type":"pythia","name.from_cfg":"pythia-14m","n_params.as_str":"1.2M","n_params.as_int":1179648,"n_params.from_name":"14m","cfg.n_params":1179648,"cfg.n_layers":6,"cfg.n_heads":4,"cfg.d_model":128,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":128,"n_ctx":2048,"d_head":32,"model_name":"pythia-14m","n_heads":4,"d_mlp":512,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-14m","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0707106781,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":8,"n_params":1179648,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 128\nn_ctx: 2048\nd_head: 32\nmodel_name: pythia-14m\nn_heads: 4\nd_mlp: 512\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-14m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.07071067811865475\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 8\nn_params: 1179648\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-14m","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 128)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (128,)\n    ln2:\n      '[w, b]': (128,)\n    attn:\n      '[W_Q, W_K, W_V]': (4, 128, 32)\n      W_O: (4, 32, 128)\n      '[b_Q, b_K, b_V]': (4, 32)\n      b_O: (128,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 8)\n    mlp:\n      W_in: (128, 512)\n      b_in: (512,)\n      W_out: (512, 128)\n      b_out: (128,)\nln_final:\n  '[w, b]': (128,)\nunembed:\n  W_U: (128, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 128)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(128,)"},"ln2":{"[w, b]":"(128,)"},"attn":{"[W_Q, W_K, W_V]":"(4, 128, 32)","W_O":"(4, 32, 128)","[b_Q, b_K, b_V]":"(4, 32)","b_O":"(128,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 8)"},"mlp":{"W_in":"(128, 512)","b_in":"(512,)","W_out":"(512, 128)","b_out":"(128,)"}}},"ln_final":{"[w, b]":"(128,)"},"unembed":{"W_U":"(128, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        4, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 4, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 512)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      128)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 128)\nhook_embed: (batch, seq_len, 128)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 4, 32)","[hook_attn_scores, hook_pattern]":"(batch, 4, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 512)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 128)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"hook_embed":"(batch, seq_len, 128)"}}
{"name.default_alias":"pythia-31m","name.huggingface":"EleutherAI\/pythia-31m","name.aliases":"pythia-31m","model_type":"pythia","name.from_cfg":"pythia-31m","n_params.as_str":"4.7M","n_params.as_int":4718592,"n_params.from_name":"31m","cfg.n_params":4718592,"cfg.n_layers":6,"cfg.n_heads":8,"cfg.d_model":256,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":256,"n_ctx":2048,"d_head":32,"model_name":"pythia-31m","n_heads":8,"d_mlp":1024,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-31m","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.05,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":8,"n_params":4718592,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 256\nn_ctx: 2048\nd_head: 32\nmodel_name: pythia-31m\nn_heads: 8\nd_mlp: 1024\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-31m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.05\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 8\nn_params: 4718592\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-31m","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 256)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (256,)\n    ln2:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 256, 32)\n      W_O: (8, 32, 256)\n      '[b_Q, b_K, b_V]': (8, 32)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 8)\n    mlp:\n      W_in: (256, 1024)\n      b_in: (1024,)\n      W_out: (1024, 256)\n      b_out: (256,)\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 256)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(256,)"},"ln2":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 256, 32)","W_O":"(8, 32, 256)","[b_Q, b_K, b_V]":"(8, 32)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 8)"},"mlp":{"W_in":"(256, 1024)","b_in":"(1024,)","W_out":"(1024, 256)","b_out":"(256,)"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 1024)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\nhook_embed: (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 32)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 1024)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"hook_embed":"(batch, seq_len, 256)"}}
{"name.default_alias":"pythia-70m","name.huggingface":"EleutherAI\/pythia-70m","name.aliases":"pythia-70m, pythia, EleutherAI\/pythia-19m, pythia-19m","model_type":"pythia","name.from_cfg":"pythia-70m","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","cfg.n_params":18874368,"cfg.n_layers":6,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":512,"n_ctx":2048,"d_head":64,"model_name":"pythia-70m","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-70m","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":18874368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-70m","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m","name.huggingface":"EleutherAI\/pythia-160m","name.aliases":"pythia-160m, EleutherAI\/pythia-125m, pythia-125m","model_type":"pythia","name.from_cfg":"pythia-160m","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"pythia-160m","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-160m","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-160m","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m","name.huggingface":"EleutherAI\/pythia-410m","name.aliases":"pythia-410m, EleutherAI\/pythia-350m, pythia-350m","model_type":"pythia","name.from_cfg":"pythia-410m","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"pythia-410m","n_heads":16,"d_mlp":4096,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-410m","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-410m","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b","name.huggingface":"EleutherAI\/pythia-1b","name.aliases":"pythia-1b, EleutherAI\/pythia-800m, pythia-800m","model_type":"pythia","name.from_cfg":"pythia-1b","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","cfg.n_params":805306368,"cfg.n_layers":16,"cfg.n_heads":8,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":2048,"n_ctx":2048,"d_head":256,"model_name":"pythia-1b","n_heads":8,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":64,"n_params":805306368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b","name.huggingface":"EleutherAI\/pythia-1.4b","name.aliases":"pythia-1.4b, EleutherAI\/pythia-1.3b, pythia-1.3b","model_type":"pythia","name.from_cfg":"pythia-1.4b","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"pythia-1.4b","n_heads":16,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1.4b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1.4b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b","name.huggingface":"EleutherAI\/pythia-2.8b","name.aliases":"pythia-2.8b, EleutherAI\/pythia-2.7b, pythia-2.7b","model_type":"pythia","name.from_cfg":"pythia-2.8b","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","cfg.n_params":2516582400,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":2560,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":2560,"n_ctx":2048,"d_head":80,"model_name":"pythia-2.8b","n_heads":32,"d_mlp":10240,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-2.8b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0158113883,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":20,"n_params":2516582400,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.015811388300841896\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-2.8b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b","name.huggingface":"EleutherAI\/pythia-6.9b","name.aliases":"pythia-6.9b, EleutherAI\/pythia-6.7b, pythia-6.7b","model_type":"pythia","name.from_cfg":"pythia-6.9b","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","cfg.n_params":6442450944,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":50432,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"pythia-6.9b","n_heads":32,"d_mlp":16384,"act_fn":"gelu","d_vocab":50432,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-6.9b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50432,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":6442450944,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-6.9b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b","name.huggingface":"EleutherAI\/pythia-12b","name.aliases":"pythia-12b, EleutherAI\/pythia-13b, pythia-13b","model_type":"pythia","name.from_cfg":"pythia-12b","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","cfg.n_params":11324620800,"cfg.n_layers":36,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":50688,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":36,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"pythia-12b","n_heads":40,"d_mlp":20480,"act_fn":"gelu","d_vocab":50688,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-12b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50688,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":11324620800,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-12b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-70m-deduped","name.huggingface":"EleutherAI\/pythia-70m-deduped","name.aliases":"pythia-70m-deduped, EleutherAI\/pythia-19m-deduped, pythia-19m-deduped","model_type":"pythia","name.from_cfg":"pythia-70m-deduped","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","cfg.n_params":18874368,"cfg.n_layers":6,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":512,"n_ctx":2048,"d_head":64,"model_name":"pythia-70m-deduped","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-70m-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":18874368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m-deduped\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-70m-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m-deduped","name.huggingface":"EleutherAI\/pythia-160m-deduped","name.aliases":"pythia-160m-deduped, EleutherAI\/pythia-125m-deduped, pythia-125m-deduped","model_type":"pythia","name.from_cfg":"pythia-160m-deduped","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"pythia-160m-deduped","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-160m-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-deduped\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-160m-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m-deduped","name.huggingface":"EleutherAI\/pythia-410m-deduped","name.aliases":"pythia-410m-deduped, EleutherAI\/pythia-350m-deduped, pythia-350m-deduped","model_type":"pythia","name.from_cfg":"pythia-410m-deduped","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"pythia-410m-deduped","n_heads":16,"d_mlp":4096,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-410m-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m-deduped\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-410m-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b-deduped","name.huggingface":"EleutherAI\/pythia-1b-deduped","name.aliases":"pythia-1b-deduped, EleutherAI\/pythia-800m-deduped, pythia-800m-deduped","model_type":"pythia","name.from_cfg":"pythia-1b-deduped","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","cfg.n_params":805306368,"cfg.n_layers":16,"cfg.n_heads":8,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":2048,"n_ctx":2048,"d_head":256,"model_name":"pythia-1b-deduped","n_heads":8,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1b-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":64,"n_params":805306368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b-deduped\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1b-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b-deduped","name.huggingface":"EleutherAI\/pythia-1.4b-deduped","name.aliases":"pythia-1.4b-deduped, EleutherAI\/pythia-1.3b-deduped, pythia-1.3b-deduped","model_type":"pythia","name.from_cfg":"pythia-1.4b-deduped","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"pythia-1.4b-deduped","n_heads":16,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1.4b-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b-deduped\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1.4b-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b-deduped","name.huggingface":"EleutherAI\/pythia-2.8b-deduped","name.aliases":"pythia-2.8b-deduped, EleutherAI\/pythia-2.7b-deduped, pythia-2.7b-deduped","model_type":"pythia","name.from_cfg":"pythia-2.8b-deduped","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","cfg.n_params":2516582400,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":2560,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":2560,"n_ctx":2048,"d_head":80,"model_name":"pythia-2.8b-deduped","n_heads":32,"d_mlp":10240,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-2.8b-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0158113883,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":20,"n_params":2516582400,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b-deduped\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.015811388300841896\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-2.8b-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b-deduped","name.huggingface":"EleutherAI\/pythia-6.9b-deduped","name.aliases":"pythia-6.9b-deduped, EleutherAI\/pythia-6.7b-deduped, pythia-6.7b-deduped","model_type":"pythia","name.from_cfg":"pythia-6.9b-deduped","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","cfg.n_params":6442450944,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":50432,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"pythia-6.9b-deduped","n_heads":32,"d_mlp":16384,"act_fn":"gelu","d_vocab":50432,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-6.9b-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50432,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":6442450944,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b-deduped\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-6.9b-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b-deduped","name.huggingface":"EleutherAI\/pythia-12b-deduped","name.aliases":"pythia-12b-deduped, EleutherAI\/pythia-13b-deduped, pythia-13b-deduped","model_type":"pythia","name.from_cfg":"pythia-12b-deduped","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","cfg.n_params":11324620800,"cfg.n_layers":36,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":50688,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":36,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"pythia-12b-deduped","n_heads":40,"d_mlp":20480,"act_fn":"gelu","d_vocab":50688,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-12b-deduped","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50688,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":11324620800,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b-deduped\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-12b-deduped","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-70m-v0","name.huggingface":"EleutherAI\/pythia-70m-v0","name.aliases":"pythia-70m-v0, pythia-v0, EleutherAI\/pythia-19m-v0, pythia-19m-v0","model_type":"pythia","name.from_cfg":"pythia-70m-v0","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","cfg.n_params":18874368,"cfg.n_layers":6,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":512,"n_ctx":2048,"d_head":64,"model_name":"pythia-70m-v0","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-70m-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":18874368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m-v0\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-70m-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m-v0","name.huggingface":"EleutherAI\/pythia-160m-v0","name.aliases":"pythia-160m-v0, EleutherAI\/pythia-125m-v0, pythia-125m-v0","model_type":"pythia","name.from_cfg":"pythia-160m-v0","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"pythia-160m-v0","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-160m-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-v0\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-160m-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m-v0","name.huggingface":"EleutherAI\/pythia-410m-v0","name.aliases":"pythia-410m-v0, EleutherAI\/pythia-350m-v0, pythia-350m-v0","model_type":"pythia","name.from_cfg":"pythia-410m-v0","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"pythia-410m-v0","n_heads":16,"d_mlp":4096,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-410m-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m-v0\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-410m-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b-v0","name.huggingface":"EleutherAI\/pythia-1b-v0","name.aliases":"pythia-1b-v0, EleutherAI\/pythia-800m-v0, pythia-800m-v0","model_type":"pythia","name.from_cfg":"pythia-1b-v0","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","cfg.n_params":805306368,"cfg.n_layers":16,"cfg.n_heads":8,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":2048,"n_ctx":2048,"d_head":256,"model_name":"pythia-1b-v0","n_heads":8,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1b-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":64,"n_params":805306368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b-v0\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1b-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b-v0","name.huggingface":"EleutherAI\/pythia-1.4b-v0","name.aliases":"pythia-1.4b-v0, EleutherAI\/pythia-1.3b-v0, pythia-1.3b-v0","model_type":"pythia","name.from_cfg":"pythia-1.4b-v0","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"pythia-1.4b-v0","n_heads":16,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1.4b-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b-v0\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1.4b-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b-v0","name.huggingface":"EleutherAI\/pythia-2.8b-v0","name.aliases":"pythia-2.8b-v0, EleutherAI\/pythia-2.7b-v0, pythia-2.7b-v0","model_type":"pythia","name.from_cfg":"pythia-2.8b-v0","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","cfg.n_params":2516582400,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":2560,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":2560,"n_ctx":2048,"d_head":80,"model_name":"pythia-2.8b-v0","n_heads":32,"d_mlp":10240,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-2.8b-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0158113883,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":20,"n_params":2516582400,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b-v0\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.015811388300841896\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-2.8b-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b-v0","name.huggingface":"EleutherAI\/pythia-6.9b-v0","name.aliases":"pythia-6.9b-v0, EleutherAI\/pythia-6.7b-v0, pythia-6.7b-v0","model_type":"pythia","name.from_cfg":"pythia-6.9b-v0","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","cfg.n_params":6442450944,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":50432,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"pythia-6.9b-v0","n_heads":32,"d_mlp":16384,"act_fn":"gelu","d_vocab":50432,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-6.9b-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50432,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":6442450944,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b-v0\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-6.9b-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b-v0","name.huggingface":"EleutherAI\/pythia-12b-v0","name.aliases":"pythia-12b-v0, EleutherAI\/pythia-13b-v0, pythia-13b-v0","model_type":"pythia","name.from_cfg":"pythia-12b-v0","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","cfg.n_params":11324620800,"cfg.n_layers":36,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":50688,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":36,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"pythia-12b-v0","n_heads":40,"d_mlp":20480,"act_fn":"gelu","d_vocab":50688,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-12b-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50688,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":11324620800,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b-v0\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-12b-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-70m-deduped-v0","name.huggingface":"EleutherAI\/pythia-70m-deduped-v0","name.aliases":"pythia-70m-deduped-v0, EleutherAI\/pythia-19m-deduped-v0, pythia-19m-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-70m-deduped-v0","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","cfg.n_params":18874368,"cfg.n_layers":6,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":512,"n_ctx":2048,"d_head":64,"model_name":"pythia-70m-deduped-v0","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-70m-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":18874368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m-deduped-v0\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-70m-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m-deduped-v0","name.huggingface":"EleutherAI\/pythia-160m-deduped-v0","name.aliases":"pythia-160m-deduped-v0, EleutherAI\/pythia-125m-deduped-v0, pythia-125m-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-160m-deduped-v0","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"pythia-160m-deduped-v0","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-160m-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-deduped-v0\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-160m-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m-deduped-v0","name.huggingface":"EleutherAI\/pythia-410m-deduped-v0","name.aliases":"pythia-410m-deduped-v0, EleutherAI\/pythia-350m-deduped-v0, pythia-350m-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-410m-deduped-v0","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"pythia-410m-deduped-v0","n_heads":16,"d_mlp":4096,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-410m-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m-deduped-v0\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-410m-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b-deduped-v0","name.huggingface":"EleutherAI\/pythia-1b-deduped-v0","name.aliases":"pythia-1b-deduped-v0, EleutherAI\/pythia-800m-deduped-v0, pythia-800m-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-1b-deduped-v0","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","cfg.n_params":805306368,"cfg.n_layers":16,"cfg.n_heads":8,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":2048,"n_ctx":2048,"d_head":256,"model_name":"pythia-1b-deduped-v0","n_heads":8,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1b-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":64,"n_params":805306368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b-deduped-v0\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1b-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b-deduped-v0","name.huggingface":"EleutherAI\/pythia-1.4b-deduped-v0","name.aliases":"pythia-1.4b-deduped-v0, EleutherAI\/pythia-1.3b-deduped-v0, pythia-1.3b-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-1.4b-deduped-v0","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"pythia-1.4b-deduped-v0","n_heads":16,"d_mlp":8192,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-1.4b-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b-deduped-v0\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-1.4b-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b-deduped-v0","name.huggingface":"EleutherAI\/pythia-2.8b-deduped-v0","name.aliases":"pythia-2.8b-deduped-v0, EleutherAI\/pythia-2.7b-deduped-v0, pythia-2.7b-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-2.8b-deduped-v0","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","cfg.n_params":2516582400,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":2560,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":2560,"n_ctx":2048,"d_head":80,"model_name":"pythia-2.8b-deduped-v0","n_heads":32,"d_mlp":10240,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-2.8b-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0158113883,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":20,"n_params":2516582400,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b-deduped-v0\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.015811388300841896\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-2.8b-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b-deduped-v0","name.huggingface":"EleutherAI\/pythia-6.9b-deduped-v0","name.aliases":"pythia-6.9b-deduped-v0, EleutherAI\/pythia-6.7b-deduped-v0, pythia-6.7b-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-6.9b-deduped-v0","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","cfg.n_params":6442450944,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":50432,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"pythia-6.9b-deduped-v0","n_heads":32,"d_mlp":16384,"act_fn":"gelu","d_vocab":50432,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-6.9b-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50432,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":6442450944,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b-deduped-v0\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-6.9b-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b-deduped-v0","name.huggingface":"EleutherAI\/pythia-12b-deduped-v0","name.aliases":"pythia-12b-deduped-v0, EleutherAI\/pythia-13b-deduped-v0, pythia-13b-deduped-v0","model_type":"pythia","name.from_cfg":"pythia-12b-deduped-v0","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","cfg.n_params":11324620800,"cfg.n_layers":36,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":50688,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":36,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"pythia-12b-deduped-v0","n_heads":40,"d_mlp":20480,"act_fn":"gelu","d_vocab":50688,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-12b-deduped-v0","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50688,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":11324620800,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b-deduped-v0\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-12b-deduped-v0","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-160m-seed1","name.huggingface":"EleutherAI\/pythia-160m-seed1","name.aliases":"pythia-160m-seed1, EleutherAI\/pythia-125m-seed1, pythia-125m-seed1","model_type":"pythia","name.from_cfg":"pythia-160m-seed1","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"pythia-160m-seed1","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-160m-seed1","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-seed1\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-seed1\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-160m-seed1","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-160m-seed2","name.huggingface":"EleutherAI\/pythia-160m-seed2","name.aliases":"pythia-160m-seed2, EleutherAI\/pythia-125m-seed2, pythia-125m-seed2","model_type":"pythia","name.from_cfg":"pythia-160m-seed2","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"pythia-160m-seed2","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-160m-seed2","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-seed2\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-seed2\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-160m-seed2","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-160m-seed3","name.huggingface":"EleutherAI\/pythia-160m-seed3","name.aliases":"pythia-160m-seed3, EleutherAI\/pythia-125m-seed3, pythia-125m-seed3","model_type":"pythia","name.from_cfg":"pythia-160m-seed3","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50304,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":2048,"d_head":64,"model_name":"pythia-160m-seed3","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":50304,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/pythia-160m-seed3","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50304,"parallel_attn_mlp":true,"rotary_dim":16,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-seed3\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-seed3\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/pythia-160m-seed3","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"solu-1l-pile","name.huggingface":"NeelNanda\/SoLU_1L_v9_old","name.aliases":"solu-1l-pile, solu-1l-old","model_type":"solu","name.from_cfg":"SoLU_1L_v9_old","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"cfg.n_params":12582912,"cfg.n_layers":1,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50278,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel-solu-old","cfg.normalization_type":"LN","config.raw__":{"n_layers":1,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"SoLU_1L_v9_old","n_heads":16,"d_mlp":4096,"act_fn":"solu_ln","d_vocab":50278,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel-solu-old","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":true,"d_vocab_out":50278,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 1\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_1L_v9_old\nn_heads: 16\nd_mlp: 4096\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (4096,)\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  w: (1024,)\nunembed:\n  W_U: (1024, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"0":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(4096,)"},"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"w":"(1024,)"},"unembed":{"W_U":"(1024, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 4096)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"solu-2l-pile","name.huggingface":"NeelNanda\/SoLU_2L_v10_old","name.aliases":"solu-2l-pile, solu-2l-old","model_type":"solu","name.from_cfg":"SoLU_2L_v10_old","n_params.as_str":"13M","n_params.as_int":12812288,"n_params.from_name":null,"cfg.n_params":12812288,"cfg.n_layers":2,"cfg.n_heads":11,"cfg.d_model":736,"cfg.d_vocab":50278,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel-solu-old","cfg.normalization_type":"LNPre","config.raw__":{"n_layers":2,"d_model":736,"n_ctx":1024,"d_head":64,"model_name":"SoLU_2L_v10_old","n_heads":11,"d_mlp":2944,"act_fn":"solu_ln","d_vocab":50278,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel-solu-old","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LNPre","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0294883912,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":true,"d_vocab_out":50278,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12812288,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 736\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_2L_v10_old\nn_heads: 11\nd_mlp: 2944\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02948839123097943\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12812288\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 736)\npos_embed:\n  W_pos: (1024, 736)\nblocks:\n  '[0-1]':\n    attn:\n      '[W_Q, W_K, W_V]': (11, 736, 64)\n      W_O: (11, 64, 736)\n      '[b_Q, b_K, b_V]': (11, 64)\n      b_O: (736,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (736, 2944)\n      b_in: (2944,)\n      W_out: (2944, 736)\n      b_out: (736,)\nunembed:\n  W_U: (736, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 736)"},"pos_embed":{"W_pos":"(1024, 736)"},"blocks":{"[0-1]":{"attn":{"[W_Q, W_K, W_V]":"(11, 736, 64)","W_O":"(11, 64, 736)","[b_Q, b_K, b_V]":"(11, 64)","b_O":"(736,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(736, 2944)","b_in":"(2944,)","W_out":"(2944, 736)","b_out":"(736,)"}}},"unembed":{"W_U":"(736, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 736)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 11, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 11, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 736)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2944)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2944)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 736)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 736)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 736)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 736)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 11, 64)","[hook_attn_scores, hook_pattern]":"(batch, 11, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 736)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2944)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2944)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 736)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 736)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 736)"}}
{"name.default_alias":"solu-4l-pile","name.huggingface":"NeelNanda\/SoLU_4L_v11_old","name.aliases":"solu-4l-pile, solu-4l-old","model_type":"solu","name.from_cfg":"SoLU_4L_v11_old","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"cfg.n_params":12582912,"cfg.n_layers":4,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":50278,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel-solu-old","cfg.normalization_type":"LNPre","config.raw__":{"n_layers":4,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"SoLU_4L_v11_old","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":50278,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel-solu-old","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LNPre","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":true,"d_vocab_out":50278,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_4L_v11_old\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nunembed:\n  W_U: (512, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"unembed":{"W_U":"(512, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-6l-pile","name.huggingface":"NeelNanda\/SoLU_6L_v13_old","name.aliases":"solu-6l-pile, solu-6l-old","model_type":"solu","name.from_cfg":"SoLU_6L_v13_old","n_params.as_str":"42M","n_params.as_int":42467328,"n_params.from_name":null,"cfg.n_params":42467328,"cfg.n_layers":6,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":50278,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel-solu-old","cfg.normalization_type":"LNPre","config.raw__":{"n_layers":6,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"SoLU_6L_v13_old","n_heads":12,"d_mlp":3072,"act_fn":"solu_ln","d_vocab":50278,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel-solu-old","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LNPre","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":true,"d_vocab_out":50278,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":42467328,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_6L_v13_old\nn_heads: 12\nd_mlp: 3072\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 42467328\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-5]':\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nunembed:\n  W_U: (768, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-5]":{"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"unembed":{"W_U":"(768, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 3072)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"solu-8l-pile","name.huggingface":"NeelNanda\/SoLU_8L_v21_old","name.aliases":"solu-8l-pile, solu-8l-old","model_type":"solu","name.from_cfg":"SoLU_8L_v21_old","n_params.as_str":"101M","n_params.as_int":100663296,"n_params.from_name":null,"cfg.n_params":100663296,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50278,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel-solu-old","cfg.normalization_type":"LNPre","config.raw__":{"n_layers":8,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"SoLU_8L_v21_old","n_heads":16,"d_mlp":4096,"act_fn":"solu_ln","d_vocab":50278,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel-solu-old","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LNPre","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50278,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":100663296,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_8L_v21_old\nn_heads: 16\nd_mlp: 4096\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 100663296\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-7]':\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nunembed:\n  W_U: (1024, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-7]":{"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"unembed":{"W_U":"(1024, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 4096)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"solu-10l-pile","name.huggingface":"NeelNanda\/SoLU_10L_v22_old","name.aliases":"solu-10l-pile, solu-10l-old","model_type":"solu","name.from_cfg":"SoLU_10L_v22_old","n_params.as_str":"197M","n_params.as_int":196608000,"n_params.from_name":null,"cfg.n_params":196608000,"cfg.n_layers":10,"cfg.n_heads":20,"cfg.d_model":1280,"cfg.d_vocab":50278,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel-solu-old","cfg.normalization_type":"LNPre","config.raw__":{"n_layers":10,"d_model":1280,"n_ctx":1024,"d_head":64,"model_name":"SoLU_10L_v22_old","n_heads":20,"d_mlp":5120,"act_fn":"solu_ln","d_vocab":50278,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel-solu-old","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LNPre","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0223606798,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50278,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":196608000,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 10\nd_model: 1280\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_10L_v22_old\nn_heads: 20\nd_mlp: 5120\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.022360679774997897\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 196608000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1280)\npos_embed:\n  W_pos: (1024, 1280)\nblocks:\n  '[0-9]':\n    attn:\n      '[W_Q, W_K, W_V]': (20, 1280, 64)\n      W_O: (20, 64, 1280)\n      '[b_Q, b_K, b_V]': (20, 64)\n      b_O: (1280,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1280, 5120)\n      b_in: (5120,)\n      W_out: (5120, 1280)\n      b_out: (1280,)\nunembed:\n  W_U: (1280, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1280)"},"pos_embed":{"W_pos":"(1024, 1280)"},"blocks":{"[0-9]":{"attn":{"[W_Q, W_K, W_V]":"(20, 1280, 64)","W_O":"(20, 64, 1280)","[b_Q, b_K, b_V]":"(20, 64)","b_O":"(1280,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1280, 5120)","b_in":"(5120,)","W_out":"(5120, 1280)","b_out":"(1280,)"}}},"unembed":{"W_U":"(1280, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-9]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 5120)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 5120)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1280)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1280)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-9]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 64)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 5120)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1280)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1280)"}}
{"name.default_alias":"solu-12l-pile","name.huggingface":"NeelNanda\/SoLU_12L_v23_old","name.aliases":"solu-12l-pile, solu-12l-old","model_type":"solu","name.from_cfg":"SoLU_12L_v23_old","n_params.as_str":"340M","n_params.as_int":339738624,"n_params.from_name":null,"cfg.n_params":339738624,"cfg.n_layers":12,"cfg.n_heads":24,"cfg.d_model":1536,"cfg.d_vocab":50278,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel-solu-old","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":1536,"n_ctx":1024,"d_head":64,"model_name":"SoLU_12L_v23_old","n_heads":24,"d_mlp":6144,"act_fn":"solu_ln","d_vocab":50278,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel-solu-old","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0204124145,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50278,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":339738624,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 1536\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_12L_v23_old\nn_heads: 24\nd_mlp: 6144\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.020412414523193152\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 339738624\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1536)\npos_embed:\n  W_pos: (1024, 1536)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (1536,)\n    ln2:\n      '[w, b]': (1536,)\n    attn:\n      '[W_Q, W_K, W_V]': (24, 1536, 64)\n      W_O: (24, 64, 1536)\n      '[b_Q, b_K, b_V]': (24, 64)\n      b_O: (1536,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (6144,)\n      W_in: (1536, 6144)\n      b_in: (6144,)\n      W_out: (6144, 1536)\n      b_out: (1536,)\nln_final:\n  '[w, b]': (1536,)\nunembed:\n  W_U: (1536, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1536)"},"pos_embed":{"W_pos":"(1024, 1536)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(1536,)"},"ln2":{"[w, b]":"(1536,)"},"attn":{"[W_Q, W_K, W_V]":"(24, 1536, 64)","W_O":"(24, 64, 1536)","[b_Q, b_K, b_V]":"(24, 64)","b_O":"(1536,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(6144,)"},"W_in":"(1536, 6144)","b_in":"(6144,)","W_out":"(6144, 1536)","b_out":"(1536,)"}}},"ln_final":{"[w, b]":"(1536,)"},"unembed":{"W_U":"(1536, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 24, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 24, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 6144)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 6144)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1536)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1536)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1536)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 24, 64)","[hook_attn_scores, hook_pattern]":"(batch, 24, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 6144)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1536)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1536)"}}
{"name.default_alias":"solu-1l","name.huggingface":"NeelNanda\/SoLU_1L512W_C4_Code","name.aliases":"solu-1l, solu-1l-new, solu-1l-c4-code","model_type":"solu","name.from_cfg":"SoLU_1L512W_C4_Code","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"cfg.n_params":3145728,"cfg.n_layers":1,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":1,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"SoLU_1L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":3145728,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_1L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-2l","name.huggingface":"NeelNanda\/SoLU_2L512W_C4_Code","name.aliases":"solu-2l, solu-2l-new, solu-2l-c4-code","model_type":"solu","name.from_cfg":"SoLU_2L512W_C4_Code","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":null,"cfg.n_params":6291456,"cfg.n_layers":2,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":2,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"SoLU_2L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":6291456,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_2L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-3l","name.huggingface":"NeelNanda\/SoLU_3L512W_C4_Code","name.aliases":"solu-3l, solu-3l-new, solu-3l-c4-code","model_type":"solu","name.from_cfg":"SoLU_3L512W_C4_Code","n_params.as_str":"9.4M","n_params.as_int":9437184,"n_params.from_name":null,"cfg.n_params":9437184,"cfg.n_layers":3,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":3,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"SoLU_3L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":9437184,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 3\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_3L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 9437184\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-2]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-2]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-2]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-2]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-4l","name.huggingface":"NeelNanda\/SoLU_4L512W_C4_Code","name.aliases":"solu-4l, solu-4l-new, solu-4l-c4-code","model_type":"solu","name.from_cfg":"SoLU_4L512W_C4_Code","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"cfg.n_params":12582912,"cfg.n_layers":4,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":4,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"SoLU_4L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_4L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-6l","name.huggingface":"NeelNanda\/SoLU_6L768W_C4_Code","name.aliases":"solu-6l, solu-6l-new, solu-6l-c4-code","model_type":"solu","name.from_cfg":"SoLU_6L768W_C4_Code","n_params.as_str":"42M","n_params.as_int":42467328,"n_params.from_name":null,"cfg.n_params":42467328,"cfg.n_layers":6,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":768,"n_ctx":1024,"d_head":64,"model_name":"SoLU_6L768W_C4_Code","n_heads":12,"d_mlp":3072,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":42467328,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 6\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_6L768W_C4_Code\nn_heads: 12\nd_mlp: 3072\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 42467328\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (3072,)\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(3072,)"},"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 3072)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"solu-8l","name.huggingface":"NeelNanda\/SoLU_8L1024W_C4_Code","name.aliases":"solu-8l, solu-8l-new, solu-8l-c4-code","model_type":"solu","name.from_cfg":"SoLU_8L1024W_C4_Code","n_params.as_str":"101M","n_params.as_int":100663296,"n_params.from_name":null,"cfg.n_params":100663296,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":1024,"n_ctx":1024,"d_head":64,"model_name":"SoLU_8L1024W_C4_Code","n_heads":16,"d_mlp":4096,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":100663296,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_8L1024W_C4_Code\nn_heads: 16\nd_mlp: 4096\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 100663296\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (4096,)\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(4096,)"},"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 4096)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"solu-10l","name.huggingface":"NeelNanda\/SoLU_10L1280W_C4_Code","name.aliases":"solu-10l, solu-10l-new, solu-10l-c4-code","model_type":"solu","name.from_cfg":"SoLU_10L1280W_C4_Code","n_params.as_str":"197M","n_params.as_int":196608000,"n_params.from_name":null,"cfg.n_params":196608000,"cfg.n_layers":10,"cfg.n_heads":20,"cfg.d_model":1280,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":10,"d_model":1280,"n_ctx":1024,"d_head":64,"model_name":"SoLU_10L1280W_C4_Code","n_heads":20,"d_mlp":5120,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0223606798,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":196608000,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 10\nd_model: 1280\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_10L1280W_C4_Code\nn_heads: 20\nd_mlp: 5120\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.022360679774997897\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 196608000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 1280)\npos_embed:\n  W_pos: (1024, 1280)\nblocks:\n  '[0-9]':\n    ln1:\n      '[w, b]': (1280,)\n    ln2:\n      '[w, b]': (1280,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 1280, 64)\n      W_O: (20, 64, 1280)\n      '[b_Q, b_K, b_V]': (20, 64)\n      b_O: (1280,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (5120,)\n      W_in: (1280, 5120)\n      b_in: (5120,)\n      W_out: (5120, 1280)\n      b_out: (1280,)\nln_final:\n  '[w, b]': (1280,)\nunembed:\n  W_U: (1280, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 1280)"},"pos_embed":{"W_pos":"(1024, 1280)"},"blocks":{"[0-9]":{"ln1":{"[w, b]":"(1280,)"},"ln2":{"[w, b]":"(1280,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 1280, 64)","W_O":"(20, 64, 1280)","[b_Q, b_K, b_V]":"(20, 64)","b_O":"(1280,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(5120,)"},"W_in":"(1280, 5120)","b_in":"(5120,)","W_out":"(5120, 1280)","b_out":"(1280,)"}}},"ln_final":{"[w, b]":"(1280,)"},"unembed":{"W_U":"(1280, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-9]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 5120)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 5120)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1280)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1280)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-9]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 64)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 5120)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1280)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1280)"}}
{"name.default_alias":"solu-12l","name.huggingface":"NeelNanda\/SoLU_12L1536W_C4_Code","name.aliases":"solu-12l, solu-12l-new, solu-12l-c4-code","model_type":"solu","name.from_cfg":"SoLU_12L1536W_C4_Code","n_params.as_str":"340M","n_params.as_int":339738624,"n_params.from_name":null,"cfg.n_params":339738624,"cfg.n_layers":12,"cfg.n_heads":24,"cfg.d_model":1536,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":1536,"n_ctx":1024,"d_head":64,"model_name":"SoLU_12L1536W_C4_Code","n_heads":24,"d_mlp":6144,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0204124145,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":339738624,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 1536\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_12L1536W_C4_Code\nn_heads: 24\nd_mlp: 6144\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.020412414523193152\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 339738624\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 1536)\npos_embed:\n  W_pos: (1024, 1536)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (1536,)\n    ln2:\n      '[w, b]': (1536,)\n    attn:\n      '[W_Q, W_K, W_V]': (24, 1536, 64)\n      W_O: (24, 64, 1536)\n      '[b_Q, b_K, b_V]': (24, 64)\n      b_O: (1536,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (6144,)\n      W_in: (1536, 6144)\n      b_in: (6144,)\n      W_out: (6144, 1536)\n      b_out: (1536,)\nln_final:\n  '[w, b]': (1536,)\nunembed:\n  W_U: (1536, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 1536)"},"pos_embed":{"W_pos":"(1024, 1536)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(1536,)"},"ln2":{"[w, b]":"(1536,)"},"attn":{"[W_Q, W_K, W_V]":"(24, 1536, 64)","W_O":"(24, 64, 1536)","[b_Q, b_K, b_V]":"(24, 64)","b_O":"(1536,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(6144,)"},"W_in":"(1536, 6144)","b_in":"(6144,)","W_out":"(6144, 1536)","b_out":"(1536,)"}}},"ln_final":{"[w, b]":"(1536,)"},"unembed":{"W_U":"(1536, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 24, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 24, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 6144)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 6144)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1536)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1536)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1536)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 24, 64)","[hook_attn_scores, hook_pattern]":"(batch, 24, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 6144)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1536)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1536)"}}
{"name.default_alias":"gelu-1l","name.huggingface":"NeelNanda\/GELU_1L512W_C4_Code","name.aliases":"gelu-1l, gelu-1l-new, gelu-1l-c4-code","model_type":"gelu","name.from_cfg":"GELU_1L512W_C4_Code","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"cfg.n_params":3145728,"cfg.n_layers":1,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":1,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"GELU_1L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":3145728,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_1L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"gelu-2l","name.huggingface":"NeelNanda\/GELU_2L512W_C4_Code","name.aliases":"gelu-2l, gelu-2l-new, gelu-2l-c4-code","model_type":"gelu","name.from_cfg":"GELU_2L512W_C4_Code","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":null,"cfg.n_params":6291456,"cfg.n_layers":2,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":2,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"GELU_2L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":6291456,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_2L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"gelu-3l","name.huggingface":"NeelNanda\/GELU_3L512W_C4_Code","name.aliases":"gelu-3l, gelu-3l-new, gelu-3l-c4-code","model_type":"gelu","name.from_cfg":"GELU_3L512W_C4_Code","n_params.as_str":"9.4M","n_params.as_int":9437184,"n_params.from_name":null,"cfg.n_params":9437184,"cfg.n_layers":3,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":3,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"GELU_3L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":9437184,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 3\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_3L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 9437184\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-2]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-2]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-2]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-2]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"gelu-4l","name.huggingface":"NeelNanda\/GELU_4L512W_C4_Code","name.aliases":"gelu-4l, gelu-4l-new, gelu-4l-c4-code","model_type":"gelu","name.from_cfg":"GELU_4L512W_C4_Code","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"cfg.n_params":12582912,"cfg.n_layers":4,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":4,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"GELU_4L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_4L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-1l","name.huggingface":"NeelNanda\/Attn_Only_1L512W_C4_Code","name.aliases":"attn-only-1l, attn-only-1l-new, attn-only-1l-c4-code","model_type":"attn-only","name.from_cfg":"Attn_Only_1L512W_C4_Code","n_params.as_str":"1.0M","n_params.as_int":1048576,"n_params.from_name":null,"cfg.n_params":1048576,"cfg.n_layers":1,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":1,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"Attn_Only_1L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":true,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1048576,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_1L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1048576\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-2l","name.huggingface":"NeelNanda\/Attn_Only_2L512W_C4_Code","name.aliases":"attn-only-2l, attn-only-2l-new, attn-only-2l-c4-code","model_type":"attn-only","name.from_cfg":"Attn_Only_2L512W_C4_Code","n_params.as_str":"2.1M","n_params.as_int":2097152,"n_params.from_name":null,"cfg.n_params":2097152,"cfg.n_layers":2,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":2,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"Attn_Only_2L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":true,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":2097152,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_2L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2097152\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-3l","name.huggingface":"NeelNanda\/Attn_Only_3L512W_C4_Code","name.aliases":"attn-only-3l, attn-only-3l-new, attn-only-3l-c4-code","model_type":"attn-only","name.from_cfg":"Attn_Only_3L512W_C4_Code","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"cfg.n_params":3145728,"cfg.n_layers":3,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":3,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"Attn_Only_3L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":true,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":3145728,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 3\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_3L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-2]':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-2]":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-2]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-2]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-4l","name.huggingface":"NeelNanda\/Attn_Only_4L512W_C4_Code","name.aliases":"attn-only-4l, attn-only-4l-new, attn-only-4l-c4-code","model_type":"attn-only","name.from_cfg":"Attn_Only_4L512W_C4_Code","n_params.as_str":"4.2M","n_params.as_int":4194304,"n_params.from_name":null,"cfg.n_params":4194304,"cfg.n_layers":4,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":4,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"Attn_Only_4L512W_C4_Code","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":true,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":4194304,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_4L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 4194304\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-2l-demo","name.huggingface":"NeelNanda\/Attn-Only-2L512W-Shortformer-6B-big-lr","name.aliases":"attn-only-2l-demo, attn-only-2l-shortformer-6b-big-lr, attn-only-2l-induction-demo, attn-only-demo","model_type":"attn-only","name.from_cfg":"Attn-Only-2L512W-Shortformer-6B-big-lr","n_params.as_str":"2.1M","n_params.as_int":2097152,"n_params.from_name":null,"cfg.n_params":2097152,"cfg.n_layers":2,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":50277,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"shortformer","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":null,"config.raw__":{"n_layers":2,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"Attn-Only-2L512W-Shortformer-6B-big-lr","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":50277,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"EleutherAI\/gpt-neox-20b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":null,"device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":true,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"shortformer","final_rms":false,"d_vocab_out":50277,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":2097152,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn-Only-2L512W-Shortformer-6B-big-lr\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 50277\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: null\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: shortformer\nfinal_rms: false\nd_vocab_out: 50277\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2097152\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"EleutherAI\/gpt-neox-20b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50277, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nunembed:\n  W_U: (512, 50277)\n  b_U: (50277,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50277, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"unembed":{"W_U":"(512, 50277)","b_U":"(50277,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-1l-wiki","name.huggingface":"NeelNanda\/SoLU_1L512W_Wiki_Finetune","name.aliases":"solu-1l-wiki, solu-1l-wiki-finetune, solu-1l-finetune","model_type":"solu","name.from_cfg":"SoLU_1L512W_Wiki_Finetune","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"cfg.n_params":3145728,"cfg.n_layers":1,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":1,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"SoLU_1L512W_Wiki_Finetune","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":3145728,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_1L512W_Wiki_Finetune\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-4l-wiki","name.huggingface":"NeelNanda\/SoLU_4L512W_Wiki_Finetune","name.aliases":"solu-4l-wiki, solu-4l-wiki-finetune, solu-4l-finetune","model_type":"solu","name.from_cfg":"SoLU_4L512W_Wiki_Finetune","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"cfg.n_params":12582912,"cfg.n_layers":4,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":48262,"cfg.act_fn":"solu_ln","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":4,"d_model":512,"n_ctx":1024,"d_head":64,"model_name":"SoLU_4L512W_Wiki_Finetune","n_heads":8,"d_mlp":2048,"act_fn":"solu_ln","d_vocab":48262,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"NeelNanda\/gpt-neox-tokenizer-digits","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":48262,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_4L512W_Wiki_Finetune\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"NeelNanda\/gpt-neox-tokenizer-digits","tokenizer.vocab_size":48262.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"PreTrainedTokenizerFast","tokenizer.vocab_hash":"AsGo9tS8Sq4-rlVHM2o3-GyDkJU","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"redwood_attn_2l","name.huggingface":"ArthurConmy\/redwood_attn_2l","name.aliases":"redwood_attn_2l","model_type":null,"name.from_cfg":"redwood_attn_2l","n_params.as_str":"524K","n_params.as_int":524288,"n_params.from_name":null,"cfg.n_params":524288,"cfg.n_layers":2,"cfg.n_heads":8,"cfg.d_model":256,"cfg.d_vocab":50259,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"shortformer","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"neel","cfg.normalization_type":"LN","config.raw__":{"n_layers":2,"d_model":256,"n_ctx":2048,"d_head":32,"model_name":"redwood_attn_2l","n_heads":8,"d_mlp":-1,"act_fn":"gelu_new","d_vocab":50259,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"neel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"ArthurConmy\/redwood_tokenizer","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":true,"seed":null,"initializer_range":0.05,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"shortformer","final_rms":false,"d_vocab_out":50259,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":524288,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 256\nn_ctx: 2048\nd_head: 32\nmodel_name: redwood_attn_2l\nn_heads: 8\nd_mlp: -1\nact_fn: gelu_new\nd_vocab: 50259\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: ArthurConmy\/redwood_tokenizer\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: 0.05\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: shortformer\nfinal_rms: false\nd_vocab_out: 50259\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 524288\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"ArthurConmy\/redwood_tokenizer","tokenizer.vocab_size":50257.0,"tokenizer.max_len":1024,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"J8auoAiqFanHN7mOtkTrFA9voRk","tensor_shapes.state_dict":"embed:\n  W_E: (50259, 256)\npos_embed:\n  W_pos: (2048, 256)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 256, 32)\n      W_O: (8, 32, 256)\n      '[b_Q, b_K, b_V]': (8, 32)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50259)\n  b_U: (50259,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50259, 256)"},"pos_embed":{"W_pos":"(2048, 256)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 256, 32)","W_O":"(8, 32, 256)","[b_Q, b_K, b_V]":"(8, 32)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50259)","b_U":"(50259,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 32)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 256)"}}
{"name.default_alias":"llama-7b","name.huggingface":"llama-7b-hf","name.aliases":"llama-7b","model_type":"llama","name.from_cfg":"llama-7b-hf","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"llama-7b-hf","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":32000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"llama-7b-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-7b-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-7b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"llama-13b","name.huggingface":"llama-13b-hf","name.aliases":"llama-13b","model_type":"llama","name.from_cfg":"llama-13b-hf","n_params.as_str":"13B","n_params.as_int":12687769600,"n_params.from_name":"13b","cfg.n_params":12687769600,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"llama-13b-hf","n_heads":40,"d_mlp":13824,"act_fn":"silu","d_vocab":32000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"llama-13b-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":12687769600,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-13b-hf\nn_heads: 40\nd_mlp: 13824\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-13b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 12687769600\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"llama-30b","name.huggingface":"llama-30b-hf","name.aliases":"llama-30b","model_type":"llama","name.from_cfg":"llama-30b-hf","n_params.as_str":"32B","n_params.as_int":32102154240,"n_params.from_name":"30b","cfg.n_params":32102154240,"cfg.n_layers":60,"cfg.n_heads":52,"cfg.d_model":6656,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":60,"d_model":6656,"n_ctx":2048,"d_head":128,"model_name":"llama-30b-hf","n_heads":52,"d_mlp":17920,"act_fn":"silu","d_vocab":32000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"llama-30b-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0098058068,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":32102154240,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 60\nd_model: 6656\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-30b-hf\nn_heads: 52\nd_mlp: 17920\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-30b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.009805806756909202\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 32102154240\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"llama-65b","name.huggingface":"llama-65b-hf","name.aliases":"llama-65b","model_type":"llama","name.from_cfg":"llama-65b-hf","n_params.as_str":"65B","n_params.as_int":64760053760,"n_params.from_name":"65b","cfg.n_params":64760053760,"cfg.n_layers":80,"cfg.n_heads":64,"cfg.d_model":8192,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":80,"d_model":8192,"n_ctx":2048,"d_head":128,"model_name":"llama-65b-hf","n_heads":64,"d_mlp":22016,"act_fn":"silu","d_vocab":32000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"llama-65b-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0088388348,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":64760053760,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 80\nd_model: 8192\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-65b-hf\nn_heads: 64\nd_mlp: 22016\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-65b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.008838834764831844\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 64760053760\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"Llama-2-7b","name.huggingface":null,"name.aliases":"","model_type":"Llama-2","name.from_cfg":"Llama-2-7b-hf","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"Llama-2-7b-hf","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Llama-2-7b-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-7b-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-7b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"Llama-2-7b-chat","name.huggingface":null,"name.aliases":"","model_type":"Llama-2","name.from_cfg":"Llama-2-7b-chat-hf","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"Llama-2-7b-chat-hf","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Llama-2-7b-chat-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-7b-chat-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-7b-chat-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"Llama-2-13b","name.huggingface":null,"name.aliases":"","model_type":"Llama-2","name.from_cfg":"Llama-2-13b-hf","n_params.as_str":"13B","n_params.as_int":12687769600,"n_params.from_name":"13b","cfg.n_params":12687769600,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":4096,"d_head":128,"model_name":"Llama-2-13b-hf","n_heads":40,"d_mlp":13824,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Llama-2-13b-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":12687769600,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-13b-hf\nn_heads: 40\nd_mlp: 13824\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-13b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 12687769600\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"Llama-2-13b-chat","name.huggingface":null,"name.aliases":"","model_type":"Llama-2","name.from_cfg":"Llama-2-13b-chat-hf","n_params.as_str":"13B","n_params.as_int":12687769600,"n_params.from_name":"13b","cfg.n_params":12687769600,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":4096,"d_head":128,"model_name":"Llama-2-13b-chat-hf","n_heads":40,"d_mlp":13824,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Llama-2-13b-chat-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0111803399,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":12687769600,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-13b-chat-hf\nn_heads: 40\nd_mlp: 13824\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-13b-chat-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.011180339887498949\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 12687769600\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"Llama-2-70b-chat","name.huggingface":null,"name.aliases":"","model_type":"Llama-2","name.from_cfg":"Llama-2-70b-chat-hf","n_params.as_str":"78B","n_params.as_int":77846282240,"n_params.from_name":"70b","cfg.n_params":77846282240,"cfg.n_layers":80,"cfg.n_heads":64,"cfg.d_model":8192,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":80,"d_model":8192,"n_ctx":4096,"d_head":128,"model_name":"Llama-2-70b-chat-hf","n_heads":64,"d_mlp":28672,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Llama-2-70b-chat-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0088388348,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":77846282240,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 80\nd_model: 8192\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-70b-chat-hf\nn_heads: 64\nd_mlp: 28672\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-70b-chat-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.008838834764831844\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 77846282240\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"CodeLlamallama-2-7b","name.huggingface":null,"name.aliases":"","model_type":"llama","name.from_cfg":"CodeLlama-7b-hf","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32016,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"CodeLlama-7b-hf","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":32016,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"CodeLlama-7b-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32016,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: CodeLlama-7b-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32016\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: CodeLlama-7b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32016\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"CodeLlama-7b-python","name.huggingface":null,"name.aliases":"","model_type":"CodeLlama","name.from_cfg":"CodeLlama-7b-Python-hf","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"CodeLlama-7b-Python-hf","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"CodeLlama-7b-Python-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: CodeLlama-7b-Python-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: CodeLlama-7b-Python-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"CodeLlama-7b-instruct","name.huggingface":null,"name.aliases":"","model_type":"CodeLlama","name.from_cfg":"CodeLlama-7b-Instruct-hf","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32016,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"CodeLlama-7b-Instruct-hf","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":32016,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"CodeLlama-7b-Instruct-hf","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":32016,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: CodeLlama-7b-Instruct-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32016\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: CodeLlama-7b-Instruct-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32016\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"meta-llama\/Meta-Llama-3-8B","name.huggingface":null,"name.aliases":"","model_type":"llama","name.from_cfg":"Meta-Llama-3-8B","n_params.as_str":"7.8B","n_params.as_int":7784628224,"n_params.from_name":"8B","cfg.n_params":7784628224,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":128256,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":8192,"d_head":128,"model_name":"Meta-Llama-3-8B","n_heads":32,"d_mlp":14336,"act_fn":"silu","d_vocab":128256,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Meta-Llama-3-8B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":128256,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":7784628224,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 8192\nd_head: 128\nmodel_name: Meta-Llama-3-8B\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 128256\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Meta-Llama-3-8B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 128256\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 7784628224\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"meta-llama\/Meta-Llama-3-8B-Instruct","name.huggingface":null,"name.aliases":"","model_type":"llama","name.from_cfg":"Meta-Llama-3-8B-Instruct","n_params.as_str":"7.8B","n_params.as_int":7784628224,"n_params.from_name":"8B","cfg.n_params":7784628224,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":128256,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":8192,"d_head":128,"model_name":"Meta-Llama-3-8B-Instruct","n_heads":32,"d_mlp":14336,"act_fn":"silu","d_vocab":128256,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Meta-Llama-3-8B-Instruct","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":128256,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":7784628224,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 8192\nd_head: 128\nmodel_name: Meta-Llama-3-8B-Instruct\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 128256\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Meta-Llama-3-8B-Instruct\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 128256\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 7784628224\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"meta-llama\/Meta-Llama-3-70B","name.huggingface":null,"name.aliases":"","model_type":"llama","name.from_cfg":"Meta-Llama-3-70B","n_params.as_str":"78B","n_params.as_int":77846282240,"n_params.from_name":"70B","cfg.n_params":77846282240,"cfg.n_layers":80,"cfg.n_heads":64,"cfg.d_model":8192,"cfg.d_vocab":128256,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":80,"d_model":8192,"n_ctx":8192,"d_head":128,"model_name":"Meta-Llama-3-70B","n_heads":64,"d_mlp":28672,"act_fn":"silu","d_vocab":128256,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Meta-Llama-3-70B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0088388348,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":128256,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":77846282240,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 80\nd_model: 8192\nn_ctx: 8192\nd_head: 128\nmodel_name: Meta-Llama-3-70B\nn_heads: 64\nd_mlp: 28672\nact_fn: silu\nd_vocab: 128256\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Meta-Llama-3-70B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.008838834764831844\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 128256\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 77846282240\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"meta-llama\/Meta-Llama-3-70B-Instruct","name.huggingface":null,"name.aliases":"","model_type":"llama","name.from_cfg":"Meta-Llama-3-70B-Instruct","n_params.as_str":"78B","n_params.as_int":77846282240,"n_params.from_name":"70B","cfg.n_params":77846282240,"cfg.n_layers":80,"cfg.n_heads":64,"cfg.d_model":8192,"cfg.d_vocab":128256,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":80,"d_model":8192,"n_ctx":8192,"d_head":128,"model_name":"Meta-Llama-3-70B-Instruct","n_heads":64,"d_mlp":28672,"act_fn":"silu","d_vocab":128256,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"meta-llama\/Meta-Llama-3-70B-Instruct","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0088388348,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":128256,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":77846282240,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 80\nd_model: 8192\nn_ctx: 8192\nd_head: 128\nmodel_name: Meta-Llama-3-70B-Instruct\nn_heads: 64\nd_mlp: 28672\nact_fn: silu\nd_vocab: 128256\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Meta-Llama-3-70B-Instruct\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.008838834764831844\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 128256\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 77846282240\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"othello-gpt","name.huggingface":"Baidicoot\/Othello-GPT-Transformer-Lens","name.aliases":"othello-gpt","model_type":null,"name.from_cfg":"Othello-GPT-Transformer-Lens","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":null,"cfg.n_params":25165824,"cfg.n_layers":8,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":61,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"mingpt","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":512,"n_ctx":59,"d_head":64,"model_name":"Othello-GPT-Transformer-Lens","n_heads":8,"d_mlp":2048,"act_fn":"gelu","d_vocab":61,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"mingpt","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":null,"window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":61,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":25165824,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 512\nn_ctx: 59\nd_head: 64\nmodel_name: Othello-GPT-Transformer-Lens\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 61\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: mingpt\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: null\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 61\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":"embed:\n  W_E: (61, 512)\npos_embed:\n  W_pos: (59, 512)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (59, 59)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 61)\n  b_U: (61,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(61, 512)"},"pos_embed":{"W_pos":"(59, 512)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(59, 59)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 61)","b_U":"(61,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"bert-base-cased","name.huggingface":"bert-base-cased","name.aliases":"","model_type":"bert","name.from_cfg":"bert-base-cased","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":28996,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"BertForMaskedLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":512,"d_head":64,"model_name":"bert-base-cased","n_heads":12,"d_mlp":3072,"act_fn":"gelu","d_vocab":28996,"eps":0.0,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"BertForMaskedLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"bert-base-cased","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"bidirectional","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":28996,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 12\nd_model: 768\nn_ctx: 512\nd_head: 64\nmodel_name: bert-base-cased\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 28996\neps: 1.0e-12\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BertForMaskedLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bert-base-cased\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: bidirectional\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 28996\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"bert-base-cased","tokenizer.vocab_size":28996.0,"tokenizer.max_len":512,"tokenizer.class":"BertTokenizerFast","tokenizer.vocab_hash":"SSKvHuFYtPbvgwMSLSIhfFE_kF8","tensor_shapes.state_dict":"embed:\n  W_E: (28996, 768)\npos_embed:\n  W_pos: (512, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (512, 512)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 28996)\n  b_U: (28996,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(28996, 768)"},"pos_embed":{"W_pos":"(512, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(512, 512)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 28996)","b_U":"(28996,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"tiny-stories-1M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-1M","n_params.as_str":"393K","n_params.as_int":393216,"n_params.from_name":"1M","cfg.n_params":393216,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":64,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":64,"n_ctx":2048,"d_head":4,"model_name":"TinyStories-1M","n_heads":16,"d_mlp":256,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-1M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.1,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":393216,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 64\nn_ctx: 2048\nd_head: 4\nmodel_name: TinyStories-1M\nn_heads: 16\nd_mlp: 256\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-1M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.1\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 393216\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-1M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 64)\npos_embed:\n  W_pos: (2048, 64)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (64,)\n    ln2:\n      '[w, b]': (64,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 64, 4)\n      W_O: (16, 4, 64)\n      '[b_Q, b_K, b_V]': (16, 4)\n      b_O: (64,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (64, 256)\n      b_in: (256,)\n      W_out: (256, 64)\n      b_out: (64,)\nln_final:\n  '[w, b]': (64,)\nunembed:\n  W_U: (64, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 64)"},"pos_embed":{"W_pos":"(2048, 64)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(64,)"},"ln2":{"[w, b]":"(64,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 64, 4)","W_O":"(16, 4, 64)","[b_Q, b_K, b_V]":"(16, 4)","b_O":"(64,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(64, 256)","b_in":"(256,)","W_out":"(256, 64)","b_out":"(64,)"}}},"ln_final":{"[w, b]":"(64,)"},"unembed":{"W_U":"(64, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 4)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 256)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 64)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 64)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 64)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 4)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 256)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 64)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 64)"}}
{"name.default_alias":"tiny-stories-3M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-3M","n_params.as_str":"1.6M","n_params.as_int":1572864,"n_params.from_name":"3M","cfg.n_params":1572864,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":128,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":128,"n_ctx":2048,"d_head":8,"model_name":"TinyStories-3M","n_heads":16,"d_mlp":512,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-3M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0707106781,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1572864,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 128\nn_ctx: 2048\nd_head: 8\nmodel_name: TinyStories-3M\nn_heads: 16\nd_mlp: 512\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-3M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.07071067811865475\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1572864\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-3M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 128)\npos_embed:\n  W_pos: (2048, 128)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (128,)\n    ln2:\n      '[w, b]': (128,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 128, 8)\n      W_O: (16, 8, 128)\n      '[b_Q, b_K, b_V]': (16, 8)\n      b_O: (128,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (128, 512)\n      b_in: (512,)\n      W_out: (512, 128)\n      b_out: (128,)\nln_final:\n  '[w, b]': (128,)\nunembed:\n  W_U: (128, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 128)"},"pos_embed":{"W_pos":"(2048, 128)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(128,)"},"ln2":{"[w, b]":"(128,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 128, 8)","W_O":"(16, 8, 128)","[b_Q, b_K, b_V]":"(16, 8)","b_O":"(128,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(128, 512)","b_in":"(512,)","W_out":"(512, 128)","b_out":"(128,)"}}},"ln_final":{"[w, b]":"(128,)"},"unembed":{"W_U":"(128, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 8)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 512)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 128)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 128)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 128)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 8)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 512)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 128)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 128)"}}
{"name.default_alias":"tiny-stories-8M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-8M","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":"8M","cfg.n_params":6291456,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":256,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":256,"n_ctx":2048,"d_head":16,"model_name":"TinyStories-8M","n_heads":16,"d_mlp":1024,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-8M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.05,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":6291456,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 256\nn_ctx: 2048\nd_head: 16\nmodel_name: TinyStories-8M\nn_heads: 16\nd_mlp: 1024\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-8M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.05\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-8M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 256)\npos_embed:\n  W_pos: (2048, 256)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (256,)\n    ln2:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 256, 16)\n      W_O: (16, 16, 256)\n      '[b_Q, b_K, b_V]': (16, 16)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (256, 1024)\n      b_in: (1024,)\n      W_out: (1024, 256)\n      b_out: (256,)\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 256)"},"pos_embed":{"W_pos":"(2048, 256)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(256,)"},"ln2":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 256, 16)","W_O":"(16, 16, 256)","[b_Q, b_K, b_V]":"(16, 16)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(256, 1024)","b_in":"(1024,)","W_out":"(1024, 256)","b_out":"(256,)"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 16)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 1024)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 16)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 1024)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 256)"}}
{"name.default_alias":"tiny-stories-28M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-28M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"28M","cfg.n_params":25165824,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":512,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":512,"n_ctx":2048,"d_head":32,"model_name":"TinyStories-28M","n_heads":16,"d_mlp":2048,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-28M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":25165824,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 512\nn_ctx: 2048\nd_head: 32\nmodel_name: TinyStories-28M\nn_heads: 16\nd_mlp: 2048\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-28M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-28M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 512)\npos_embed:\n  W_pos: (2048, 512)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 512, 32)\n      W_O: (16, 32, 512)\n      '[b_Q, b_K, b_V]': (16, 32)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 512)"},"pos_embed":{"W_pos":"(2048, 512)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 512, 32)","W_O":"(16, 32, 512)","[b_Q, b_K, b_V]":"(16, 32)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 32)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"tiny-stories-33M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-33M","n_params.as_str":"28M","n_params.as_int":28311552,"n_params.from_name":"33M","cfg.n_params":28311552,"cfg.n_layers":4,"cfg.n_heads":16,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":4,"d_model":768,"n_ctx":2048,"d_head":48,"model_name":"TinyStories-33M","n_heads":16,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-33M","window_size":256,"attn_types":["global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":28311552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 4\nd_model: 768\nn_ctx: 2048\nd_head: 48\nmodel_name: TinyStories-33M\nn_heads: 16\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-33M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 28311552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-33M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 768, 48)\n      W_O: (16, 48, 768)\n      '[b_Q, b_K, b_V]': (16, 48)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 768, 48)","W_O":"(16, 48, 768)","[b_Q, b_K, b_V]":"(16, 48)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 48)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 48)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"tiny-stories-instruct-1M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-1M","n_params.as_str":"393K","n_params.as_int":393216,"n_params.from_name":"1M","cfg.n_params":393216,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":64,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":64,"n_ctx":2048,"d_head":4,"model_name":"TinyStories-Instruct-1M","n_heads":16,"d_mlp":256,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-Instruct-1M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.1,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":393216,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 64\nn_ctx: 2048\nd_head: 4\nmodel_name: TinyStories-Instruct-1M\nn_heads: 16\nd_mlp: 256\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-1M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.1\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 393216\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-Instruct-1M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 64)\npos_embed:\n  W_pos: (2048, 64)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (64,)\n    ln2:\n      '[w, b]': (64,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 64, 4)\n      W_O: (16, 4, 64)\n      '[b_Q, b_K, b_V]': (16, 4)\n      b_O: (64,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (64, 256)\n      b_in: (256,)\n      W_out: (256, 64)\n      b_out: (64,)\nln_final:\n  '[w, b]': (64,)\nunembed:\n  W_U: (64, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 64)"},"pos_embed":{"W_pos":"(2048, 64)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(64,)"},"ln2":{"[w, b]":"(64,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 64, 4)","W_O":"(16, 4, 64)","[b_Q, b_K, b_V]":"(16, 4)","b_O":"(64,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(64, 256)","b_in":"(256,)","W_out":"(256, 64)","b_out":"(64,)"}}},"ln_final":{"[w, b]":"(64,)"},"unembed":{"W_U":"(64, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 4)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 256)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 64)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 64)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 64)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 4)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 256)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 64)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 64)"}}
{"name.default_alias":"tiny-stories-instruct-3M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-3M","n_params.as_str":"1.6M","n_params.as_int":1572864,"n_params.from_name":"3M","cfg.n_params":1572864,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":128,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":128,"n_ctx":2048,"d_head":8,"model_name":"TinyStories-Instruct-3M","n_heads":16,"d_mlp":512,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-Instruct-3M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0707106781,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1572864,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 128\nn_ctx: 2048\nd_head: 8\nmodel_name: TinyStories-Instruct-3M\nn_heads: 16\nd_mlp: 512\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-3M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.07071067811865475\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1572864\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-Instruct-3M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 128)\npos_embed:\n  W_pos: (2048, 128)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (128,)\n    ln2:\n      '[w, b]': (128,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 128, 8)\n      W_O: (16, 8, 128)\n      '[b_Q, b_K, b_V]': (16, 8)\n      b_O: (128,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (128, 512)\n      b_in: (512,)\n      W_out: (512, 128)\n      b_out: (128,)\nln_final:\n  '[w, b]': (128,)\nunembed:\n  W_U: (128, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 128)"},"pos_embed":{"W_pos":"(2048, 128)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(128,)"},"ln2":{"[w, b]":"(128,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 128, 8)","W_O":"(16, 8, 128)","[b_Q, b_K, b_V]":"(16, 8)","b_O":"(128,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(128, 512)","b_in":"(512,)","W_out":"(512, 128)","b_out":"(128,)"}}},"ln_final":{"[w, b]":"(128,)"},"unembed":{"W_U":"(128, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 8)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 512)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 128)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 128)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 128)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 8)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 512)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 128)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 128)"}}
{"name.default_alias":"tiny-stories-instruct-8M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-8M","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":"8M","cfg.n_params":6291456,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":256,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":256,"n_ctx":2048,"d_head":16,"model_name":"TinyStories-Instruct-8M","n_heads":16,"d_mlp":1024,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-Instruct-8M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.05,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":6291456,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 256\nn_ctx: 2048\nd_head: 16\nmodel_name: TinyStories-Instruct-8M\nn_heads: 16\nd_mlp: 1024\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-8M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.05\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-Instruct-8M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 256)\npos_embed:\n  W_pos: (2048, 256)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (256,)\n    ln2:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 256, 16)\n      W_O: (16, 16, 256)\n      '[b_Q, b_K, b_V]': (16, 16)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (256, 1024)\n      b_in: (1024,)\n      W_out: (1024, 256)\n      b_out: (256,)\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 256)"},"pos_embed":{"W_pos":"(2048, 256)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(256,)"},"ln2":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 256, 16)","W_O":"(16, 16, 256)","[b_Q, b_K, b_V]":"(16, 16)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(256, 1024)","b_in":"(1024,)","W_out":"(1024, 256)","b_out":"(256,)"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 16)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 1024)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 16)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 1024)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 256)"}}
{"name.default_alias":"tiny-stories-instruct-28M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-28M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"28M","cfg.n_params":25165824,"cfg.n_layers":8,"cfg.n_heads":16,"cfg.d_model":512,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":8,"d_model":512,"n_ctx":2048,"d_head":32,"model_name":"TinyStories-Instruct-28M","n_heads":16,"d_mlp":2048,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-Instruct-28M","window_size":256,"attn_types":["global","local","global","local","global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":25165824,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 8\nd_model: 512\nn_ctx: 2048\nd_head: 32\nmodel_name: TinyStories-Instruct-28M\nn_heads: 16\nd_mlp: 2048\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-28M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-Instruct-28M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 512)\npos_embed:\n  W_pos: (2048, 512)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 512, 32)\n      W_O: (16, 32, 512)\n      '[b_Q, b_K, b_V]': (16, 32)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 512)"},"pos_embed":{"W_pos":"(2048, 512)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 512, 32)","W_O":"(16, 32, 512)","[b_Q, b_K, b_V]":"(16, 32)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 32)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"tiny-stories-instruct-33M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-33M","n_params.as_str":"28M","n_params.as_int":28311552,"n_params.from_name":"33M","cfg.n_params":28311552,"cfg.n_layers":4,"cfg.n_heads":16,"cfg.d_model":768,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":4,"d_model":768,"n_ctx":2048,"d_head":48,"model_name":"TinyStories-Instruct-33M","n_heads":16,"d_mlp":3072,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-Instruct-33M","window_size":256,"attn_types":["global","local","global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":28311552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 4\nd_model: 768\nn_ctx: 2048\nd_head: 48\nmodel_name: TinyStories-Instruct-33M\nn_heads: 16\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-33M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 28311552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-Instruct-33M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 768, 48)\n      W_O: (16, 48, 768)\n      '[b_Q, b_K, b_V]': (16, 48)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 768, 48)","W_O":"(16, 48, 768)","[b_Q, b_K, b_V]":"(16, 48)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 48)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 48)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"tiny-stories-1L-21M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-1Layer-21M","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":"21M","cfg.n_params":12582912,"cfg.n_layers":1,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":1,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"TinyStories-1Layer-21M","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-1Layer-21M","window_size":256,"attn_types":["global"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 1\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-1Layer-21M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-1Layer-21M\nwindow_size: 256\nattn_types:\n- global\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-1Layer-21M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"0":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"tiny-stories-2L-33M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-2Layers-33M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"33M","cfg.n_params":25165824,"cfg.n_layers":2,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":2,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"TinyStories-2Layers-33M","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-2Layers-33M","window_size":256,"attn_types":["global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":25165824,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-2Layers-33M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-2Layers-33M\nwindow_size: 256\nattn_types:\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-2Layers-33M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"tiny-stories-instruct-1L-21M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-Instuct-1Layer-21M","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":"21M","cfg.n_params":12582912,"cfg.n_layers":1,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":1,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"TinyStories-Instuct-1Layer-21M","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-Instuct-1Layer-21M","window_size":256,"attn_types":["global"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":12582912,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 1\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-Instuct-1Layer-21M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instuct-1Layer-21M\nwindow_size: 256\nattn_types:\n- global\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-Instuct-1Layer-21M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"0":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"tiny-stories-instruct-2L-33M","name.huggingface":null,"name.aliases":"","model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-2Layers-33M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"33M","cfg.n_params":25165824,"cfg.n_layers":2,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":50257,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPTNeoForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":2,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"TinyStories-Instruct-2Layers-33M","n_heads":16,"d_mlp":4096,"act_fn":"gelu_new","d_vocab":50257,"eps":0.00001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"GPTNeoForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"roneneldan\/TinyStories-Instruct-2Layers-33M","window_size":256,"attn_types":["global","local"],"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":50257,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":25165824,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 2\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-Instruct-2Layers-33M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-2Layers-33M\nwindow_size: 256\nattn_types:\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"roneneldan\/TinyStories-Instruct-2Layers-33M","tokenizer.vocab_size":50257.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"v8xfIj5kwZX5RwgLU66lZNZUlE4","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stablelm-base-alpha-3b","name.huggingface":"stabilityai\/stablelm-base-alpha-3b","name.aliases":"stablelm-base-alpha-3b, stablelm-base-3b","model_type":"stablelm","name.from_cfg":"stablelm-base-alpha-3b","n_params.as_str":"3.2B","n_params.as_int":3221225472,"n_params.from_name":"3b","cfg.n_params":3221225472,"cfg.n_layers":16,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":50688,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"stablelm-base-alpha-3b","n_heads":32,"d_mlp":16384,"act_fn":"gelu","d_vocab":50688,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stabilityai\/stablelm-base-alpha-3b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50688,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":3221225472,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-base-alpha-3b\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-base-alpha-3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 3221225472\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stabilityai\/stablelm-base-alpha-3b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 4096)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 4096)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"stablelm-base-alpha-7b","name.huggingface":"stabilityai\/stablelm-base-alpha-7b","name.aliases":"stablelm-base-alpha-7b, stablelm-base-7b","model_type":"stablelm","name.from_cfg":"stablelm-base-alpha-7b","n_params.as_str":"7.2B","n_params.as_int":7247757312,"n_params.from_name":"7b","cfg.n_params":7247757312,"cfg.n_layers":16,"cfg.n_heads":48,"cfg.d_model":6144,"cfg.d_vocab":50432,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":6144,"n_ctx":4096,"d_head":128,"model_name":"stablelm-base-alpha-7b","n_heads":48,"d_mlp":24576,"act_fn":"gelu","d_vocab":50432,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stabilityai\/stablelm-base-alpha-7b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0102062073,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50432,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":7247757312,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 6144\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-base-alpha-7b\nn_heads: 48\nd_mlp: 24576\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-base-alpha-7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.010206207261596576\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 7247757312\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stabilityai\/stablelm-base-alpha-7b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"96EawM8Lij99W7OBTk0KW2ELUrQ","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 6144)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (6144,)\n    ln2:\n      '[w, b]': (6144,)\n    attn:\n      '[W_Q, W_K, W_V]': (48, 6144, 128)\n      W_O: (48, 128, 6144)\n      '[b_Q, b_K, b_V]': (48, 128)\n      b_O: (6144,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (6144, 24576)\n      b_in: (24576,)\n      W_out: (24576, 6144)\n      b_out: (6144,)\nln_final:\n  '[w, b]': (6144,)\nunembed:\n  W_U: (6144, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 6144)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(6144,)"},"ln2":{"[w, b]":"(6144,)"},"attn":{"[W_Q, W_K, W_V]":"(48, 6144, 128)","W_O":"(48, 128, 6144)","[b_Q, b_K, b_V]":"(48, 128)","b_O":"(6144,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(6144, 24576)","b_in":"(24576,)","W_out":"(24576, 6144)","b_out":"(6144,)"}}},"ln_final":{"[w, b]":"(6144,)"},"unembed":{"W_U":"(6144, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        48, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 48, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      6144)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 6144)\nhook_embed: (batch, seq_len, 6144)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 48, 128)","[hook_attn_scores, hook_pattern]":"(batch, 48, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 6144)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"hook_embed":"(batch, seq_len, 6144)"}}
{"name.default_alias":"stablelm-tuned-alpha-3b","name.huggingface":"stabilityai\/stablelm-tuned-alpha-3b","name.aliases":"stablelm-tuned-alpha-3b, stablelm-tuned-3b","model_type":"stablelm","name.from_cfg":"stablelm-tuned-alpha-3b","n_params.as_str":"3.2B","n_params.as_int":3221225472,"n_params.from_name":"3b","cfg.n_params":3221225472,"cfg.n_layers":16,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":50688,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"stablelm-tuned-alpha-3b","n_heads":32,"d_mlp":16384,"act_fn":"gelu","d_vocab":50688,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stabilityai\/stablelm-tuned-alpha-3b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50688,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":3221225472,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-tuned-alpha-3b\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-tuned-alpha-3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 3221225472\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stabilityai\/stablelm-tuned-alpha-3b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"RD3vcWSd_TiTpqo5dHyICzaXtGQ","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 4096)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 4096)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"stablelm-tuned-alpha-7b","name.huggingface":"stabilityai\/stablelm-tuned-alpha-7b","name.aliases":"stablelm-tuned-alpha-7b, stablelm-tuned-7b","model_type":"stablelm","name.from_cfg":"stablelm-tuned-alpha-7b","n_params.as_str":"7.2B","n_params.as_int":7247757312,"n_params.from_name":"7b","cfg.n_params":7247757312,"cfg.n_layers":16,"cfg.n_heads":48,"cfg.d_model":6144,"cfg.d_vocab":50432,"cfg.act_fn":"gelu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"GPTNeoXForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":16,"d_model":6144,"n_ctx":4096,"d_head":128,"model_name":"stablelm-tuned-alpha-7b","n_heads":48,"d_mlp":24576,"act_fn":"gelu","d_vocab":50432,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPTNeoXForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"stabilityai\/stablelm-tuned-alpha-7b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0102062073,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":50432,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":7247757312,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 16\nd_model: 6144\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-tuned-alpha-7b\nn_heads: 48\nd_mlp: 24576\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-tuned-alpha-7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.010206207261596576\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 7247757312\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"stabilityai\/stablelm-tuned-alpha-7b","tokenizer.vocab_size":50254.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GPTNeoXTokenizerFast","tokenizer.vocab_hash":"RD3vcWSd_TiTpqo5dHyICzaXtGQ","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 6144)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (6144,)\n    ln2:\n      '[w, b]': (6144,)\n    attn:\n      '[W_Q, W_K, W_V]': (48, 6144, 128)\n      W_O: (48, 128, 6144)\n      '[b_Q, b_K, b_V]': (48, 128)\n      b_O: (6144,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (6144, 24576)\n      b_in: (24576,)\n      W_out: (24576, 6144)\n      b_out: (6144,)\nln_final:\n  '[w, b]': (6144,)\nunembed:\n  W_U: (6144, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 6144)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(6144,)"},"ln2":{"[w, b]":"(6144,)"},"attn":{"[W_Q, W_K, W_V]":"(48, 6144, 128)","W_O":"(48, 128, 6144)","[b_Q, b_K, b_V]":"(48, 128)","b_O":"(6144,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(6144, 24576)","b_in":"(24576,)","W_out":"(24576, 6144)","b_out":"(6144,)"}}},"ln_final":{"[w, b]":"(6144,)"},"unembed":{"W_U":"(6144, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        48, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 48, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      6144)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 6144)\nhook_embed: (batch, seq_len, 6144)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 48, 128)","[hook_attn_scores, hook_pattern]":"(batch, 48, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 6144)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"hook_embed":"(batch, seq_len, 6144)"}}
{"name.default_alias":"mistral-7b","name.huggingface":"mistralai\/Mistral-7B-v0.1","name.aliases":"mistral-7b","model_type":"mistral","name.from_cfg":"Mistral-7B-v0.1","n_params.as_str":"7.8B","n_params.as_int":7784628224,"n_params.from_name":"7b","cfg.n_params":7784628224,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"MistralForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Mistral-7B-v0.1","n_heads":32,"d_mlp":14336,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"MistralForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"mistralai\/Mistral-7B-v0.1","window_size":4096,"attn_types":["local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local"],"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":7784628224,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Mistral-7B-v0.1\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: MistralForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: mistralai\/Mistral-7B-v0.1\nwindow_size: 4096\nattn_types:\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 7784628224\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"mistralai\/Mistral-7B-v0.1","tokenizer.vocab_size":32000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"kkCQxUk-PF9Ay_ZKDdKCh02YaGQ","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      W_Q: (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      b_Q: (32, 128)\n      b_O: (4096,)\n      '[_W_K, _W_V]': (8, 4096, 128)\n      '[_b_K, _b_V]': (8, 128)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 14336)\n      W_out: (14336, 4096)\n      b_in: (14336,)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"W_Q":"(32, 4096, 128)","W_O":"(32, 128, 4096)","b_Q":"(32, 128)","b_O":"(4096,)","[_W_K, _W_V]":"(8, 4096, 128)","[_b_K, _b_V]":"(8, 128)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 14336)","W_out":"(14336, 4096)","b_in":"(14336,)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 8, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 14336)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 32, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 8, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 14336)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"mistral-7b-instruct","name.huggingface":"mistralai\/Mistral-7B-Instruct-v0.1","name.aliases":"mistral-7b-instruct","model_type":"mistral","name.from_cfg":"Mistral-7B-Instruct-v0.1","n_params.as_str":"7.8B","n_params.as_int":7784628224,"n_params.from_name":"7b","cfg.n_params":7784628224,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"MistralForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Mistral-7B-Instruct-v0.1","n_heads":32,"d_mlp":14336,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":true,"original_architecture":"MistralForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"mistralai\/Mistral-7B-Instruct-v0.1","window_size":4096,"attn_types":["local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local","local"],"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":7784628224,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Mistral-7B-Instruct-v0.1\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: MistralForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: mistralai\/Mistral-7B-Instruct-v0.1\nwindow_size: 4096\nattn_types:\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 7784628224\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"mistralai\/Mistral-7B-Instruct-v0.1","tokenizer.vocab_size":32000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"kkCQxUk-PF9Ay_ZKDdKCh02YaGQ","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      W_Q: (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      b_Q: (32, 128)\n      b_O: (4096,)\n      '[_W_K, _W_V]': (8, 4096, 128)\n      '[_b_K, _b_V]': (8, 128)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 14336)\n      W_out: (14336, 4096)\n      b_in: (14336,)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"W_Q":"(32, 4096, 128)","W_O":"(32, 128, 4096)","b_Q":"(32, 128)","b_O":"(4096,)","[_W_K, _W_V]":"(8, 4096, 128)","[_b_K, _b_V]":"(8, 128)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 14336)","W_out":"(14336, 4096)","b_in":"(14336,)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 8, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 14336)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 32, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 8, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 14336)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"mixtral","name.huggingface":"mistralai\/Mixtral-8x7B-v0.1","name.aliases":"mixtral, mixtral-8x7b","model_type":null,"name.from_cfg":"Mixtral-8x7B-v0.1","n_params.as_str":"47B","n_params.as_int":47245688832,"n_params.from_name":null,"cfg.n_params":47245688832,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"MixtralForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Mixtral-8x7B-v0.1","n_heads":32,"d_mlp":14336,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"MixtralForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"mistralai\/Mixtral-8x7B-v0.1","window_size":null,"attn_types":["global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global"],"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":47245688832,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":8,"experts_per_token":2,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Mixtral-8x7B-v0.1\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: MixtralForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: mistralai\/Mixtral-8x7B-v0.1\nwindow_size: null\nattn_types:\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 47245688832\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: 8\nexperts_per_token: 2\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"mistralai\/Mixtral-8x7B-v0.1","tokenizer.vocab_size":32000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"kkCQxUk-PF9Ay_ZKDdKCh02YaGQ","tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"mixtral-instruct","name.huggingface":"mistralai\/Mixtral-8x7B-Instruct-v0.1","name.aliases":"mixtral-instruct, mixtral-8x7b-instruct","model_type":null,"name.from_cfg":"Mixtral-8x7B-Instruct-v0.1","n_params.as_str":"47B","n_params.as_int":47245688832,"n_params.from_name":null,"cfg.n_params":47245688832,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":32000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"MixtralForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Mixtral-8x7B-Instruct-v0.1","n_heads":32,"d_mlp":14336,"act_fn":"silu","d_vocab":32000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"MixtralForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"mistralai\/Mixtral-8x7B-Instruct-v0.1","window_size":null,"attn_types":["global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global","global"],"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":32000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":47245688832,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":8,"experts_per_token":2,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Mixtral-8x7B-Instruct-v0.1\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: MixtralForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: mistralai\/Mixtral-8x7B-Instruct-v0.1\nwindow_size: null\nattn_types:\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\n- global\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 47245688832\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: 8\nexperts_per_token: 2\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"mistralai\/Mixtral-8x7B-Instruct-v0.1","tokenizer.vocab_size":32000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"kkCQxUk-PF9Ay_ZKDdKCh02YaGQ","tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"bloom-560m","name.huggingface":"bigscience\/bloom-560m","name.aliases":"bloom-560m","model_type":"bloom","name.from_cfg":"bloom-560m","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"560m","cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":250880,"cfg.act_fn":"gelu_fast","cfg.positional_embedding_type":"alibi","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"BloomForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"bloom-560m","n_heads":16,"d_mlp":4096,"act_fn":"gelu_fast","d_vocab":250880,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"BloomForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"bigscience\/bloom-560m","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"alibi","final_rms":false,"d_vocab_out":250880,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":true,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: bloom-560m\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-560m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"bigscience\/bloom-560m","tokenizer.vocab_size":250680.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"BloomTokenizerFast","tokenizer.vocab_hash":"OO9NZoesMCpWsijo1O2DAbq9GqI","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (1024,)\n  W_E: (250880, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(1024,)"},"W_E":"(250880, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"}},"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"bloom-1b1","name.huggingface":"bigscience\/bloom-1b1","name.aliases":"bloom-1b1","model_type":"bloom","name.from_cfg":"bloom-1b1","n_params.as_str":"679M","n_params.as_int":679477248,"n_params.from_name":null,"cfg.n_params":679477248,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1536,"cfg.d_vocab":250880,"cfg.act_fn":"gelu_fast","cfg.positional_embedding_type":"alibi","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"BloomForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1536,"n_ctx":2048,"d_head":96,"model_name":"bloom-1b1","n_heads":16,"d_mlp":6144,"act_fn":"gelu_fast","d_vocab":250880,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"BloomForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"bigscience\/bloom-1b1","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0204124145,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"alibi","final_rms":false,"d_vocab_out":250880,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":679477248,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":true,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1536\nn_ctx: 2048\nd_head: 96\nmodel_name: bloom-1b1\nn_heads: 16\nd_mlp: 6144\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-1b1\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.020412414523193152\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 679477248\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"bigscience\/bloom-1b1","tokenizer.vocab_size":250680.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"BloomTokenizerFast","tokenizer.vocab_hash":"OO9NZoesMCpWsijo1O2DAbq9GqI","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (1536,)\n  W_E: (250880, 1536)\npos_embed:\n  W_pos: (2048, 1536)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1536,)\n    ln2:\n      '[w, b]': (1536,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1536, 96)\n      W_O: (16, 96, 1536)\n      '[b_Q, b_K, b_V]': (16, 96)\n      b_O: (1536,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1536, 6144)\n      b_in: (6144,)\n      W_out: (6144, 1536)\n      b_out: (1536,)\nln_final:\n  '[w, b]': (1536,)\nunembed:\n  W_U: (1536, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(1536,)"},"W_E":"(250880, 1536)"},"pos_embed":{"W_pos":"(2048, 1536)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1536,)"},"ln2":{"[w, b]":"(1536,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1536, 96)","W_O":"(16, 96, 1536)","[b_Q, b_K, b_V]":"(16, 96)","b_O":"(1536,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1536, 6144)","b_in":"(6144,)","W_out":"(6144, 1536)","b_out":"(1536,)"}}},"ln_final":{"[w, b]":"(1536,)"},"unembed":{"W_U":"(1536, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 1536)\nblocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 96)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 6144)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1536)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1536)\nhook_embed: (batch, seq_len, 1536)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"}},"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 96)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 6144)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1536)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"hook_embed":"(batch, seq_len, 1536)"}}
{"name.default_alias":"bloom-1b7","name.huggingface":"bigscience\/bloom-1b7","name.aliases":"bloom-1b7","model_type":"bloom","name.from_cfg":"bloom-1b7","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":null,"cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":250880,"cfg.act_fn":"gelu_fast","cfg.positional_embedding_type":"alibi","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"BloomForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"bloom-1b7","n_heads":16,"d_mlp":8192,"act_fn":"gelu_fast","d_vocab":250880,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"BloomForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"bigscience\/bloom-1b7","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"alibi","final_rms":false,"d_vocab_out":250880,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":true,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: bloom-1b7\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-1b7\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"bigscience\/bloom-1b7","tokenizer.vocab_size":250680.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"BloomTokenizerFast","tokenizer.vocab_hash":"OO9NZoesMCpWsijo1O2DAbq9GqI","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (2048,)\n  W_E: (250880, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(2048,)"},"W_E":"(250880, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"}},"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"bloom-3b","name.huggingface":"bigscience\/bloom-3b","name.aliases":"bloom-3b","model_type":"bloom","name.from_cfg":"bloom-3b","n_params.as_str":"2.4B","n_params.as_int":2359296000,"n_params.from_name":"3b","cfg.n_params":2359296000,"cfg.n_layers":30,"cfg.n_heads":32,"cfg.d_model":2560,"cfg.d_vocab":250880,"cfg.act_fn":"gelu_fast","cfg.positional_embedding_type":"alibi","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"BloomForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":30,"d_model":2560,"n_ctx":2048,"d_head":80,"model_name":"bloom-3b","n_heads":32,"d_mlp":10240,"act_fn":"gelu_fast","d_vocab":250880,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"BloomForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"bigscience\/bloom-3b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0158113883,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"alibi","final_rms":false,"d_vocab_out":250880,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":2359296000,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":true,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 30\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: bloom-3b\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.015811388300841896\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2359296000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"bigscience\/bloom-3b","tokenizer.vocab_size":250680.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"BloomTokenizerFast","tokenizer.vocab_hash":"OO9NZoesMCpWsijo1O2DAbq9GqI","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (2560,)\n  W_E: (250880, 2560)\npos_embed:\n  W_pos: (2048, 2560)\nblocks:\n  '[0-29]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(2560,)"},"W_E":"(250880, 2560)"},"pos_embed":{"W_pos":"(2048, 2560)"},"blocks":{"[0-29]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 2560)\nblocks:\n  '[0-29]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"}},"blocks":{"[0-29]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"bloom-7b1","name.huggingface":"bigscience\/bloom-7b1","name.aliases":"bloom-7b1","model_type":"bloom","name.from_cfg":"bloom-7b1","n_params.as_str":"6.0B","n_params.as_int":6039797760,"n_params.from_name":null,"cfg.n_params":6039797760,"cfg.n_layers":30,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":250880,"cfg.act_fn":"gelu_fast","cfg.positional_embedding_type":"alibi","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"BloomForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":30,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"bloom-7b1","n_heads":32,"d_mlp":16384,"act_fn":"gelu_fast","d_vocab":250880,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"BloomForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"bigscience\/bloom-7b1","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"alibi","final_rms":false,"d_vocab_out":250880,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":6039797760,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":true,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 30\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: bloom-7b1\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-7b1\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6039797760\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"bigscience\/bloom-7b1","tokenizer.vocab_size":250680.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"BloomTokenizerFast","tokenizer.vocab_hash":"OO9NZoesMCpWsijo1O2DAbq9GqI","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (4096,)\n  W_E: (250880, 4096)\npos_embed:\n  W_pos: (2048, 4096)\nblocks:\n  '[0-29]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(4096,)"},"W_E":"(250880, 4096)"},"pos_embed":{"W_pos":"(2048, 4096)"},"blocks":{"[0-29]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 4096)\nblocks:\n  '[0-29]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"}},"blocks":{"[0-29]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"santacoder","name.huggingface":"bigcode\/santacoder","name.aliases":"santacoder","model_type":null,"name.from_cfg":"santacoder","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":null,"cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":49280,"cfg.act_fn":"gelu_fast","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadCustomModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"santacoder","n_heads":16,"d_mlp":8192,"act_fn":"gelu_fast","d_vocab":49280,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadCustomModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"bigcode\/santacoder","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":49280,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: santacoder\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu_fast\nd_vocab: 49280\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadCustomModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigcode\/santacoder\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 49280\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"bigcode\/santacoder","tokenizer.vocab_size":49152.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"GiKC-dU7fpR4sGNkpwn7JKK6qys","tensor_shapes.state_dict":"embed:\n  W_E: (49280, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 49280)\n  b_U: (49280,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(49280, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 49280)","b_U":"(49280,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2048)"}}
{"name.default_alias":"qwen-1.8b","name.huggingface":"Qwen\/Qwen-1_8B","name.aliases":"qwen-1.8b","model_type":"qwen","name.from_cfg":"Qwen-1_8B","n_params.as_str":"1.2B","n_params.as_int":1214251008,"n_params.from_name":"1.8b","cfg.n_params":1214251008,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"QWenLMHeadModel","cfg.normalization_type":"RMS","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"Qwen-1_8B","n_heads":16,"d_mlp":5504,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"QWenLMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen-1_8B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":1214251008,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-1_8B\nn_heads: 16\nd_mlp: 5504\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-1_8B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 1214251008\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"qwen-7b","name.huggingface":"Qwen\/Qwen-7B","name.aliases":"qwen-7b","model_type":"qwen","name.from_cfg":"Qwen-7B","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"QWenLMHeadModel","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Qwen-7B","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"QWenLMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen-7B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-7B\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-7B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"qwen-14b","name.huggingface":"Qwen\/Qwen-14B","name.aliases":"qwen-14b","model_type":"qwen","name.from_cfg":"Qwen-14B","n_params.as_str":"13B","n_params.as_int":12609126400,"n_params.from_name":"14b","cfg.n_params":12609126400,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":152064,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"QWenLMHeadModel","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"Qwen-14B","n_heads":40,"d_mlp":13696,"act_fn":"silu","d_vocab":152064,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"QWenLMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen-14B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":152064,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":12609126400,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-14B\nn_heads: 40\nd_mlp: 13696\nact_fn: silu\nd_vocab: 152064\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-14B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 152064\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 12609126400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"qwen-1.8b-chat","name.huggingface":"Qwen\/Qwen-1_8B-Chat","name.aliases":"qwen-1.8b-chat","model_type":"qwen","name.from_cfg":"Qwen-1_8B-Chat","n_params.as_str":"1.2B","n_params.as_int":1214251008,"n_params.from_name":"1.8b","cfg.n_params":1214251008,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"QWenLMHeadModel","cfg.normalization_type":"RMS","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"Qwen-1_8B-Chat","n_heads":16,"d_mlp":5504,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"QWenLMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen-1_8B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":1214251008,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-1_8B-Chat\nn_heads: 16\nd_mlp: 5504\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-1_8B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 1214251008\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"qwen-7b-chat","name.huggingface":"Qwen\/Qwen-7B-Chat","name.aliases":"qwen-7b-chat","model_type":"qwen","name.from_cfg":"Qwen-7B-Chat","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"QWenLMHeadModel","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Qwen-7B-Chat","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"QWenLMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen-7B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-7B-Chat\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-7B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"qwen-14b-chat","name.huggingface":"Qwen\/Qwen-14B-Chat","name.aliases":"qwen-14b-chat","model_type":"qwen","name.from_cfg":"Qwen-14B-Chat","n_params.as_str":"13B","n_params.as_int":12609126400,"n_params.from_name":"14b","cfg.n_params":12609126400,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":152064,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"QWenLMHeadModel","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"Qwen-14B-Chat","n_heads":40,"d_mlp":13696,"act_fn":"silu","d_vocab":152064,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"QWenLMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen-14B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":152064,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":12609126400,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-14B-Chat\nn_heads: 40\nd_mlp: 13696\nact_fn: silu\nd_vocab: 152064\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-14B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 152064\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 12609126400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"qwen1.5-0.5b","name.huggingface":"Qwen\/Qwen1.5-0.5B","name.aliases":"qwen1.5-0.5b","model_type":"qwen","name.from_cfg":"Qwen1.5-0.5B","n_params.as_str":"308M","n_params.as_int":308281344,"n_params.from_name":"0.5b","cfg.n_params":308281344,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"Qwen1.5-0.5B","n_heads":16,"d_mlp":2816,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-0.5B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":64,"n_params":308281344,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: Qwen1.5-0.5B\nn_heads: 16\nd_mlp: 2816\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-0.5B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 64\nn_params: 308281344\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-0.5B","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      w: (1024,)\n    ln2:\n      w: (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      '[W_in, W_gate]': (1024, 2816)\n      W_out: (2816, 1024)\n      b_in: (2816,)\n      b_out: (1024,)\nln_final:\n  w: (1024,)\nunembed:\n  W_U: (1024, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 1024)"},"blocks":{"[0-23]":{"ln1":{"w":"(1024,)"},"ln2":{"w":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"[W_in, W_gate]":"(1024, 2816)","W_out":"(2816, 1024)","b_in":"(2816,)","b_out":"(1024,)"}}},"ln_final":{"w":"(1024,)"},"unembed":{"W_U":"(1024, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 2816)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 2816)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"qwen1.5-0.5b-chat","name.huggingface":"Qwen\/Qwen1.5-0.5B-Chat","name.aliases":"qwen1.5-0.5b-chat","model_type":"qwen","name.from_cfg":"Qwen1.5-0.5B-Chat","n_params.as_str":"308M","n_params.as_int":308281344,"n_params.from_name":"0.5b","cfg.n_params":308281344,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":2048,"d_head":64,"model_name":"Qwen1.5-0.5B-Chat","n_heads":16,"d_mlp":2816,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-0.5B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":64,"n_params":308281344,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: Qwen1.5-0.5B-Chat\nn_heads: 16\nd_mlp: 2816\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-0.5B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 64\nn_params: 308281344\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-0.5B-Chat","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      w: (1024,)\n    ln2:\n      w: (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      '[W_in, W_gate]': (1024, 2816)\n      W_out: (2816, 1024)\n      b_in: (2816,)\n      b_out: (1024,)\nln_final:\n  w: (1024,)\nunembed:\n  W_U: (1024, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 1024)"},"blocks":{"[0-23]":{"ln1":{"w":"(1024,)"},"ln2":{"w":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"[W_in, W_gate]":"(1024, 2816)","W_out":"(2816, 1024)","b_in":"(2816,)","b_out":"(1024,)"}}},"ln_final":{"w":"(1024,)"},"unembed":{"W_U":"(1024, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 2816)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 2816)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"qwen1.5-1.8b","name.huggingface":"Qwen\/Qwen1.5-1.8B","name.aliases":"qwen1.5-1.8b","model_type":"qwen","name.from_cfg":"Qwen1.5-1.8B","n_params.as_str":"1.2B","n_params.as_int":1214251008,"n_params.from_name":"1.8b","cfg.n_params":1214251008,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-1.8B","n_heads":16,"d_mlp":5504,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-1.8B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":1214251008,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-1.8B\nn_heads: 16\nd_mlp: 5504\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-1.8B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 1214251008\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-1.8B","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      w: (2048,)\n    ln2:\n      w: (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (2048, 5504)\n      W_out: (5504, 2048)\n      b_in: (5504,)\n      b_out: (2048,)\nln_final:\n  w: (2048,)\nunembed:\n  W_U: (2048, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 2048)"},"blocks":{"[0-23]":{"ln1":{"w":"(2048,)"},"ln2":{"w":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(2048, 5504)","W_out":"(5504, 2048)","b_in":"(5504,)","b_out":"(2048,)"}}},"ln_final":{"w":"(2048,)"},"unembed":{"W_U":"(2048, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 5504)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 5504)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"qwen1.5-1.8b-chat","name.huggingface":"Qwen\/Qwen1.5-1.8B-Chat","name.aliases":"qwen1.5-1.8b-chat","model_type":"qwen","name.from_cfg":"Qwen1.5-1.8B-Chat","n_params.as_str":"1.2B","n_params.as_int":1214251008,"n_params.from_name":"1.8b","cfg.n_params":1214251008,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-1.8B-Chat","n_heads":16,"d_mlp":5504,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-1.8B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":1214251008,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-1.8B-Chat\nn_heads: 16\nd_mlp: 5504\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-1.8B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 1214251008\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-1.8B-Chat","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      w: (2048,)\n    ln2:\n      w: (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (2048, 5504)\n      W_out: (5504, 2048)\n      b_in: (5504,)\n      b_out: (2048,)\nln_final:\n  w: (2048,)\nunembed:\n  W_U: (2048, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 2048)"},"blocks":{"[0-23]":{"ln1":{"w":"(2048,)"},"ln2":{"w":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(2048, 5504)","W_out":"(5504, 2048)","b_in":"(5504,)","b_out":"(2048,)"}}},"ln_final":{"w":"(2048,)"},"unembed":{"W_U":"(2048, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 5504)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 5504)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"qwen1.5-4b","name.huggingface":"Qwen\/Qwen1.5-4B","name.aliases":"qwen1.5-4b","model_type":"qwen","name.from_cfg":"Qwen1.5-4B","n_params.as_str":"3.2B","n_params.as_int":3171942400,"n_params.from_name":"4b","cfg.n_params":3171942400,"cfg.n_layers":40,"cfg.n_heads":20,"cfg.d_model":2560,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":2560,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-4B","n_heads":20,"d_mlp":6912,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-4B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":3171942400,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":5000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 2560\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-4B\nn_heads: 20\nd_mlp: 6912\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-4B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 3171942400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 5000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-4B","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 2560)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (2560,)\n    ln2:\n      w: (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 2560, 128)\n      W_O: (20, 128, 2560)\n      '[b_Q, b_K, b_V]': (20, 128)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (2560, 6912)\n      W_out: (6912, 2560)\n      b_in: (6912,)\n      b_out: (2560,)\nln_final:\n  w: (2560,)\nunembed:\n  W_U: (2560, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 2560)"},"blocks":{"[0-39]":{"ln1":{"w":"(2560,)"},"ln2":{"w":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 2560, 128)","W_O":"(20, 128, 2560)","[b_Q, b_K, b_V]":"(20, 128)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(2560, 6912)","W_out":"(6912, 2560)","b_in":"(6912,)","b_out":"(2560,)"}}},"ln_final":{"w":"(2560,)"},"unembed":{"W_U":"(2560, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        20, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 6912)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 20, 128)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 6912)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"qwen1.5-4b-chat","name.huggingface":"Qwen\/Qwen1.5-4B-Chat","name.aliases":"qwen1.5-4b-chat","model_type":"qwen","name.from_cfg":"Qwen1.5-4B-Chat","n_params.as_str":"3.2B","n_params.as_int":3171942400,"n_params.from_name":"4b","cfg.n_params":3171942400,"cfg.n_layers":40,"cfg.n_heads":20,"cfg.d_model":2560,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":2560,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-4B-Chat","n_heads":20,"d_mlp":6912,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-4B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":3171942400,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":5000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 2560\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-4B-Chat\nn_heads: 20\nd_mlp: 6912\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-4B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 3171942400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 5000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-4B-Chat","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 2560)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (2560,)\n    ln2:\n      w: (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 2560, 128)\n      W_O: (20, 128, 2560)\n      '[b_Q, b_K, b_V]': (20, 128)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (2560, 6912)\n      W_out: (6912, 2560)\n      b_in: (6912,)\n      b_out: (2560,)\nln_final:\n  w: (2560,)\nunembed:\n  W_U: (2560, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 2560)"},"blocks":{"[0-39]":{"ln1":{"w":"(2560,)"},"ln2":{"w":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 2560, 128)","W_O":"(20, 128, 2560)","[b_Q, b_K, b_V]":"(20, 128)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(2560, 6912)","W_out":"(6912, 2560)","b_in":"(6912,)","b_out":"(2560,)"}}},"ln_final":{"w":"(2560,)"},"unembed":{"W_U":"(2560, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        20, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 6912)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 20, 128)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 6912)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"qwen1.5-7b","name.huggingface":"Qwen\/Qwen1.5-7B","name.aliases":"qwen1.5-7b","model_type":"qwen","name.from_cfg":"Qwen1.5-7B","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-7B","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-7B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-7B\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-7B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-7B","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      W_out: (11008, 4096)\n      b_in: (11008,)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","W_out":"(11008, 4096)","b_in":"(11008,)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"qwen1.5-7b-chat","name.huggingface":"Qwen\/Qwen1.5-7B-Chat","name.aliases":"qwen1.5-7b-chat","model_type":"qwen","name.from_cfg":"Qwen1.5-7B-Chat","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"7b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":151936,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-7B-Chat","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":151936,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-7B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":151936,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-7B-Chat\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-7B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-7B-Chat","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      W_out: (11008, 4096)\n      b_in: (11008,)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","W_out":"(11008, 4096)","b_in":"(11008,)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"qwen1.5-14b","name.huggingface":"Qwen\/Qwen1.5-14B","name.aliases":"qwen1.5-14b","model_type":"qwen","name.from_cfg":"Qwen1.5-14B","n_params.as_str":"13B","n_params.as_int":12609126400,"n_params.from_name":"14b","cfg.n_params":12609126400,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":152064,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-14B","n_heads":40,"d_mlp":13696,"act_fn":"silu","d_vocab":152064,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-14B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":152064,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":12609126400,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-14B\nn_heads: 40\nd_mlp: 13696\nact_fn: silu\nd_vocab: 152064\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-14B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 152064\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 12609126400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-14B","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (152064, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (5120,)\n    ln2:\n      w: (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (5120, 13696)\n      W_out: (13696, 5120)\n      b_in: (13696,)\n      b_out: (5120,)\nln_final:\n  w: (5120,)\nunembed:\n  W_U: (5120, 152064)\n  b_U: (152064,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(152064, 5120)"},"blocks":{"[0-39]":{"ln1":{"w":"(5120,)"},"ln2":{"w":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(5120, 13696)","W_out":"(13696, 5120)","b_in":"(13696,)","b_out":"(5120,)"}}},"ln_final":{"w":"(5120,)"},"unembed":{"W_U":"(5120, 152064)","b_U":"(152064,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13696)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 13696)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"qwen1.5-14b-chat","name.huggingface":"Qwen\/Qwen1.5-14B-Chat","name.aliases":"qwen1.5-14b-chat","model_type":"qwen","name.from_cfg":"Qwen1.5-14B-Chat","n_params.as_str":"13B","n_params.as_int":12609126400,"n_params.from_name":"14b","cfg.n_params":12609126400,"cfg.n_layers":40,"cfg.n_heads":40,"cfg.d_model":5120,"cfg.d_vocab":152064,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Qwen2ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":40,"d_model":5120,"n_ctx":2048,"d_head":128,"model_name":"Qwen1.5-14B-Chat","n_heads":40,"d_mlp":13696,"act_fn":"silu","d_vocab":152064,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Qwen2ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"Qwen\/Qwen1.5-14B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":152064,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":12609126400,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":true,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":1000000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen1.5-14B-Chat\nn_heads: 40\nd_mlp: 13696\nact_fn: silu\nd_vocab: 152064\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Qwen2ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen1.5-14B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 152064\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 12609126400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"Qwen\/Qwen1.5-14B-Chat","tokenizer.vocab_size":151643.0,"tokenizer.max_len":32768,"tokenizer.class":"Qwen2TokenizerFast","tokenizer.vocab_hash":"vakQOjPaHpZ23kxcqX0tTXi2EzQ","tensor_shapes.state_dict":"embed:\n  W_E: (152064, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (5120,)\n    ln2:\n      w: (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (5120, 13696)\n      W_out: (13696, 5120)\n      b_in: (13696,)\n      b_out: (5120,)\nln_final:\n  w: (5120,)\nunembed:\n  W_U: (5120, 152064)\n  b_U: (152064,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(152064, 5120)"},"blocks":{"[0-39]":{"ln1":{"w":"(5120,)"},"ln2":{"w":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(5120, 13696)","W_out":"(13696, 5120)","b_in":"(13696,)","b_out":"(5120,)"}}},"ln_final":{"w":"(5120,)"},"unembed":{"W_U":"(5120, 152064)","b_U":"(152064,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13696)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 13696)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"phi-1","name.huggingface":"microsoft\/phi-1","name.aliases":"phi-1","model_type":"phi","name.from_cfg":"phi-1","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":null,"cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":32,"cfg.d_model":2048,"cfg.d_vocab":51200,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"PhiForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":64,"model_name":"phi-1","n_heads":32,"d_mlp":8192,"act_fn":"gelu_new","d_vocab":51200,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"PhiForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"microsoft\/phi-1","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":51200,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 64\nmodel_name: phi-1\nn_heads: 32\nd_mlp: 8192\nact_fn: gelu_new\nd_vocab: 51200\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: PhiForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: microsoft\/phi-1\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 51200\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":"embed:\n  W_E: (51200, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2048, 64)\n      W_O: (32, 64, 2048)\n      '[b_Q, b_K, b_V]': (32, 64)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 51200)\n  b_U: (51200,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(51200, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2048, 64)","W_O":"(32, 64, 2048)","[b_Q, b_K, b_V]":"(32, 64)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 51200)","b_U":"(51200,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 64)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"phi-1_5","name.huggingface":"microsoft\/phi-1_5","name.aliases":"phi-1_5","model_type":"phi","name.from_cfg":"phi-1_5","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":null,"cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":32,"cfg.d_model":2048,"cfg.d_vocab":51200,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"PhiForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":64,"model_name":"phi-1_5","n_heads":32,"d_mlp":8192,"act_fn":"gelu_new","d_vocab":51200,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"PhiForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"microsoft\/phi-1_5","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":51200,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 64\nmodel_name: phi-1_5\nn_heads: 32\nd_mlp: 8192\nact_fn: gelu_new\nd_vocab: 51200\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: PhiForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: microsoft\/phi-1_5\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 51200\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":"embed:\n  W_E: (51200, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2048, 64)\n      W_O: (32, 64, 2048)\n      '[b_Q, b_K, b_V]': (32, 64)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 51200)\n  b_U: (51200,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(51200, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2048, 64)","W_O":"(32, 64, 2048)","[b_Q, b_K, b_V]":"(32, 64)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 51200)","b_U":"(51200,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 64)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"phi-2","name.huggingface":"microsoft\/phi-2","name.aliases":"phi-2","model_type":"phi","name.from_cfg":"phi-2","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":null,"cfg.n_params":2516582400,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":2560,"cfg.d_vocab":51200,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":true,"cfg.original_architecture":"PhiForCausalLM","cfg.normalization_type":"LN","config.raw__":{"n_layers":32,"d_model":2560,"n_ctx":2048,"d_head":80,"model_name":"phi-2","n_heads":32,"d_mlp":10240,"act_fn":"gelu_new","d_vocab":51200,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"PhiForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"microsoft\/phi-2","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":51200,"parallel_attn_mlp":true,"rotary_dim":32,"n_params":2516582400,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: phi-2\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu_new\nd_vocab: 51200\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: PhiForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: microsoft\/phi-2\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 51200\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":"embed:\n  W_E: (51200, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 51200)\n  b_U: (51200,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(51200, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 51200)","b_U":"(51200,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"phi-3","name.huggingface":"microsoft\/Phi-3-mini-4k-instruct","name.aliases":"phi-3","model_type":"phi","name.from_cfg":"Phi-3-mini-4k-instruct","n_params.as_str":"3.6B","n_params.as_int":3623878656,"n_params.from_name":null,"cfg.n_params":3623878656,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":3072,"cfg.d_vocab":32064,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"Phi3ForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":3072,"n_ctx":4096,"d_head":96,"model_name":"Phi-3-mini-4k-instruct","n_heads":32,"d_mlp":8192,"act_fn":"silu","d_vocab":32064,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"Phi3ForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"microsoft\/Phi-3-mini-4k-instruct","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":false,"d_vocab_out":32064,"parallel_attn_mlp":false,"rotary_dim":96,"n_params":3623878656,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":true,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 3072\nn_ctx: 4096\nd_head: 96\nmodel_name: Phi-3-mini-4k-instruct\nn_heads: 32\nd_mlp: 8192\nact_fn: silu\nd_vocab: 32064\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: Phi3ForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: microsoft\/Phi-3-mini-4k-instruct\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 32064\nparallel_attn_mlp: false\nrotary_dim: 96\nn_params: 3623878656\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: true\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":null,"tokenizer.vocab_size":null,"tokenizer.max_len":null,"tokenizer.class":null,"tokenizer.vocab_hash":null,"tensor_shapes.state_dict":"embed:\n  W_E: (32064, 3072)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (3072,)\n    ln2:\n      w: (3072,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 3072, 96)\n      W_O: (32, 96, 3072)\n      '[b_Q, b_K, b_V]': (32, 96)\n      b_O: (3072,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 96)\n    mlp:\n      '[W_in, W_gate]': (3072, 8192)\n      W_out: (8192, 3072)\n      b_in: (8192,)\n      b_out: (3072,)\nln_final:\n  w: (3072,)\nunembed:\n  W_U: (3072, 32064)\n  b_U: (32064,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32064, 3072)"},"blocks":{"[0-31]":{"ln1":{"w":"(3072,)"},"ln2":{"w":"(3072,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 3072, 96)","W_O":"(32, 96, 3072)","[b_Q, b_K, b_V]":"(32, 96)","b_O":"(3072,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 96)"},"mlp":{"[W_in, W_gate]":"(3072, 8192)","W_out":"(8192, 3072)","b_in":"(8192,)","b_out":"(3072,)"}}},"ln_final":{"w":"(3072,)"},"unembed":{"W_U":"(3072, 32064)","b_U":"(32064,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 3072)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 96)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 3072)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 3072)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 3072)\nhook_embed: (batch, seq_len, 3072)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 96)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 3072)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"hook_embed":"(batch, seq_len, 3072)"}}
{"name.default_alias":"gemma-2b","name.huggingface":"google\/gemma-2b","name.aliases":"gemma-2b","model_type":"gemma","name.from_cfg":"gemma-2b","n_params.as_str":"2.1B","n_params.as_int":2113929216,"n_params.from_name":"2b","cfg.n_params":2113929216,"cfg.n_layers":18,"cfg.n_heads":8,"cfg.d_model":2048,"cfg.d_vocab":256000,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GemmaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":18,"d_model":2048,"n_ctx":8192,"d_head":256,"model_name":"gemma-2b","n_heads":8,"d_mlp":16384,"act_fn":"gelu_new","d_vocab":256000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GemmaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"google\/gemma-2b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":256000,"parallel_attn_mlp":false,"rotary_dim":256,"n_params":2113929216,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":1,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 18\nd_model: 2048\nn_ctx: 8192\nd_head: 256\nmodel_name: gemma-2b\nn_heads: 8\nd_mlp: 16384\nact_fn: gelu_new\nd_vocab: 256000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GemmaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: google\/gemma-2b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 256000\nparallel_attn_mlp: false\nrotary_dim: 256\nn_params: 2113929216\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 1\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"google\/gemma-2b","tokenizer.vocab_size":256000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GemmaTokenizerFast","tokenizer.vocab_hash":"87mmm7o-5SoGMD05LzhcJdB_XBk","tensor_shapes.state_dict":"embed:\n  W_E: (256000, 2048)\nblocks:\n  '[0-17]':\n    ln1:\n      w: (2048,)\n    ln2:\n      w: (2048,)\n    attn:\n      W_Q: (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      b_Q: (8, 256)\n      b_O: (2048,)\n      '[_W_K, _W_V]': (1, 2048, 256)\n      '[_b_K, _b_V]': (1, 256)\n      mask: (8192, 8192)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (8192, 256)\n    mlp:\n      '[W_in, W_gate]': (2048, 16384)\n      W_out: (16384, 2048)\n      b_in: (16384,)\n      b_out: (2048,)\nln_final:\n  w: (2048,)\nunembed:\n  W_U: (2048, 256000)\n  b_U: (256000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(256000, 2048)"},"blocks":{"[0-17]":{"ln1":{"w":"(2048,)"},"ln2":{"w":"(2048,)"},"attn":{"W_Q":"(8, 2048, 256)","W_O":"(8, 256, 2048)","b_Q":"(8, 256)","b_O":"(2048,)","[_W_K, _W_V]":"(1, 2048, 256)","[_b_K, _b_V]":"(1, 256)","mask":"(8192, 8192)","IGNORE":"()","[rotary_sin, rotary_cos]":"(8192, 256)"},"mlp":{"[W_in, W_gate]":"(2048, 16384)","W_out":"(16384, 2048)","b_in":"(16384,)","b_out":"(2048,)"}}},"ln_final":{"w":"(2048,)"},"unembed":{"W_U":"(2048, 256000)","b_U":"(256000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-17]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 8, 256)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 1, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-17]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 8, 256)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 1, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"gemma-7b","name.huggingface":"google\/gemma-7b","name.aliases":"gemma-7b","model_type":"gemma","name.from_cfg":"gemma-7b","n_params.as_str":"7.8B","n_params.as_int":7751073792,"n_params.from_name":"7b","cfg.n_params":7751073792,"cfg.n_layers":28,"cfg.n_heads":16,"cfg.d_model":3072,"cfg.d_vocab":256000,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GemmaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":28,"d_model":3072,"n_ctx":8192,"d_head":256,"model_name":"gemma-7b","n_heads":16,"d_mlp":24576,"act_fn":"gelu_new","d_vocab":256000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GemmaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"google\/gemma-7b","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":256000,"parallel_attn_mlp":false,"rotary_dim":256,"n_params":7751073792,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":16,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 28\nd_model: 3072\nn_ctx: 8192\nd_head: 256\nmodel_name: gemma-7b\nn_heads: 16\nd_mlp: 24576\nact_fn: gelu_new\nd_vocab: 256000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GemmaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: google\/gemma-7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 256000\nparallel_attn_mlp: false\nrotary_dim: 256\nn_params: 7751073792\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 16\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"google\/gemma-7b","tokenizer.vocab_size":256000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GemmaTokenizerFast","tokenizer.vocab_hash":"87mmm7o-5SoGMD05LzhcJdB_XBk","tensor_shapes.state_dict":"embed:\n  W_E: (256000, 3072)\nblocks:\n  '[0-27]':\n    ln1:\n      w: (3072,)\n    ln2:\n      w: (3072,)\n    attn:\n      '[W_Q, _W_K, _W_V]': (16, 3072, 256)\n      W_O: (16, 256, 3072)\n      '[b_Q, _b_K, _b_V]': (16, 256)\n      b_O: (3072,)\n      mask: (8192, 8192)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (8192, 256)\n    mlp:\n      '[W_in, W_gate]': (3072, 24576)\n      W_out: (24576, 3072)\n      b_in: (24576,)\n      b_out: (3072,)\nln_final:\n  w: (3072,)\nunembed:\n  W_U: (3072, 256000)\n  b_U: (256000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(256000, 3072)"},"blocks":{"[0-27]":{"ln1":{"w":"(3072,)"},"ln2":{"w":"(3072,)"},"attn":{"[W_Q, _W_K, _W_V]":"(16, 3072, 256)","W_O":"(16, 256, 3072)","[b_Q, _b_K, _b_V]":"(16, 256)","b_O":"(3072,)","mask":"(8192, 8192)","IGNORE":"()","[rotary_sin, rotary_cos]":"(8192, 256)"},"mlp":{"[W_in, W_gate]":"(3072, 24576)","W_out":"(24576, 3072)","b_in":"(24576,)","b_out":"(3072,)"}}},"ln_final":{"w":"(3072,)"},"unembed":{"W_U":"(3072, 256000)","b_U":"(256000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-27]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 3072)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 3072)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 3072)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 3072)\nhook_embed: (batch, seq_len, 3072)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-27]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 256)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 3072)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"hook_embed":"(batch, seq_len, 3072)"}}
{"name.default_alias":"gemma-2b-it","name.huggingface":"google\/gemma-2b-it","name.aliases":"gemma-2b-it","model_type":"gemma","name.from_cfg":"gemma-2b-it","n_params.as_str":"2.1B","n_params.as_int":2113929216,"n_params.from_name":"2b","cfg.n_params":2113929216,"cfg.n_layers":18,"cfg.n_heads":8,"cfg.d_model":2048,"cfg.d_vocab":256000,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GemmaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":18,"d_model":2048,"n_ctx":8192,"d_head":256,"model_name":"gemma-2b-it","n_heads":8,"d_mlp":16384,"act_fn":"gelu_new","d_vocab":256000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GemmaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"google\/gemma-2b-it","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":256000,"parallel_attn_mlp":false,"rotary_dim":256,"n_params":2113929216,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":1,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 18\nd_model: 2048\nn_ctx: 8192\nd_head: 256\nmodel_name: gemma-2b-it\nn_heads: 8\nd_mlp: 16384\nact_fn: gelu_new\nd_vocab: 256000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GemmaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: google\/gemma-2b-it\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 256000\nparallel_attn_mlp: false\nrotary_dim: 256\nn_params: 2113929216\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 1\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"google\/gemma-2b-it","tokenizer.vocab_size":256000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GemmaTokenizerFast","tokenizer.vocab_hash":"87mmm7o-5SoGMD05LzhcJdB_XBk","tensor_shapes.state_dict":"embed:\n  W_E: (256000, 2048)\nblocks:\n  '[0-17]':\n    ln1:\n      w: (2048,)\n    ln2:\n      w: (2048,)\n    attn:\n      W_Q: (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      b_Q: (8, 256)\n      b_O: (2048,)\n      '[_W_K, _W_V]': (1, 2048, 256)\n      '[_b_K, _b_V]': (1, 256)\n      mask: (8192, 8192)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (8192, 256)\n    mlp:\n      '[W_in, W_gate]': (2048, 16384)\n      W_out: (16384, 2048)\n      b_in: (16384,)\n      b_out: (2048,)\nln_final:\n  w: (2048,)\nunembed:\n  W_U: (2048, 256000)\n  b_U: (256000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(256000, 2048)"},"blocks":{"[0-17]":{"ln1":{"w":"(2048,)"},"ln2":{"w":"(2048,)"},"attn":{"W_Q":"(8, 2048, 256)","W_O":"(8, 256, 2048)","b_Q":"(8, 256)","b_O":"(2048,)","[_W_K, _W_V]":"(1, 2048, 256)","[_b_K, _b_V]":"(1, 256)","mask":"(8192, 8192)","IGNORE":"()","[rotary_sin, rotary_cos]":"(8192, 256)"},"mlp":{"[W_in, W_gate]":"(2048, 16384)","W_out":"(16384, 2048)","b_in":"(16384,)","b_out":"(2048,)"}}},"ln_final":{"w":"(2048,)"},"unembed":{"W_U":"(2048, 256000)","b_U":"(256000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-17]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 8, 256)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 1, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-17]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 8, 256)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 1, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"gemma-7b-it","name.huggingface":"google\/gemma-7b-it","name.aliases":"gemma-7b-it","model_type":"gemma","name.from_cfg":"gemma-7b-it","n_params.as_str":"7.8B","n_params.as_int":7751073792,"n_params.from_name":"7b","cfg.n_params":7751073792,"cfg.n_layers":28,"cfg.n_heads":16,"cfg.d_model":3072,"cfg.d_vocab":256000,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GemmaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":28,"d_model":3072,"n_ctx":8192,"d_head":256,"model_name":"gemma-7b-it","n_heads":16,"d_mlp":24576,"act_fn":"gelu_new","d_vocab":256000,"eps":0.000001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GemmaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"google\/gemma-7b-it","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.02,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":256000,"parallel_attn_mlp":false,"rotary_dim":256,"n_params":7751073792,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":16,"post_embedding_ln":false,"rotary_base":10000.0,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 28\nd_model: 3072\nn_ctx: 8192\nd_head: 256\nmodel_name: gemma-7b-it\nn_heads: 16\nd_mlp: 24576\nact_fn: gelu_new\nd_vocab: 256000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GemmaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: google\/gemma-7b-it\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 256000\nparallel_attn_mlp: false\nrotary_dim: 256\nn_params: 7751073792\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 16\npost_embedding_ln: false\nrotary_base: 10000.0\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"google\/gemma-7b-it","tokenizer.vocab_size":256000.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"GemmaTokenizerFast","tokenizer.vocab_hash":"87mmm7o-5SoGMD05LzhcJdB_XBk","tensor_shapes.state_dict":"embed:\n  W_E: (256000, 3072)\nblocks:\n  '[0-27]':\n    ln1:\n      w: (3072,)\n    ln2:\n      w: (3072,)\n    attn:\n      '[W_Q, _W_K, _W_V]': (16, 3072, 256)\n      W_O: (16, 256, 3072)\n      '[b_Q, _b_K, _b_V]': (16, 256)\n      b_O: (3072,)\n      mask: (8192, 8192)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (8192, 256)\n    mlp:\n      '[W_in, W_gate]': (3072, 24576)\n      W_out: (24576, 3072)\n      b_in: (24576,)\n      b_out: (3072,)\nln_final:\n  w: (3072,)\nunembed:\n  W_U: (3072, 256000)\n  b_U: (256000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(256000, 3072)"},"blocks":{"[0-27]":{"ln1":{"w":"(3072,)"},"ln2":{"w":"(3072,)"},"attn":{"[W_Q, _W_K, _W_V]":"(16, 3072, 256)","W_O":"(16, 256, 3072)","[b_Q, _b_K, _b_V]":"(16, 256)","b_O":"(3072,)","mask":"(8192, 8192)","IGNORE":"()","[rotary_sin, rotary_cos]":"(8192, 256)"},"mlp":{"[W_in, W_gate]":"(3072, 24576)","W_out":"(24576, 3072)","b_in":"(24576,)","b_out":"(3072,)"}}},"ln_final":{"w":"(3072,)"},"unembed":{"W_U":"(3072, 256000)","b_U":"(256000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-27]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 3072)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 3072)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 3072)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 3072)\nhook_embed: (batch, seq_len, 3072)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-27]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 256)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 3072)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"hook_embed":"(batch, seq_len, 3072)"}}
{"name.default_alias":"yi-6b","name.huggingface":"01-ai\/Yi-6B","name.aliases":"yi-6b, Yi-6B","model_type":"yi","name.from_cfg":"Yi-6B","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"6b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":64000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"Yi-6B","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":64000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"01-ai\/Yi-6B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":64000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":4,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: Yi-6B\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 64000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: 01-ai\/Yi-6B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 64000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 4\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"01-ai\/Yi-6B","tokenizer.vocab_size":63992.0,"tokenizer.max_len":4096,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"VGXAFrTzytwGdUlX6AWH0NacncM","tensor_shapes.state_dict":"embed:\n  W_E: (64000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      W_Q: (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      b_Q: (32, 128)\n      b_O: (4096,)\n      '[_W_K, _W_V]': (4, 4096, 128)\n      '[_b_K, _b_V]': (4, 128)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      W_out: (11008, 4096)\n      b_in: (11008,)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 64000)\n  b_U: (64000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(64000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"W_Q":"(32, 4096, 128)","W_O":"(32, 128, 4096)","b_Q":"(32, 128)","b_O":"(4096,)","[_W_K, _W_V]":"(4, 4096, 128)","[_b_K, _b_V]":"(4, 128)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","W_out":"(11008, 4096)","b_in":"(11008,)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 64000)","b_U":"(64000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 4, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 32, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 4, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"yi-34b","name.huggingface":"01-ai\/Yi-34B","name.aliases":"yi-34b, Yi-34B","model_type":"yi","name.from_cfg":"Yi-34B","n_params.as_str":"39B","n_params.as_int":38755368960,"n_params.from_name":"34b","cfg.n_params":38755368960,"cfg.n_layers":60,"cfg.n_heads":56,"cfg.d_model":7168,"cfg.d_vocab":64000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":60,"d_model":7168,"n_ctx":4096,"d_head":128,"model_name":"Yi-34B","n_heads":56,"d_mlp":20480,"act_fn":"silu","d_vocab":64000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"01-ai\/Yi-34B","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0094491118,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":64000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":38755368960,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 60\nd_model: 7168\nn_ctx: 4096\nd_head: 128\nmodel_name: Yi-34B\nn_heads: 56\nd_mlp: 20480\nact_fn: silu\nd_vocab: 64000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: 01-ai\/Yi-34B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.00944911182523068\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 64000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 38755368960\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"01-ai\/Yi-34B","tokenizer.vocab_size":64000.0,"tokenizer.max_len":4096,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"VBBPi7l7j0Xrv93YNq1tizlalWw","tensor_shapes.state_dict":"embed:\n  W_E: (64000, 7168)\nblocks:\n  '[0-59]':\n    ln1:\n      w: (7168,)\n    ln2:\n      w: (7168,)\n    attn:\n      W_Q: (56, 7168, 128)\n      W_O: (56, 128, 7168)\n      b_Q: (56, 128)\n      b_O: (7168,)\n      '[_W_K, _W_V]': (8, 7168, 128)\n      '[_b_K, _b_V]': (8, 128)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (7168, 20480)\n      W_out: (20480, 7168)\n      b_in: (20480,)\n      b_out: (7168,)\nln_final:\n  w: (7168,)\nunembed:\n  W_U: (7168, 64000)\n  b_U: (64000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(64000, 7168)"},"blocks":{"[0-59]":{"ln1":{"w":"(7168,)"},"ln2":{"w":"(7168,)"},"attn":{"W_Q":"(56, 7168, 128)","W_O":"(56, 128, 7168)","b_Q":"(56, 128)","b_O":"(7168,)","[_W_K, _W_V]":"(8, 7168, 128)","[_b_K, _b_V]":"(8, 128)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(7168, 20480)","W_out":"(20480, 7168)","b_in":"(20480,)","b_out":"(7168,)"}}},"ln_final":{"w":"(7168,)"},"unembed":{"W_U":"(7168, 64000)","b_U":"(64000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-59]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 56, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 8, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 56, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 7168)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 7168)\nhook_embed: (batch, seq_len, 7168)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-59]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 56, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 8, 128)","[hook_attn_scores, hook_pattern]":"(batch, 56, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 7168)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"hook_embed":"(batch, seq_len, 7168)"}}
{"name.default_alias":"yi-6b-chat","name.huggingface":"01-ai\/Yi-6B-Chat","name.aliases":"yi-6b-chat, Yi-6B-Chat","model_type":"yi","name.from_cfg":"Yi-6B-Chat","n_params.as_str":"6.5B","n_params.as_int":6476005376,"n_params.from_name":"6b","cfg.n_params":6476005376,"cfg.n_layers":32,"cfg.n_heads":32,"cfg.d_model":4096,"cfg.d_vocab":64000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":32,"d_model":4096,"n_ctx":4096,"d_head":128,"model_name":"Yi-6B-Chat","n_heads":32,"d_mlp":11008,"act_fn":"silu","d_vocab":64000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"01-ai\/Yi-6B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0125,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":64000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":6476005376,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":4,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: Yi-6B-Chat\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 64000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: 01-ai\/Yi-6B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.0125\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 64000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 6476005376\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 4\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"01-ai\/Yi-6B-Chat","tokenizer.vocab_size":63992.0,"tokenizer.max_len":4096,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"VGXAFrTzytwGdUlX6AWH0NacncM","tensor_shapes.state_dict":"embed:\n  W_E: (64000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      W_Q: (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      b_Q: (32, 128)\n      b_O: (4096,)\n      '[_W_K, _W_V]': (4, 4096, 128)\n      '[_b_K, _b_V]': (4, 128)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      W_out: (11008, 4096)\n      b_in: (11008,)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 64000)\n  b_U: (64000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(64000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"W_Q":"(32, 4096, 128)","W_O":"(32, 128, 4096)","b_Q":"(32, 128)","b_O":"(4096,)","[_W_K, _W_V]":"(4, 4096, 128)","[_b_K, _b_V]":"(4, 128)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","W_out":"(11008, 4096)","b_in":"(11008,)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 64000)","b_U":"(64000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 4, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 32, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 4, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"yi-34b-chat","name.huggingface":"01-ai\/Yi-34B-Chat","name.aliases":"yi-34b-chat, Yi-34B-Chat","model_type":"yi","name.from_cfg":"Yi-34B-Chat","n_params.as_str":"39B","n_params.as_int":38755368960,"n_params.from_name":"34b","cfg.n_params":38755368960,"cfg.n_layers":60,"cfg.n_heads":56,"cfg.d_model":7168,"cfg.d_vocab":64000,"cfg.act_fn":"silu","cfg.positional_embedding_type":"rotary","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"LlamaForCausalLM","cfg.normalization_type":"RMS","config.raw__":{"n_layers":60,"d_model":7168,"n_ctx":4096,"d_head":128,"model_name":"Yi-34B-Chat","n_heads":56,"d_mlp":20480,"act_fn":"silu","d_vocab":64000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"LlamaForCausalLM","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"01-ai\/Yi-34B-Chat","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"RMS","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0094491118,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"rotary","final_rms":true,"d_vocab_out":64000,"parallel_attn_mlp":false,"rotary_dim":128,"n_params":38755368960,"use_hook_tokens":false,"gated_mlp":true,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":8,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 60\nd_model: 7168\nn_ctx: 4096\nd_head: 128\nmodel_name: Yi-34B-Chat\nn_heads: 56\nd_mlp: 20480\nact_fn: silu\nd_vocab: 64000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: 01-ai\/Yi-34B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.00944911182523068\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 64000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 38755368960\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"01-ai\/Yi-34B-Chat","tokenizer.vocab_size":63992.0,"tokenizer.max_len":4096,"tokenizer.class":"LlamaTokenizerFast","tokenizer.vocab_hash":"VGXAFrTzytwGdUlX6AWH0NacncM","tensor_shapes.state_dict":"embed:\n  W_E: (64000, 7168)\nblocks:\n  '[0-59]':\n    ln1:\n      w: (7168,)\n    ln2:\n      w: (7168,)\n    attn:\n      W_Q: (56, 7168, 128)\n      W_O: (56, 128, 7168)\n      b_Q: (56, 128)\n      b_O: (7168,)\n      '[_W_K, _W_V]': (8, 7168, 128)\n      '[_b_K, _b_V]': (8, 128)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (7168, 20480)\n      W_out: (20480, 7168)\n      b_in: (20480,)\n      b_out: (7168,)\nln_final:\n  w: (7168,)\nunembed:\n  W_U: (7168, 64000)\n  b_U: (64000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(64000, 7168)"},"blocks":{"[0-59]":{"ln1":{"w":"(7168,)"},"ln2":{"w":"(7168,)"},"attn":{"W_Q":"(56, 7168, 128)","W_O":"(56, 128, 7168)","b_Q":"(56, 128)","b_O":"(7168,)","[_W_K, _W_V]":"(8, 7168, 128)","[_b_K, _b_V]":"(8, 128)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(7168, 20480)","W_out":"(20480, 7168)","b_in":"(20480,)","b_out":"(7168,)"}}},"ln_final":{"w":"(7168,)"},"unembed":{"W_U":"(7168, 64000)","b_U":"(64000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-59]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 56, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 8, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 56, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 7168)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 7168)\nhook_embed: (batch, seq_len, 7168)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-59]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 56, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 8, 128)","[hook_attn_scores, hook_pattern]":"(batch, 56, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 7168)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"hook_embed":"(batch, seq_len, 7168)"}}
{"name.default_alias":"t5-small","name.huggingface":"google-t5\/t5-small","name.aliases":"t5-small","model_type":"t5","name.from_cfg":"t5-small","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":null,"cfg.n_params":18874368,"cfg.n_layers":6,"cfg.n_heads":8,"cfg.d_model":512,"cfg.d_vocab":32128,"cfg.act_fn":"relu","cfg.positional_embedding_type":"relative_positional_bias","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"T5ForConditionalGeneration","cfg.normalization_type":"LN","config.raw__":{"n_layers":6,"d_model":512,"n_ctx":20,"d_head":64,"model_name":"t5-small","n_heads":8,"d_mlp":2048,"act_fn":"relu","d_vocab":32128,"eps":0.000001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"T5ForConditionalGeneration","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"google-t5\/t5-small","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"bidirectional","attn_only":false,"seed":null,"initializer_range":0.0353553391,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"relative_positional_bias","final_rms":false,"d_vocab_out":32128,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":18874368,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":128,"relative_attention_num_buckets":32,"decoder_start_token_id":0,"tie_word_embeddings":true},"config":"n_layers: 6\nd_model: 512\nn_ctx: 20\nd_head: 64\nmodel_name: t5-small\nn_heads: 8\nd_mlp: 2048\nact_fn: relu\nd_vocab: 32128\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: T5ForConditionalGeneration\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: google-t5\/t5-small\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: bidirectional\nattn_only: false\nseed: null\ninitializer_range: 0.035355339059327376\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: relative_positional_bias\nfinal_rms: false\nd_vocab_out: 32128\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: 128\nrelative_attention_num_buckets: 32\ndecoder_start_token_id: 0\ntie_word_embeddings: true\n","tokenizer.name":"google-t5\/t5-small","tokenizer.vocab_size":32100.0,"tokenizer.max_len":512,"tokenizer.class":"T5TokenizerFast","tokenizer.vocab_hash":"jQeywCyCMVL_vza2wKfpuwjNVys","tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"t5-base","name.huggingface":"google-t5\/t5-base","name.aliases":"t5-base","model_type":"t5","name.from_cfg":"t5-base","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"cfg.n_params":84934656,"cfg.n_layers":12,"cfg.n_heads":12,"cfg.d_model":768,"cfg.d_vocab":32128,"cfg.act_fn":"relu","cfg.positional_embedding_type":"relative_positional_bias","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"T5ForConditionalGeneration","cfg.normalization_type":"LN","config.raw__":{"n_layers":12,"d_model":768,"n_ctx":20,"d_head":64,"model_name":"t5-base","n_heads":12,"d_mlp":3072,"act_fn":"relu","d_vocab":32128,"eps":0.000001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"T5ForConditionalGeneration","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"google-t5\/t5-base","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"bidirectional","attn_only":false,"seed":null,"initializer_range":0.0288675135,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"relative_positional_bias","final_rms":false,"d_vocab_out":32128,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":84934656,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":128,"relative_attention_num_buckets":32,"decoder_start_token_id":0,"tie_word_embeddings":true},"config":"n_layers: 12\nd_model: 768\nn_ctx: 20\nd_head: 64\nmodel_name: t5-base\nn_heads: 12\nd_mlp: 3072\nact_fn: relu\nd_vocab: 32128\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: T5ForConditionalGeneration\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: google-t5\/t5-base\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: bidirectional\nattn_only: false\nseed: null\ninitializer_range: 0.02886751345948129\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: relative_positional_bias\nfinal_rms: false\nd_vocab_out: 32128\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: 128\nrelative_attention_num_buckets: 32\ndecoder_start_token_id: 0\ntie_word_embeddings: true\n","tokenizer.name":"google-t5\/t5-base","tokenizer.vocab_size":32100.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"T5TokenizerFast","tokenizer.vocab_hash":"jQeywCyCMVL_vza2wKfpuwjNVys","tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"t5-large","name.huggingface":"google-t5\/t5-large","name.aliases":"t5-large","model_type":"t5","name.from_cfg":"t5-large","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"cfg.n_params":301989888,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":1024,"cfg.d_vocab":32128,"cfg.act_fn":"relu","cfg.positional_embedding_type":"relative_positional_bias","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"T5ForConditionalGeneration","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":1024,"n_ctx":20,"d_head":64,"model_name":"t5-large","n_heads":16,"d_mlp":4096,"act_fn":"relu","d_vocab":32128,"eps":0.000001,"use_attn_result":false,"use_attn_scale":false,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"T5ForConditionalGeneration","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"google-t5\/t5-large","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"bidirectional","attn_only":false,"seed":null,"initializer_range":0.025,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"relative_positional_bias","final_rms":false,"d_vocab_out":32128,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":301989888,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":128,"relative_attention_num_buckets":32,"decoder_start_token_id":0,"tie_word_embeddings":true},"config":"n_layers: 24\nd_model: 1024\nn_ctx: 20\nd_head: 64\nmodel_name: t5-large\nn_heads: 16\nd_mlp: 4096\nact_fn: relu\nd_vocab: 32128\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: T5ForConditionalGeneration\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: google-t5\/t5-large\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: bidirectional\nattn_only: false\nseed: null\ninitializer_range: 0.025\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: relative_positional_bias\nfinal_rms: false\nd_vocab_out: 32128\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: 128\nrelative_attention_num_buckets: 32\ndecoder_start_token_id: 0\ntie_word_embeddings: true\n","tokenizer.name":"google-t5\/t5-large","tokenizer.vocab_size":32100.0,"tokenizer.max_len":1000000000000000019884624838656,"tokenizer.class":"T5TokenizerFast","tokenizer.vocab_hash":"jQeywCyCMVL_vza2wKfpuwjNVys","tensor_shapes.state_dict":null,"tensor_shapes.state_dict.raw__":null,"tensor_shapes.activation_cache":null,"tensor_shapes.activation_cache.raw__":null}
{"name.default_alias":"mGPT","name.huggingface":null,"name.aliases":"","model_type":null,"name.from_cfg":"mGPT","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":null,"cfg.n_params":1207959552,"cfg.n_layers":24,"cfg.n_heads":16,"cfg.d_model":2048,"cfg.d_vocab":100000,"cfg.act_fn":"gelu_new","cfg.positional_embedding_type":"standard","cfg.parallel_attn_mlp":false,"cfg.original_architecture":"GPT2LMHeadModel","cfg.normalization_type":"LN","config.raw__":{"n_layers":24,"d_model":2048,"n_ctx":2048,"d_head":128,"model_name":"mGPT","n_heads":16,"d_mlp":8192,"act_fn":"gelu_new","d_vocab":100000,"eps":0.00001,"use_attn_result":false,"use_attn_scale":true,"use_split_qkv_input":false,"use_hook_mlp_in":false,"use_attn_in":false,"use_local_attn":false,"original_architecture":"GPT2LMHeadModel","from_checkpoint":false,"checkpoint_index":null,"checkpoint_label_type":null,"checkpoint_value":null,"tokenizer_name":"ai-forever\/mGPT","window_size":null,"attn_types":null,"init_mode":"gpt2","normalization_type":"LN","device":"cpu","n_devices":1,"attention_dir":"causal","attn_only":false,"seed":null,"initializer_range":0.0176776695,"init_weights":false,"scale_attn_by_inverse_layer_idx":false,"positional_embedding_type":"standard","final_rms":false,"d_vocab_out":100000,"parallel_attn_mlp":false,"rotary_dim":null,"n_params":1207959552,"use_hook_tokens":false,"gated_mlp":false,"default_prepend_bos":true,"dtype":"torch.float32","tokenizer_prepends_bos":null,"n_key_value_heads":null,"post_embedding_ln":false,"rotary_base":10000,"trust_remote_code":false,"rotary_adjacent_pairs":false,"load_in_4bit":false,"num_experts":null,"experts_per_token":null,"relative_attention_max_distance":null,"relative_attention_num_buckets":null,"decoder_start_token_id":null,"tie_word_embeddings":false},"config":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: mGPT\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu_new\nd_vocab: 100000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: ai-forever\/mGPT\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.017677669529663688\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 100000\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\nload_in_4bit: false\nnum_experts: null\nexperts_per_token: null\nrelative_attention_max_distance: null\nrelative_attention_num_buckets: null\ndecoder_start_token_id: null\ntie_word_embeddings: false\n","tokenizer.name":"ai-forever\/mGPT","tokenizer.vocab_size":100000.0,"tokenizer.max_len":2048,"tokenizer.class":"GPT2TokenizerFast","tokenizer.vocab_hash":"8j6CU_p3zgyeEBZ1Z3lu358tiy0","tensor_shapes.state_dict":"embed:\n  W_E: (100000, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 100000)\n  b_U: (100000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(100000, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 100000)","b_U":"(100000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2048)"}}
