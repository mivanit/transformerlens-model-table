{"name.default_alias":"gpt2-small","name.official":"gpt2","name.aliases":["gpt2-small"],"model_type":"gpt2","name.from_cfg":"gpt2","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"gpt2-medium","name.official":"gpt2-medium","name.aliases":[],"model_type":"gpt2","name.from_cfg":"gpt2-medium","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2-medium\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2-medium\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"gpt2-large","name.official":"gpt2-large","name.aliases":[],"model_type":"gpt2","name.from_cfg":"gpt2-large","n_params.as_str":"708M","n_params.as_int":707788800,"n_params.from_name":null,"config.n_params":707788800,"config.n_layers":36,"config.n_heads":20,"config.d_model":1280,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 36\nd_model: 1280\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2-large\nn_heads: 20\nd_mlp: 5120\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2-large\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bfllj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 707788800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1280)\npos_embed:\n  W_pos: (1024, 1280)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (1280,)\n    ln2:\n      '[w, b]': (1280,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 1280, 64)\n      W_O: (20, 64, 1280)\n      '[b_Q, b_K, b_V]': (20, 64)\n      b_O: (1280,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1280, 5120)\n      b_in: (5120,)\n      W_out: (5120, 1280)\n      b_out: (1280,)\nln_final:\n  '[w, b]': (1280,)\nunembed:\n  W_U: (1280, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1280)"},"pos_embed":{"W_pos":"(1024, 1280)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(1280,)"},"ln2":{"[w, b]":"(1280,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 1280, 64)","W_O":"(20, 64, 1280)","[b_Q, b_K, b_V]":"(20, 64)","b_O":"(1280,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1280, 5120)","b_in":"(5120,)","W_out":"(5120, 1280)","b_out":"(1280,)"}}},"ln_final":{"[w, b]":"(1280,)"},"unembed":{"W_U":"(1280, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 5120)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1280)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1280)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 64)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 5120)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1280)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1280)"}}
{"name.default_alias":"gpt2-xl","name.official":"gpt2-xl","name.aliases":[],"model_type":"gpt2","name.from_cfg":"gpt2-xl","n_params.as_str":"1.5B","n_params.as_int":1474560000,"n_params.from_name":null,"config.n_params":1474560000,"config.n_layers":48,"config.n_heads":25,"config.d_model":1600,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 48\nd_model: 1600\nn_ctx: 1024\nd_head: 64\nmodel_name: gpt2-xl\nn_heads: 25\nd_mlp: 6400\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: gpt2-xl\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  exSuR+F6lD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1474560000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1600)\npos_embed:\n  W_pos: (1024, 1600)\nblocks:\n  '[0-47]':\n    ln1:\n      '[w, b]': (1600,)\n    ln2:\n      '[w, b]': (1600,)\n    attn:\n      '[W_Q, W_K, W_V]': (25, 1600, 64)\n      W_O: (25, 64, 1600)\n      '[b_Q, b_K, b_V]': (25, 64)\n      b_O: (1600,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1600, 6400)\n      b_in: (6400,)\n      W_out: (6400, 1600)\n      b_out: (1600,)\nln_final:\n  '[w, b]': (1600,)\nunembed:\n  W_U: (1600, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1600)"},"pos_embed":{"W_pos":"(1024, 1600)"},"blocks":{"[0-47]":{"ln1":{"[w, b]":"(1600,)"},"ln2":{"[w, b]":"(1600,)"},"attn":{"[W_Q, W_K, W_V]":"(25, 1600, 64)","W_O":"(25, 64, 1600)","[b_Q, b_K, b_V]":"(25, 64)","b_O":"(1600,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1600, 6400)","b_in":"(6400,)","W_out":"(6400, 1600)","b_out":"(1600,)"}}},"ln_final":{"[w, b]":"(1600,)"},"unembed":{"W_U":"(1600, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-47]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1600)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 25, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 25, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1600)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 6400)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1600)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1600)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1600)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-47]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1600)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 25, 64)","[hook_attn_scores, hook_pattern]":"(batch, 25, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1600)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 6400)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1600)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1600)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1600)"}}
{"name.default_alias":"distillgpt2","name.official":"distilgpt2","name.aliases":["distillgpt2","distill-gpt2","distil-gpt2","gpt2-xs"],"model_type":"gpt2","name.from_cfg":"distilgpt2","n_params.as_str":"42M","n_params.as_int":42467328,"n_params.from_name":null,"config.n_params":42467328,"config.n_layers":6,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: distilgpt2\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: distilgpt2\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 42467328\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"opt-125m","name.official":"facebook\/opt-125m","name.aliases":["opt-125m","opt-small","opt"],"model_type":"opt","name.from_cfg":"opt-125m","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"125m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50272,"config.act_fn":"relu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"OPTForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: opt-125m\nn_heads: 12\nd_mlp: 3072\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-125m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"opt-1.3b","name.official":"facebook\/opt-1.3b","name.aliases":["opt-1.3b","opt-medium"],"model_type":"opt","name.from_cfg":"opt-1.3b","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.3b","config.n_params":1207959552,"config.n_layers":24,"config.n_heads":32,"config.d_model":2048,"config.d_vocab":50272,"config.act_fn":"relu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"OPTForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 64\nmodel_name: opt-1.3b\nn_heads: 32\nd_mlp: 8192\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-1.3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2048, 64)\n      W_O: (32, 64, 2048)\n      '[b_Q, b_K, b_V]': (32, 64)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2048, 64)","W_O":"(32, 64, 2048)","[b_Q, b_K, b_V]":"(32, 64)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 64)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2048)"}}
{"name.default_alias":"opt-2.7b","name.official":"facebook\/opt-2.7b","name.aliases":["opt-2.7b","opt-large"],"model_type":"opt","name.from_cfg":"opt-2.7b","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.7b","config.n_params":2516582400,"config.n_layers":32,"config.n_heads":32,"config.d_model":2560,"config.d_vocab":50272,"config.act_fn":"relu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"OPTForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: opt-2.7b\nn_heads: 32\nd_mlp: 10240\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-2.7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  cjqgTtwwkD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 2560)\npos_embed:\n  W_pos: (2048, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 2560)"},"pos_embed":{"W_pos":"(2048, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2560)"}}
{"name.default_alias":"opt-6.7b","name.official":"facebook\/opt-6.7b","name.aliases":["opt-6.7b","opt-xl"],"model_type":"opt","name.from_cfg":"opt-6.7b","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.7b","config.n_params":6442450944,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":50272,"config.act_fn":"relu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"OPTForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-6.7b\nn_heads: 32\nd_mlp: 16384\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-6.7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 4096)\npos_embed:\n  W_pos: (2048, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 4096)"},"pos_embed":{"W_pos":"(2048, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 4096)"}}
{"name.default_alias":"opt-13b","name.official":"facebook\/opt-13b","name.aliases":["opt-13b","opt-xxl"],"model_type":"opt","name.from_cfg":"opt-13b","n_params.as_str":"13B","n_params.as_int":12582912000,"n_params.from_name":"13b","config.n_params":12582912000,"config.n_layers":40,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":50272,"config.act_fn":"relu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"OPTForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-13b\nn_heads: 40\nd_mlp: 20480\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-13b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 5120)\npos_embed:\n  W_pos: (2048, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 5120)"},"pos_embed":{"W_pos":"(2048, 5120)"},"blocks":{"[0-39]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 5120)"}}
{"name.default_alias":"opt-30b","name.official":"facebook\/opt-30b","name.aliases":["opt-30b","opt-xxxl"],"model_type":"opt","name.from_cfg":"opt-30b","n_params.as_str":"30B","n_params.as_int":29595009024,"n_params.from_name":"30b","config.n_params":29595009024,"config.n_layers":48,"config.n_heads":56,"config.d_model":7168,"config.d_vocab":50272,"config.act_fn":"relu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"OPTForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 48\nd_model: 7168\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-30b\nn_heads: 56\nd_mlp: 28672\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-30b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  hfkfUg5agz8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 29595009024\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 7168)\npos_embed:\n  W_pos: (2048, 7168)\nblocks:\n  '[0-47]':\n    ln1:\n      '[w, b]': (7168,)\n    ln2:\n      '[w, b]': (7168,)\n    attn:\n      '[W_Q, W_K, W_V]': (56, 7168, 128)\n      W_O: (56, 128, 7168)\n      '[b_Q, b_K, b_V]': (56, 128)\n      b_O: (7168,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (7168, 28672)\n      b_in: (28672,)\n      W_out: (28672, 7168)\n      b_out: (7168,)\nln_final:\n  '[w, b]': (7168,)\nunembed:\n  W_U: (7168, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 7168)"},"pos_embed":{"W_pos":"(2048, 7168)"},"blocks":{"[0-47]":{"ln1":{"[w, b]":"(7168,)"},"ln2":{"[w, b]":"(7168,)"},"attn":{"[W_Q, W_K, W_V]":"(56, 7168, 128)","W_O":"(56, 128, 7168)","[b_Q, b_K, b_V]":"(56, 128)","b_O":"(7168,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(7168, 28672)","b_in":"(28672,)","W_out":"(28672, 7168)","b_out":"(7168,)"}}},"ln_final":{"[w, b]":"(7168,)"},"unembed":{"W_U":"(7168, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-47]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 56, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 56, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 7168)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 28672)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 7168)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 7168)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 7168)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-47]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 56, 128)","[hook_attn_scores, hook_pattern]":"(batch, 56, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 28672)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 7168)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 7168)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 7168)"}}
{"name.default_alias":"opt-66b","name.official":"facebook\/opt-66b","name.aliases":["opt-66b","opt-xxxxl"],"model_type":"opt","name.from_cfg":"opt-66b","n_params.as_str":"65B","n_params.as_int":65229815808,"n_params.from_name":"66b","config.n_params":65229815808,"config.n_layers":64,"config.n_heads":72,"config.d_model":9216,"config.d_vocab":50272,"config.act_fn":"relu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"OPTForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 64\nd_model: 9216\nn_ctx: 2048\nd_head: 128\nmodel_name: opt-66b\nn_heads: 72\nd_mlp: 36864\nact_fn: relu\nd_vocab: 50272\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: OPTForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: facebook\/opt-66b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  ERERERERgT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50272\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 65229815808\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50272, 9216)\npos_embed:\n  W_pos: (2048, 9216)\nblocks:\n  '[0-63]':\n    ln1:\n      '[w, b]': (9216,)\n    ln2:\n      '[w, b]': (9216,)\n    attn:\n      '[W_Q, W_K, W_V]': (72, 9216, 128)\n      W_O: (72, 128, 9216)\n      '[b_Q, b_K, b_V]': (72, 128)\n      b_O: (9216,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (9216, 36864)\n      b_in: (36864,)\n      W_out: (36864, 9216)\n      b_out: (9216,)\nln_final:\n  '[w, b]': (9216,)\nunembed:\n  W_U: (9216, 50272)\n  b_U: (50272,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50272, 9216)"},"pos_embed":{"W_pos":"(2048, 9216)"},"blocks":{"[0-63]":{"ln1":{"[w, b]":"(9216,)"},"ln2":{"[w, b]":"(9216,)"},"attn":{"[W_Q, W_K, W_V]":"(72, 9216, 128)","W_O":"(72, 128, 9216)","[b_Q, b_K, b_V]":"(72, 128)","b_O":"(9216,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(9216, 36864)","b_in":"(36864,)","W_out":"(36864, 9216)","b_out":"(9216,)"}}},"ln_final":{"[w, b]":"(9216,)"},"unembed":{"W_U":"(9216, 50272)","b_U":"(50272,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-63]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 9216)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 72, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 72, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 9216)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 36864)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 9216)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 9216)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 9216)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-63]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 9216)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 72, 128)","[hook_attn_scores, hook_pattern]":"(batch, 72, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 9216)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 36864)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 9216)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 9216)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 9216)"}}
{"name.default_alias":"gpt-neo-125M","name.official":null,"name.aliases":[],"model_type":"gpt-neo","name.from_cfg":"gpt-neo-125M","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"125M","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: gpt-neo-125M\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neo-125M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"gpt-neo-1.3B","name.official":null,"name.aliases":[],"model_type":"gpt-neo","name.from_cfg":"gpt-neo-1.3B","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.3B","config.n_params":1207959552,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: gpt-neo-1.3B\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neo-1.3B\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2048)"}}
{"name.default_alias":"gpt-neo-2.7B","name.official":null,"name.aliases":[],"model_type":"gpt-neo","name.from_cfg":"gpt-neo-2.7B","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.7B","config.n_params":2516582400,"config.n_layers":32,"config.n_heads":20,"config.d_model":2560,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 128\nmodel_name: gpt-neo-2.7B\nn_heads: 20\nd_mlp: 10240\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neo-2.7B\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  cjqgTtwwkD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 2560)\npos_embed:\n  W_pos: (2048, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 2560, 128)\n      W_O: (20, 128, 2560)\n      '[b_Q, b_K, b_V]': (20, 128)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 2560)"},"pos_embed":{"W_pos":"(2048, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 2560, 128)","W_O":"(20, 128, 2560)","[b_Q, b_K, b_V]":"(20, 128)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 128)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2560)"}}
{"name.default_alias":"gpt-j-6B","name.official":null,"name.aliases":[],"model_type":"gpt-j","name.from_cfg":"gpt-j-6B","n_params.as_str":"5.6B","n_params.as_int":5637144576,"n_params.from_name":"6B","config.n_params":5637144576,"config.n_layers":28,"config.n_heads":16,"config.d_model":4096,"config.d_vocab":50400,"config.act_fn":"gelu_new","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTJForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 28\nd_model: 4096\nn_ctx: 2048\nd_head: 256\nmodel_name: gpt-j-6B\nn_heads: 16\nd_mlp: 16384\nact_fn: gelu_new\nd_vocab: 50400\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTJForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-j-6B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50400\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 5637144576\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: true\n","tensor_shapes.state_dict":"embed:\n  W_E: (50400, 4096)\nblocks:\n  '[0-27]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 4096, 256)\n      W_O: (16, 256, 4096)\n      '[b_Q, b_K, b_V]': (16, 256)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50400)\n  b_U: (50400,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50400, 4096)"},"blocks":{"[0-27]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 4096, 256)","W_O":"(16, 256, 4096)","[b_Q, b_K, b_V]":"(16, 256)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50400)","b_U":"(50400,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-27]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-27]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 256)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"gpt-neox-20b","name.official":"EleutherAI\/gpt-neox-20b","name.aliases":["gpt-neox-20b","gpt-neox","neox"],"model_type":"gpt-neo","name.from_cfg":"gpt-neox-20b","n_params.as_str":"20B","n_params.as_int":19931332608,"n_params.from_name":"20b","config.n_params":19931332608,"config.n_layers":44,"config.n_heads":64,"config.d_model":6144,"config.d_vocab":50432,"config.act_fn":"gelu_fast","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 44\nd_model: 6144\nn_ctx: 2048\nd_head: 96\nmodel_name: gpt-neox-20b\nn_heads: 64\nd_mlp: 24576\nact_fn: gelu_fast\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  MvA88\/3mhD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 24\nn_params: 19931332608\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 6144)\nblocks:\n  '[0-43]':\n    ln1:\n      '[w, b]': (6144,)\n    ln2:\n      '[w, b]': (6144,)\n    attn:\n      '[W_Q, W_K, W_V]': (64, 6144, 96)\n      W_O: (64, 96, 6144)\n      '[b_Q, b_K, b_V]': (64, 96)\n      b_O: (6144,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 24)\n    mlp:\n      W_in: (6144, 24576)\n      b_in: (24576,)\n      W_out: (24576, 6144)\n      b_out: (6144,)\nln_final:\n  '[w, b]': (6144,)\nunembed:\n  W_U: (6144, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 6144)"},"blocks":{"[0-43]":{"ln1":{"[w, b]":"(6144,)"},"ln2":{"[w, b]":"(6144,)"},"attn":{"[W_Q, W_K, W_V]":"(64, 6144, 96)","W_O":"(64, 96, 6144)","[b_Q, b_K, b_V]":"(64, 96)","b_O":"(6144,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 24)"},"mlp":{"W_in":"(6144, 24576)","b_in":"(24576,)","W_out":"(24576, 6144)","b_out":"(6144,)"}}},"ln_final":{"[w, b]":"(6144,)"},"unembed":{"W_U":"(6144, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-43]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        64, 96)\n      '[hook_attn_scores, hook_pattern]': (batch, 64, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      6144)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 6144)\nhook_embed: (batch, seq_len, 6144)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-43]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 64, 96)","[hook_attn_scores, hook_pattern]":"(batch, 64, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 6144)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"hook_embed":"(batch, seq_len, 6144)"}}
{"name.default_alias":"stanford-gpt2-small-a","name.official":"stanford-crfm\/alias-gpt2-small-x21","name.aliases":["stanford-gpt2-small-a","alias-gpt2-small-x21","gpt2-mistral-small-a","gpt2-stanford-small-a"],"model_type":"gpt2","name.from_cfg":"alias-gpt2-small-x21","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: alias-gpt2-small-x21\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/alias-gpt2-small-x21\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-b","name.official":"stanford-crfm\/battlestar-gpt2-small-x49","name.aliases":["stanford-gpt2-small-b","battlestar-gpt2-small-x49","gpt2-mistral-small-b","gpt2-mistral-small-b"],"model_type":"gpt2","name.from_cfg":"battlestar-gpt2-small-x49","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: battlestar-gpt2-small-x49\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/battlestar-gpt2-small-x49\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-c","name.official":"stanford-crfm\/caprica-gpt2-small-x81","name.aliases":["stanford-gpt2-small-c","caprica-gpt2-small-x81","gpt2-mistral-small-c","gpt2-stanford-small-c"],"model_type":"gpt2","name.from_cfg":"caprica-gpt2-small-x81","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: caprica-gpt2-small-x81\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/caprica-gpt2-small-x81\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-d","name.official":"stanford-crfm\/darkmatter-gpt2-small-x343","name.aliases":["stanford-gpt2-small-d","darkmatter-gpt2-small-x343","gpt2-mistral-small-d","gpt2-mistral-small-d"],"model_type":"gpt2","name.from_cfg":"darkmatter-gpt2-small-x343","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: darkmatter-gpt2-small-x343\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/darkmatter-gpt2-small-x343\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-small-e","name.official":"stanford-crfm\/expanse-gpt2-small-x777","name.aliases":["stanford-gpt2-small-e","expanse-gpt2-small-x777","gpt2-mistral-small-e","gpt2-mistral-small-e"],"model_type":"gpt2","name.from_cfg":"expanse-gpt2-small-x777","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: expanse-gpt2-small-x777\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/expanse-gpt2-small-x777\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"stanford-gpt2-medium-a","name.official":"stanford-crfm\/arwen-gpt2-medium-x21","name.aliases":["stanford-gpt2-medium-a","arwen-gpt2-medium-x21","gpt2-medium-small-a","gpt2-stanford-medium-a"],"model_type":"gpt2","name.from_cfg":"arwen-gpt2-medium-x21","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: arwen-gpt2-medium-x21\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/arwen-gpt2-medium-x21\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-b","name.official":"stanford-crfm\/beren-gpt2-medium-x49","name.aliases":["stanford-gpt2-medium-b","beren-gpt2-medium-x49","gpt2-medium-small-b","gpt2-stanford-medium-b"],"model_type":"gpt2","name.from_cfg":"beren-gpt2-medium-x49","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: beren-gpt2-medium-x49\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/beren-gpt2-medium-x49\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-c","name.official":"stanford-crfm\/celebrimbor-gpt2-medium-x81","name.aliases":["stanford-gpt2-medium-c","celebrimbor-gpt2-medium-x81","gpt2-medium-small-c","gpt2-medium-small-c"],"model_type":"gpt2","name.from_cfg":"celebrimbor-gpt2-medium-x81","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: celebrimbor-gpt2-medium-x81\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/celebrimbor-gpt2-medium-x81\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-d","name.official":"stanford-crfm\/durin-gpt2-medium-x343","name.aliases":["stanford-gpt2-medium-d","durin-gpt2-medium-x343","gpt2-medium-small-d","gpt2-stanford-medium-d"],"model_type":"gpt2","name.from_cfg":"durin-gpt2-medium-x343","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: durin-gpt2-medium-x343\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/durin-gpt2-medium-x343\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stanford-gpt2-medium-e","name.official":"stanford-crfm\/eowyn-gpt2-medium-x777","name.aliases":["stanford-gpt2-medium-e","eowyn-gpt2-medium-x777","gpt2-medium-small-e","gpt2-stanford-medium-e"],"model_type":"gpt2","name.from_cfg":"eowyn-gpt2-medium-x777","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":null,"config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadModel","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: eowyn-gpt2-medium-x777\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stanford-crfm\/eowyn-gpt2-medium-x777\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: true\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-14m","name.official":"EleutherAI\/pythia-14m","name.aliases":["pythia-14m"],"model_type":"pythia","name.from_cfg":"pythia-14m","n_params.as_str":"1.2M","n_params.as_int":1179648,"n_params.from_name":"14m","config.n_params":1179648,"config.n_layers":6,"config.n_heads":4,"config.d_model":128,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 128\nn_ctx: 2048\nd_head: 32\nmodel_name: pythia-14m\nn_heads: 4\nd_mlp: 512\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-14m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgasj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 8\nn_params: 1179648\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 128)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (128,)\n    ln2:\n      '[w, b]': (128,)\n    attn:\n      '[W_Q, W_K, W_V]': (4, 128, 32)\n      W_O: (4, 32, 128)\n      '[b_Q, b_K, b_V]': (4, 32)\n      b_O: (128,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 8)\n    mlp:\n      W_in: (128, 512)\n      b_in: (512,)\n      W_out: (512, 128)\n      b_out: (128,)\nln_final:\n  '[w, b]': (128,)\nunembed:\n  W_U: (128, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 128)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(128,)"},"ln2":{"[w, b]":"(128,)"},"attn":{"[W_Q, W_K, W_V]":"(4, 128, 32)","W_O":"(4, 32, 128)","[b_Q, b_K, b_V]":"(4, 32)","b_O":"(128,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 8)"},"mlp":{"W_in":"(128, 512)","b_in":"(512,)","W_out":"(512, 128)","b_out":"(128,)"}}},"ln_final":{"[w, b]":"(128,)"},"unembed":{"W_U":"(128, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        4, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 4, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 512)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      128)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 128)\nhook_embed: (batch, seq_len, 128)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 4, 32)","[hook_attn_scores, hook_pattern]":"(batch, 4, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 512)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 128)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"hook_embed":"(batch, seq_len, 128)"}}
{"name.default_alias":"pythia-31m","name.official":"EleutherAI\/pythia-31m","name.aliases":["pythia-31m"],"model_type":"pythia","name.from_cfg":"pythia-31m","n_params.as_str":"4.7M","n_params.as_int":4718592,"n_params.from_name":"31m","config.n_params":4718592,"config.n_layers":6,"config.n_heads":8,"config.d_model":256,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 256\nn_ctx: 2048\nd_head: 32\nmodel_name: pythia-31m\nn_heads: 8\nd_mlp: 1024\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-31m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZqT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 8\nn_params: 4718592\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 256)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (256,)\n    ln2:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 256, 32)\n      W_O: (8, 32, 256)\n      '[b_Q, b_K, b_V]': (8, 32)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 8)\n    mlp:\n      W_in: (256, 1024)\n      b_in: (1024,)\n      W_out: (1024, 256)\n      b_out: (256,)\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 256)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(256,)"},"ln2":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 256, 32)","W_O":"(8, 32, 256)","[b_Q, b_K, b_V]":"(8, 32)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 8)"},"mlp":{"W_in":"(256, 1024)","b_in":"(1024,)","W_out":"(1024, 256)","b_out":"(256,)"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 1024)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\nhook_embed: (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 32)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 1024)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"hook_embed":"(batch, seq_len, 256)"}}
{"name.default_alias":"pythia-70m","name.official":"EleutherAI\/pythia-70m","name.aliases":["pythia-70m","pythia","EleutherAI\/pythia-19m","pythia-19m"],"model_type":"pythia","name.from_cfg":"pythia-70m","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","config.n_params":18874368,"config.n_layers":6,"config.n_heads":8,"config.d_model":512,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m","name.official":"EleutherAI\/pythia-160m","name.aliases":["pythia-160m","EleutherAI\/pythia-125m","pythia-125m"],"model_type":"pythia","name.from_cfg":"pythia-160m","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m","name.official":"EleutherAI\/pythia-410m","name.aliases":["pythia-410m","EleutherAI\/pythia-350m","pythia-350m"],"model_type":"pythia","name.from_cfg":"pythia-410m","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b","name.official":"EleutherAI\/pythia-1b","name.aliases":["pythia-1b","EleutherAI\/pythia-800m","pythia-800m"],"model_type":"pythia","name.from_cfg":"pythia-1b","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","config.n_params":805306368,"config.n_layers":16,"config.n_heads":8,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b","name.official":"EleutherAI\/pythia-1.4b","name.aliases":["pythia-1.4b","EleutherAI\/pythia-1.3b","pythia-1.3b"],"model_type":"pythia","name.from_cfg":"pythia-1.4b","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","config.n_params":1207959552,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b","name.official":"EleutherAI\/pythia-2.8b","name.aliases":["pythia-2.8b","EleutherAI\/pythia-2.7b","pythia-2.7b"],"model_type":"pythia","name.from_cfg":"pythia-2.8b","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","config.n_params":2516582400,"config.n_layers":32,"config.n_heads":32,"config.d_model":2560,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  cjqgTtwwkD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b","name.official":"EleutherAI\/pythia-6.9b","name.aliases":["pythia-6.9b","EleutherAI\/pythia-6.7b","pythia-6.7b"],"model_type":"pythia","name.from_cfg":"pythia-6.9b","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","config.n_params":6442450944,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":50432,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b","name.official":"EleutherAI\/pythia-12b","name.aliases":["pythia-12b","EleutherAI\/pythia-13b","pythia-13b"],"model_type":"pythia","name.from_cfg":"pythia-12b","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","config.n_params":11324620800,"config.n_layers":36,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":50688,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-70m-deduped","name.official":"EleutherAI\/pythia-70m-deduped","name.aliases":["pythia-70m-deduped","EleutherAI\/pythia-19m-deduped","pythia-19m-deduped"],"model_type":"pythia","name.from_cfg":"pythia-70m-deduped","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","config.n_params":18874368,"config.n_layers":6,"config.n_heads":8,"config.d_model":512,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m-deduped\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m-deduped","name.official":"EleutherAI\/pythia-160m-deduped","name.aliases":["pythia-160m-deduped","EleutherAI\/pythia-125m-deduped","pythia-125m-deduped"],"model_type":"pythia","name.from_cfg":"pythia-160m-deduped","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-deduped\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m-deduped","name.official":"EleutherAI\/pythia-410m-deduped","name.aliases":["pythia-410m-deduped","EleutherAI\/pythia-350m-deduped","pythia-350m-deduped"],"model_type":"pythia","name.from_cfg":"pythia-410m-deduped","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m-deduped\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b-deduped","name.official":"EleutherAI\/pythia-1b-deduped","name.aliases":["pythia-1b-deduped","EleutherAI\/pythia-800m-deduped","pythia-800m-deduped"],"model_type":"pythia","name.from_cfg":"pythia-1b-deduped","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","config.n_params":805306368,"config.n_layers":16,"config.n_heads":8,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b-deduped\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b-deduped","name.official":"EleutherAI\/pythia-1.4b-deduped","name.aliases":["pythia-1.4b-deduped","EleutherAI\/pythia-1.3b-deduped","pythia-1.3b-deduped"],"model_type":"pythia","name.from_cfg":"pythia-1.4b-deduped","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","config.n_params":1207959552,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b-deduped\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b-deduped","name.official":"EleutherAI\/pythia-2.8b-deduped","name.aliases":["pythia-2.8b-deduped","EleutherAI\/pythia-2.7b-deduped","pythia-2.7b-deduped"],"model_type":"pythia","name.from_cfg":"pythia-2.8b-deduped","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","config.n_params":2516582400,"config.n_layers":32,"config.n_heads":32,"config.d_model":2560,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b-deduped\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  cjqgTtwwkD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b-deduped","name.official":"EleutherAI\/pythia-6.9b-deduped","name.aliases":["pythia-6.9b-deduped","EleutherAI\/pythia-6.7b-deduped","pythia-6.7b-deduped"],"model_type":"pythia","name.from_cfg":"pythia-6.9b-deduped","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","config.n_params":6442450944,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":50432,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b-deduped\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b-deduped","name.official":"EleutherAI\/pythia-12b-deduped","name.aliases":["pythia-12b-deduped","EleutherAI\/pythia-13b-deduped","pythia-13b-deduped"],"model_type":"pythia","name.from_cfg":"pythia-12b-deduped","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","config.n_params":11324620800,"config.n_layers":36,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":50688,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b-deduped\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b-deduped\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-70m-v0","name.official":"EleutherAI\/pythia-70m-v0","name.aliases":["pythia-70m-v0","pythia-v0","EleutherAI\/pythia-19m-v0","pythia-19m-v0"],"model_type":"pythia","name.from_cfg":"pythia-70m-v0","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","config.n_params":18874368,"config.n_layers":6,"config.n_heads":8,"config.d_model":512,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m-v0\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m-v0","name.official":"EleutherAI\/pythia-160m-v0","name.aliases":["pythia-160m-v0","EleutherAI\/pythia-125m-v0","pythia-125m-v0"],"model_type":"pythia","name.from_cfg":"pythia-160m-v0","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-v0\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m-v0","name.official":"EleutherAI\/pythia-410m-v0","name.aliases":["pythia-410m-v0","EleutherAI\/pythia-350m-v0","pythia-350m-v0"],"model_type":"pythia","name.from_cfg":"pythia-410m-v0","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m-v0\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b-v0","name.official":"EleutherAI\/pythia-1b-v0","name.aliases":["pythia-1b-v0","EleutherAI\/pythia-800m-v0","pythia-800m-v0"],"model_type":"pythia","name.from_cfg":"pythia-1b-v0","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","config.n_params":805306368,"config.n_layers":16,"config.n_heads":8,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b-v0\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b-v0","name.official":"EleutherAI\/pythia-1.4b-v0","name.aliases":["pythia-1.4b-v0","EleutherAI\/pythia-1.3b-v0","pythia-1.3b-v0"],"model_type":"pythia","name.from_cfg":"pythia-1.4b-v0","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","config.n_params":1207959552,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b-v0\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b-v0","name.official":"EleutherAI\/pythia-2.8b-v0","name.aliases":["pythia-2.8b-v0","EleutherAI\/pythia-2.7b-v0","pythia-2.7b-v0"],"model_type":"pythia","name.from_cfg":"pythia-2.8b-v0","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","config.n_params":2516582400,"config.n_layers":32,"config.n_heads":32,"config.d_model":2560,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b-v0\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  cjqgTtwwkD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b-v0","name.official":"EleutherAI\/pythia-6.9b-v0","name.aliases":["pythia-6.9b-v0","EleutherAI\/pythia-6.7b-v0","pythia-6.7b-v0"],"model_type":"pythia","name.from_cfg":"pythia-6.9b-v0","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","config.n_params":6442450944,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":50432,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b-v0\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b-v0","name.official":"EleutherAI\/pythia-12b-v0","name.aliases":["pythia-12b-v0","EleutherAI\/pythia-13b-v0","pythia-13b-v0"],"model_type":"pythia","name.from_cfg":"pythia-12b-v0","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","config.n_params":11324620800,"config.n_layers":36,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":50688,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b-v0\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-70m-deduped-v0","name.official":"EleutherAI\/pythia-70m-deduped-v0","name.aliases":["pythia-70m-deduped-v0","EleutherAI\/pythia-19m-deduped-v0","pythia-19m-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-70m-deduped-v0","n_params.as_str":"19M","n_params.as_int":18874368,"n_params.from_name":"70m","config.n_params":18874368,"config.n_layers":6,"config.n_heads":8,"config.d_model":512,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 512\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-70m-deduped-v0\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-70m-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 18874368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 512)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 512)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\nhook_embed: (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"hook_embed":"(batch, seq_len, 512)"}}
{"name.default_alias":"pythia-160m-deduped-v0","name.official":"EleutherAI\/pythia-160m-deduped-v0","name.aliases":["pythia-160m-deduped-v0","EleutherAI\/pythia-125m-deduped-v0","pythia-125m-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-160m-deduped-v0","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-deduped-v0\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-410m-deduped-v0","name.official":"EleutherAI\/pythia-410m-deduped-v0","name.aliases":["pythia-410m-deduped-v0","EleutherAI\/pythia-350m-deduped-v0","pythia-350m-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-410m-deduped-v0","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"410m","config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-410m-deduped-v0\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-410m-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"pythia-1b-deduped-v0","name.official":"EleutherAI\/pythia-1b-deduped-v0","name.aliases":["pythia-1b-deduped-v0","EleutherAI\/pythia-800m-deduped-v0","pythia-800m-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-1b-deduped-v0","n_params.as_str":"805M","n_params.as_int":805306368,"n_params.from_name":"1b","config.n_params":805306368,"config.n_layers":16,"config.n_heads":8,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 2048\nn_ctx: 2048\nd_head: 256\nmodel_name: pythia-1b-deduped-v0\nn_heads: 8\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 64\nn_params: 805306368\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 2048, 256)\n      W_O: (8, 256, 2048)\n      '[b_Q, b_K, b_V]': (8, 256)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 64)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 2048, 256)","W_O":"(8, 256, 2048)","[b_Q, b_K, b_V]":"(8, 256)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 64)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        8, 256)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 8, 256)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-1.4b-deduped-v0","name.official":"EleutherAI\/pythia-1.4b-deduped-v0","name.aliases":["pythia-1.4b-deduped-v0","EleutherAI\/pythia-1.3b-deduped-v0","pythia-1.3b-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-1.4b-deduped-v0","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":"1.4b","config.n_params":1207959552,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-1.4b-deduped-v0\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-1.4b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"pythia-2.8b-deduped-v0","name.official":"EleutherAI\/pythia-2.8b-deduped-v0","name.aliases":["pythia-2.8b-deduped-v0","EleutherAI\/pythia-2.7b-deduped-v0","pythia-2.7b-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-2.8b-deduped-v0","n_params.as_str":"2.5B","n_params.as_int":2516582400,"n_params.from_name":"2.8b","config.n_params":2516582400,"config.n_layers":32,"config.n_heads":32,"config.d_model":2560,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: pythia-2.8b-deduped-v0\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-2.8b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  cjqgTtwwkD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 20\nn_params: 2516582400\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 2560)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 20)\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 2560)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 20)"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"pythia-6.9b-deduped-v0","name.official":"EleutherAI\/pythia-6.9b-deduped-v0","name.aliases":["pythia-6.9b-deduped-v0","EleutherAI\/pythia-6.7b-deduped-v0","pythia-6.7b-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-6.9b-deduped-v0","n_params.as_str":"6.4B","n_params.as_int":6442450944,"n_params.from_name":"6.9b","config.n_params":6442450944,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":50432,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-6.9b-deduped-v0\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-6.9b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 6442450944\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 4096)"},"blocks":{"[0-31]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"pythia-12b-deduped-v0","name.official":"EleutherAI\/pythia-12b-deduped-v0","name.aliases":["pythia-12b-deduped-v0","EleutherAI\/pythia-13b-deduped-v0","pythia-13b-deduped-v0"],"model_type":"pythia","name.from_cfg":"pythia-12b-deduped-v0","n_params.as_str":"11B","n_params.as_int":11324620800,"n_params.from_name":"12b","config.n_params":11324620800,"config.n_layers":36,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":50688,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 36\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: pythia-12b-deduped-v0\nn_heads: 40\nd_mlp: 20480\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-12b-deduped-v0\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 11324620800\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 5120)\nblocks:\n  '[0-35]':\n    ln1:\n      '[w, b]': (5120,)\n    ln2:\n      '[w, b]': (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 32)\n    mlp:\n      W_in: (5120, 20480)\n      b_in: (20480,)\n      W_out: (20480, 5120)\n      b_out: (5120,)\nln_final:\n  '[w, b]': (5120,)\nunembed:\n  W_U: (5120, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 5120)"},"blocks":{"[0-35]":{"ln1":{"[w, b]":"(5120,)"},"ln2":{"[w, b]":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 32)"},"mlp":{"W_in":"(5120, 20480)","b_in":"(20480,)","W_out":"(20480, 5120)","b_out":"(5120,)"}}},"ln_final":{"[w, b]":"(5120,)"},"unembed":{"W_U":"(5120, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-35]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 20480)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-35]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 20480)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"pythia-160m-seed1","name.official":"EleutherAI\/pythia-160m-seed1","name.aliases":["pythia-160m-seed1","EleutherAI\/pythia-125m-seed1","pythia-125m-seed1"],"model_type":"pythia","name.from_cfg":"pythia-160m-seed1","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-seed1\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-seed1\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-160m-seed2","name.official":"EleutherAI\/pythia-160m-seed2","name.aliases":["pythia-160m-seed2","EleutherAI\/pythia-125m-seed2","pythia-125m-seed2"],"model_type":"pythia","name.from_cfg":"pythia-160m-seed2","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-seed2\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-seed2\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"pythia-160m-seed3","name.official":"EleutherAI\/pythia-160m-seed3","name.aliases":["pythia-160m-seed3","EleutherAI\/pythia-125m-seed3","pythia-125m-seed3"],"model_type":"pythia","name.from_cfg":"pythia-160m-seed3","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":"160m","config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50304,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 2048\nd_head: 64\nmodel_name: pythia-160m-seed3\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 50304\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/pythia-160m-seed3\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50304\nparallel_attn_mlp: true\nrotary_dim: 16\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50304, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 16)\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50304)\n  b_U: (50304,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50304, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 16)"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50304)","b_U":"(50304,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\nhook_embed: (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"hook_embed":"(batch, seq_len, 768)"}}
{"name.default_alias":"solu-1l-pile","name.official":"NeelNanda\/SoLU_1L_v9_old","name.aliases":["solu-1l-pile","solu-1l-old"],"model_type":"solu","name.from_cfg":"SoLU_1L_v9_old","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"config.n_params":12582912,"config.n_layers":1,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50278,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel-solu-old","config.normalization_type":"LN","cfg":"n_layers: 1\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_1L_v9_old\nn_heads: 16\nd_mlp: 4096\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (4096,)\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  w: (1024,)\nunembed:\n  W_U: (1024, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"0":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(4096,)"},"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"w":"(1024,)"},"unembed":{"W_U":"(1024, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 4096)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"solu-2l-pile","name.official":"NeelNanda\/SoLU_2L_v10_old","name.aliases":["solu-2l-pile","solu-2l-old"],"model_type":"solu","name.from_cfg":"SoLU_2L_v10_old","n_params.as_str":"13M","n_params.as_int":12812288,"n_params.from_name":null,"config.n_params":12812288,"config.n_layers":2,"config.n_heads":11,"config.d_model":736,"config.d_vocab":50278,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel-solu-old","config.normalization_type":"LNPre","cfg":"n_layers: 2\nd_model: 736\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_2L_v10_old\nn_heads: 11\nd_mlp: 2944\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  l3rLbzQynj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12812288\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 736)\npos_embed:\n  W_pos: (1024, 736)\nblocks:\n  '[0-1]':\n    attn:\n      '[W_Q, W_K, W_V]': (11, 736, 64)\n      W_O: (11, 64, 736)\n      '[b_Q, b_K, b_V]': (11, 64)\n      b_O: (736,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (736, 2944)\n      b_in: (2944,)\n      W_out: (2944, 736)\n      b_out: (736,)\nunembed:\n  W_U: (736, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 736)"},"pos_embed":{"W_pos":"(1024, 736)"},"blocks":{"[0-1]":{"attn":{"[W_Q, W_K, W_V]":"(11, 736, 64)","W_O":"(11, 64, 736)","[b_Q, b_K, b_V]":"(11, 64)","b_O":"(736,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(736, 2944)","b_in":"(2944,)","W_out":"(2944, 736)","b_out":"(736,)"}}},"unembed":{"W_U":"(736, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 736)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 11, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 11, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 736)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2944)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2944)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 736)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 736)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 736)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 736)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 11, 64)","[hook_attn_scores, hook_pattern]":"(batch, 11, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 736)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2944)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2944)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 736)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 736)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 736)"}}
{"name.default_alias":"solu-4l-pile","name.official":"NeelNanda\/SoLU_4L_v11_old","name.aliases":["solu-4l-pile","solu-4l-old"],"model_type":"solu","name.from_cfg":"SoLU_4L_v11_old","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"config.n_params":12582912,"config.n_layers":4,"config.n_heads":8,"config.d_model":512,"config.d_vocab":50278,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel-solu-old","config.normalization_type":"LNPre","cfg":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_4L_v11_old\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nunembed:\n  W_U: (512, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"unembed":{"W_U":"(512, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-6l-pile","name.official":"NeelNanda\/SoLU_6L_v13_old","name.aliases":["solu-6l-pile","solu-6l-old"],"model_type":"solu","name.from_cfg":"SoLU_6L_v13_old","n_params.as_str":"42M","n_params.as_int":42467328,"n_params.from_name":null,"config.n_params":42467328,"config.n_layers":6,"config.n_heads":12,"config.d_model":768,"config.d_vocab":50278,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel-solu-old","config.normalization_type":"LNPre","cfg":"n_layers: 6\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_6L_v13_old\nn_heads: 12\nd_mlp: 3072\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: true\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 42467328\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-5]':\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nunembed:\n  W_U: (768, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-5]":{"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"unembed":{"W_U":"(768, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 3072)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"solu-8l-pile","name.official":"NeelNanda\/SoLU_8L_v21_old","name.aliases":["solu-8l-pile","solu-8l-old"],"model_type":"solu","name.from_cfg":"SoLU_8L_v21_old","n_params.as_str":"101M","n_params.as_int":100663296,"n_params.from_name":null,"config.n_params":100663296,"config.n_layers":8,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50278,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel-solu-old","config.normalization_type":"LNPre","cfg":"n_layers: 8\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_8L_v21_old\nn_heads: 16\nd_mlp: 4096\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 100663296\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-7]':\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nunembed:\n  W_U: (1024, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-7]":{"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"unembed":{"W_U":"(1024, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 4096)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"solu-10l-pile","name.official":"NeelNanda\/SoLU_10L_v22_old","name.aliases":["solu-10l-pile","solu-10l-old"],"model_type":"solu","name.from_cfg":"SoLU_10L_v22_old","n_params.as_str":"197M","n_params.as_int":196608000,"n_params.from_name":null,"config.n_params":196608000,"config.n_layers":10,"config.n_heads":20,"config.d_model":1280,"config.d_vocab":50278,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel-solu-old","config.normalization_type":"LNPre","cfg":"n_layers: 10\nd_model: 1280\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_10L_v22_old\nn_heads: 20\nd_mlp: 5120\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LNPre\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bfllj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 196608000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1280)\npos_embed:\n  W_pos: (1024, 1280)\nblocks:\n  '[0-9]':\n    attn:\n      '[W_Q, W_K, W_V]': (20, 1280, 64)\n      W_O: (20, 64, 1280)\n      '[b_Q, b_K, b_V]': (20, 64)\n      b_O: (1280,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (1280, 5120)\n      b_in: (5120,)\n      W_out: (5120, 1280)\n      b_out: (1280,)\nunembed:\n  W_U: (1280, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1280)"},"pos_embed":{"W_pos":"(1024, 1280)"},"blocks":{"[0-9]":{"attn":{"[W_Q, W_K, W_V]":"(20, 1280, 64)","W_O":"(20, 64, 1280)","[b_Q, b_K, b_V]":"(20, 64)","b_O":"(1280,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(1280, 5120)","b_in":"(5120,)","W_out":"(5120, 1280)","b_out":"(1280,)"}}},"unembed":{"W_U":"(1280, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-9]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 5120)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 5120)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1280)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1280)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-9]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 64)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 5120)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1280)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1280)"}}
{"name.default_alias":"solu-12l-pile","name.official":"NeelNanda\/SoLU_12L_v23_old","name.aliases":["solu-12l-pile","solu-12l-old"],"model_type":"solu","name.from_cfg":"SoLU_12L_v23_old","n_params.as_str":"340M","n_params.as_int":339738624,"n_params.from_name":null,"config.n_params":339738624,"config.n_layers":12,"config.n_heads":24,"config.d_model":1536,"config.d_vocab":50278,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel-solu-old","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 1536\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_12L_v23_old\nn_heads: 24\nd_mlp: 6144\nact_fn: solu_ln\nd_vocab: 50278\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel-solu-old\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  MvA88\/3mlD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50278\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 339738624\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50278, 1536)\npos_embed:\n  W_pos: (1024, 1536)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (1536,)\n    ln2:\n      '[w, b]': (1536,)\n    attn:\n      '[W_Q, W_K, W_V]': (24, 1536, 64)\n      W_O: (24, 64, 1536)\n      '[b_Q, b_K, b_V]': (24, 64)\n      b_O: (1536,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (6144,)\n      W_in: (1536, 6144)\n      b_in: (6144,)\n      W_out: (6144, 1536)\n      b_out: (1536,)\nln_final:\n  '[w, b]': (1536,)\nunembed:\n  W_U: (1536, 50278)\n  b_U: (50278,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50278, 1536)"},"pos_embed":{"W_pos":"(1024, 1536)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(1536,)"},"ln2":{"[w, b]":"(1536,)"},"attn":{"[W_Q, W_K, W_V]":"(24, 1536, 64)","W_O":"(24, 64, 1536)","[b_Q, b_K, b_V]":"(24, 64)","b_O":"(1536,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(6144,)"},"W_in":"(1536, 6144)","b_in":"(6144,)","W_out":"(6144, 1536)","b_out":"(1536,)"}}},"ln_final":{"[w, b]":"(1536,)"},"unembed":{"W_U":"(1536, 50278)","b_U":"(50278,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 24, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 24, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 6144)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 6144)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1536)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1536)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1536)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 24, 64)","[hook_attn_scores, hook_pattern]":"(batch, 24, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 6144)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1536)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1536)"}}
{"name.default_alias":"solu-1l","name.official":"NeelNanda\/SoLU_1L512W_C4_Code","name.aliases":["solu-1l","solu-1l-new","solu-1l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_1L512W_C4_Code","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"config.n_params":3145728,"config.n_layers":1,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_1L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-2l","name.official":"NeelNanda\/SoLU_2L512W_C4_Code","name.aliases":["solu-2l","solu-2l-new","solu-2l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_2L512W_C4_Code","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":null,"config.n_params":6291456,"config.n_layers":2,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_2L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-3l","name.official":"NeelNanda\/SoLU_3L512W_C4_Code","name.aliases":["solu-3l","solu-3l-new","solu-3l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_3L512W_C4_Code","n_params.as_str":"9.4M","n_params.as_int":9437184,"n_params.from_name":null,"config.n_params":9437184,"config.n_layers":3,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 3\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_3L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 9437184\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-2]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-2]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-2]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-2]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-4l","name.official":"NeelNanda\/SoLU_4L512W_C4_Code","name.aliases":["solu-4l","solu-4l-new","solu-4l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_4L512W_C4_Code","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"config.n_params":12582912,"config.n_layers":4,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_4L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-6l","name.official":"NeelNanda\/SoLU_6L768W_C4_Code","name.aliases":["solu-6l","solu-6l-new","solu-6l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_6L768W_C4_Code","n_params.as_str":"42M","n_params.as_int":42467328,"n_params.from_name":null,"config.n_params":42467328,"config.n_layers":6,"config.n_heads":12,"config.d_model":768,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 6\nd_model: 768\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_6L768W_C4_Code\nn_heads: 12\nd_mlp: 3072\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 42467328\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 768)\npos_embed:\n  W_pos: (1024, 768)\nblocks:\n  '[0-5]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (3072,)\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 768)"},"pos_embed":{"W_pos":"(1024, 768)"},"blocks":{"[0-5]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(3072,)"},"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-5]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 3072)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-5]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 3072)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"solu-8l","name.official":"NeelNanda\/SoLU_8L1024W_C4_Code","name.aliases":["solu-8l","solu-8l-new","solu-8l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_8L1024W_C4_Code","n_params.as_str":"101M","n_params.as_int":100663296,"n_params.from_name":null,"config.n_params":100663296,"config.n_layers":8,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 1024\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_8L1024W_C4_Code\nn_heads: 16\nd_mlp: 4096\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 100663296\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 1024)\npos_embed:\n  W_pos: (1024, 1024)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (4096,)\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 1024)"},"pos_embed":{"W_pos":"(1024, 1024)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(4096,)"},"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 4096)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"solu-10l","name.official":"NeelNanda\/SoLU_10L1280W_C4_Code","name.aliases":["solu-10l","solu-10l-new","solu-10l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_10L1280W_C4_Code","n_params.as_str":"197M","n_params.as_int":196608000,"n_params.from_name":null,"config.n_params":196608000,"config.n_layers":10,"config.n_heads":20,"config.d_model":1280,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 10\nd_model: 1280\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_10L1280W_C4_Code\nn_heads: 20\nd_mlp: 5120\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bfllj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 196608000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 1280)\npos_embed:\n  W_pos: (1024, 1280)\nblocks:\n  '[0-9]':\n    ln1:\n      '[w, b]': (1280,)\n    ln2:\n      '[w, b]': (1280,)\n    attn:\n      '[W_Q, W_K, W_V]': (20, 1280, 64)\n      W_O: (20, 64, 1280)\n      '[b_Q, b_K, b_V]': (20, 64)\n      b_O: (1280,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (5120,)\n      W_in: (1280, 5120)\n      b_in: (5120,)\n      W_out: (5120, 1280)\n      b_out: (1280,)\nln_final:\n  '[w, b]': (1280,)\nunembed:\n  W_U: (1280, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 1280)"},"pos_embed":{"W_pos":"(1024, 1280)"},"blocks":{"[0-9]":{"ln1":{"[w, b]":"(1280,)"},"ln2":{"[w, b]":"(1280,)"},"attn":{"[W_Q, W_K, W_V]":"(20, 1280, 64)","W_O":"(20, 64, 1280)","[b_Q, b_K, b_V]":"(20, 64)","b_O":"(1280,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(5120,)"},"W_in":"(1280, 5120)","b_in":"(5120,)","W_out":"(5120, 1280)","b_out":"(1280,)"}}},"ln_final":{"[w, b]":"(1280,)"},"unembed":{"W_U":"(1280, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-9]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1280)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 5120)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 5120)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1280)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1280)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-9]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 20, 64)","[hook_attn_scores, hook_pattern]":"(batch, 20, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 5120)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1280)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1280)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1280)"}}
{"name.default_alias":"solu-12l","name.official":"NeelNanda\/SoLU_12L1536W_C4_Code","name.aliases":["solu-12l","solu-12l-new","solu-12l-c4-code"],"model_type":"solu","name.from_cfg":"SoLU_12L1536W_C4_Code","n_params.as_str":"340M","n_params.as_int":339738624,"n_params.from_name":null,"config.n_params":339738624,"config.n_layers":12,"config.n_heads":24,"config.d_model":1536,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 1536\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_12L1536W_C4_Code\nn_heads: 24\nd_mlp: 6144\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  MvA88\/3mlD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 339738624\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 1536)\npos_embed:\n  W_pos: (1024, 1536)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (1536,)\n    ln2:\n      '[w, b]': (1536,)\n    attn:\n      '[W_Q, W_K, W_V]': (24, 1536, 64)\n      W_O: (24, 64, 1536)\n      '[b_Q, b_K, b_V]': (24, 64)\n      b_O: (1536,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (6144,)\n      W_in: (1536, 6144)\n      b_in: (6144,)\n      W_out: (6144, 1536)\n      b_out: (1536,)\nln_final:\n  '[w, b]': (1536,)\nunembed:\n  W_U: (1536, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 1536)"},"pos_embed":{"W_pos":"(1024, 1536)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(1536,)"},"ln2":{"[w, b]":"(1536,)"},"attn":{"[W_Q, W_K, W_V]":"(24, 1536, 64)","W_O":"(24, 64, 1536)","[b_Q, b_K, b_V]":"(24, 64)","b_O":"(1536,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(6144,)"},"W_in":"(1536, 6144)","b_in":"(6144,)","W_out":"(6144, 1536)","b_out":"(1536,)"}}},"ln_final":{"[w, b]":"(1536,)"},"unembed":{"W_U":"(1536, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 24, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 24, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 6144)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 6144)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1536)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1536)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1536)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 24, 64)","[hook_attn_scores, hook_pattern]":"(batch, 24, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 6144)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1536)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1536)"}}
{"name.default_alias":"gelu-1l","name.official":"NeelNanda\/GELU_1L512W_C4_Code","name.aliases":["gelu-1l","gelu-1l-new","gelu-1l-c4-code"],"model_type":"gelu","name.from_cfg":"GELU_1L512W_C4_Code","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"config.n_params":3145728,"config.n_layers":1,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"gelu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_1L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"gelu-2l","name.official":"NeelNanda\/GELU_2L512W_C4_Code","name.aliases":["gelu-2l","gelu-2l-new","gelu-2l-c4-code"],"model_type":"gelu","name.from_cfg":"GELU_2L512W_C4_Code","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":null,"config.n_params":6291456,"config.n_layers":2,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"gelu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_2L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"gelu-3l","name.official":"NeelNanda\/GELU_3L512W_C4_Code","name.aliases":["gelu-3l","gelu-3l-new","gelu-3l-c4-code"],"model_type":"gelu","name.from_cfg":"GELU_3L512W_C4_Code","n_params.as_str":"9.4M","n_params.as_int":9437184,"n_params.from_name":null,"config.n_params":9437184,"config.n_layers":3,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"gelu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 3\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_3L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 9437184\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-2]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-2]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-2]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-2]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"gelu-4l","name.official":"NeelNanda\/GELU_4L512W_C4_Code","name.aliases":["gelu-4l","gelu-4l-new","gelu-4l-c4-code"],"model_type":"gelu","name.from_cfg":"GELU_4L512W_C4_Code","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"config.n_params":12582912,"config.n_layers":4,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"gelu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: GELU_4L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-1l","name.official":"NeelNanda\/Attn_Only_1L512W_C4_Code","name.aliases":["attn-only-1l","attn-only-1l-new","attn-only-1l-c4-code"],"model_type":"attn-only","name.from_cfg":"Attn_Only_1L512W_C4_Code","n_params.as_str":"1.0M","n_params.as_int":1048576,"n_params.from_name":null,"config.n_params":1048576,"config.n_layers":1,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_1L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1048576\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-2l","name.official":"NeelNanda\/Attn_Only_2L512W_C4_Code","name.aliases":["attn-only-2l","attn-only-2l-new","attn-only-2l-c4-code"],"model_type":"attn-only","name.from_cfg":"Attn_Only_2L512W_C4_Code","n_params.as_str":"2.1M","n_params.as_int":2097152,"n_params.from_name":null,"config.n_params":2097152,"config.n_layers":2,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_2L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2097152\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-3l","name.official":"NeelNanda\/Attn_Only_3L512W_C4_Code","name.aliases":["attn-only-3l","attn-only-3l-new","attn-only-3l-c4-code"],"model_type":"attn-only","name.from_cfg":"Attn_Only_3L512W_C4_Code","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"config.n_params":3145728,"config.n_layers":3,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 3\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_3L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-2]':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-2]":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-2]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-2]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-4l","name.official":"NeelNanda\/Attn_Only_4L512W_C4_Code","name.aliases":["attn-only-4l","attn-only-4l-new","attn-only-4l-c4-code"],"model_type":"attn-only","name.from_cfg":"Attn_Only_4L512W_C4_Code","n_params.as_str":"4.2M","n_params.as_int":4194304,"n_params.from_name":null,"config.n_params":4194304,"config.n_layers":4,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn_Only_4L512W_C4_Code\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 4194304\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"attn-only-2l-demo","name.official":"NeelNanda\/Attn-Only-2L512W-Shortformer-6B-big-lr","name.aliases":["attn-only-2l-demo","attn-only-2l-shortformer-6b-big-lr","attn-only-2l-induction-demo","attn-only-demo"],"model_type":"attn-only","name.from_cfg":"Attn-Only-2L512W-Shortformer-6B-big-lr","n_params.as_str":"2.1M","n_params.as_int":2097152,"n_params.from_name":null,"config.n_params":2097152,"config.n_layers":2,"config.n_heads":8,"config.d_model":512,"config.d_vocab":50277,"config.act_fn":"solu_ln","config.positional_embedding_type":"shortformer","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":null,"cfg":"n_layers: 2\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: Attn-Only-2L512W-Shortformer-6B-big-lr\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 50277\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: EleutherAI\/gpt-neox-20b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: null\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: shortformer\nfinal_rms: false\nd_vocab_out: 50277\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2097152\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50277, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-1]':\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\nunembed:\n  W_U: (512, 50277)\n  b_U: (50277,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50277, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-1]":{"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"}}},"unembed":{"W_U":"(512, 50277)","b_U":"(50277,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 512)"}},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-1l-wiki","name.official":"NeelNanda\/SoLU_1L512W_Wiki_Finetune","name.aliases":["solu-1l-wiki","solu-1l-wiki-finetune","solu-1l-finetune"],"model_type":"solu","name.from_cfg":"SoLU_1L512W_Wiki_Finetune","n_params.as_str":"3.1M","n_params.as_int":3145728,"n_params.from_name":null,"config.n_params":3145728,"config.n_layers":1,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 1\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_1L512W_Wiki_Finetune\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 3145728\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"0":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"solu-4l-wiki","name.official":"NeelNanda\/SoLU_4L512W_Wiki_Finetune","name.aliases":["solu-4l-wiki","solu-4l-wiki-finetune","solu-4l-finetune"],"model_type":"solu","name.from_cfg":"SoLU_4L512W_Wiki_Finetune","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":null,"config.n_params":12582912,"config.n_layers":4,"config.n_heads":8,"config.d_model":512,"config.d_vocab":48262,"config.act_fn":"solu_ln","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 4\nd_model: 512\nn_ctx: 1024\nd_head: 64\nmodel_name: SoLU_4L512W_Wiki_Finetune\nn_heads: 8\nd_mlp: 2048\nact_fn: solu_ln\nd_vocab: 48262\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: NeelNanda\/gpt-neox-tokenizer-digits\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 48262\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (48262, 512)\npos_embed:\n  W_pos: (1024, 512)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (1024, 1024)\n      IGNORE: ()\n    mlp:\n      ln:\n        '[w, b]': (2048,)\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 48262)\n  b_U: (48262,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(48262, 512)"},"pos_embed":{"W_pos":"(1024, 512)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(1024, 1024)","IGNORE":"()"},"mlp":{"ln":{"[w, b]":"(2048,)"},"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 48262)","b_U":"(48262,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      ln:\n        hook_scale: (batch, seq_len, 1)\n        hook_normalized: (batch, seq_len, 2048)\n      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_pre, hook_mid, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"redwood_attn_2l","name.official":"ArthurConmy\/redwood_attn_2l","name.aliases":["redwood_attn_2l"],"model_type":null,"name.from_cfg":"redwood_attn_2l","n_params.as_str":"524K","n_params.as_int":524288,"n_params.from_name":null,"config.n_params":524288,"config.n_layers":2,"config.n_heads":8,"config.d_model":256,"config.d_vocab":50259,"config.act_fn":"gelu_new","config.positional_embedding_type":"shortformer","config.parallel_attn_mlp":false,"config.original_architecture":"neel","config.normalization_type":"LN","cfg":"n_layers: 2\nd_model: 256\nn_ctx: 2048\nd_head: 32\nmodel_name: redwood_attn_2l\nn_heads: 8\nd_mlp: -1\nact_fn: gelu_new\nd_vocab: 50259\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: neel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: ArthurConmy\/redwood_tokenizer\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: true\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZqT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: shortformer\nfinal_rms: false\nd_vocab_out: 50259\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 524288\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50259, 256)\npos_embed:\n  W_pos: (2048, 256)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 256, 32)\n      W_O: (8, 32, 256)\n      '[b_Q, b_K, b_V]': (8, 32)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50259)\n  b_U: (50259,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50259, 256)"},"pos_embed":{"W_pos":"(2048, 256)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 256, 32)","W_O":"(8, 32, 256)","[b_Q, b_K, b_V]":"(8, 32)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50259)","b_U":"(50259,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 32)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"[hook_resid_pre, hook_attn_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 256)"}}
{"name.default_alias":"llama-7b","name.official":"llama-7b-hf","name.aliases":["llama-7b"],"model_type":"llama","name.from_cfg":"llama-7b-hf","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-7b-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-7b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"llama-13b","name.official":"llama-13b-hf","name.aliases":["llama-13b"],"model_type":"llama","name.from_cfg":"llama-13b-hf","n_params.as_str":"9.9B","n_params.as_int":9856614400,"n_params.from_name":"13b","config.n_params":9856614400,"config.n_layers":40,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-13b-hf\nn_heads: 40\nd_mlp: 13824\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-13b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 9856614400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (5120,)\n    ln2:\n      w: (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (5120, 13824)\n      b_in: (13824,)\n      W_out: (13824, 5120)\n      b_out: (5120,)\nln_final:\n  w: (5120,)\nunembed:\n  W_U: (5120, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 5120)"},"blocks":{"[0-39]":{"ln1":{"w":"(5120,)"},"ln2":{"w":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(5120, 13824)","b_in":"(13824,)","W_out":"(13824, 5120)","b_out":"(5120,)"}}},"ln_final":{"w":"(5120,)"},"unembed":{"W_U":"(5120, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13824)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 13824)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"llama-30b","name.official":"llama-30b-hf","name.aliases":["llama-30b"],"model_type":"llama","name.from_cfg":"llama-30b-hf","n_params.as_str":"25B","n_params.as_int":24945623040,"n_params.from_name":"30b","config.n_params":24945623040,"config.n_layers":60,"config.n_heads":52,"config.d_model":6656,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 60\nd_model: 6656\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-30b-hf\nn_heads: 52\nd_mlp: 17920\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-30b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  sJGnGhEVhD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 24945623040\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 6656)\nblocks:\n  '[0-59]':\n    ln1:\n      w: (6656,)\n    ln2:\n      w: (6656,)\n    attn:\n      '[W_Q, W_K, W_V]': (52, 6656, 128)\n      W_O: (52, 128, 6656)\n      '[b_Q, b_K, b_V]': (52, 128)\n      b_O: (6656,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (6656, 17920)\n      b_in: (17920,)\n      W_out: (17920, 6656)\n      b_out: (6656,)\nln_final:\n  w: (6656,)\nunembed:\n  W_U: (6656, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 6656)"},"blocks":{"[0-59]":{"ln1":{"w":"(6656,)"},"ln2":{"w":"(6656,)"},"attn":{"[W_Q, W_K, W_V]":"(52, 6656, 128)","W_O":"(52, 128, 6656)","[b_Q, b_K, b_V]":"(52, 128)","b_O":"(6656,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(6656, 17920)","b_in":"(17920,)","W_out":"(17920, 6656)","b_out":"(6656,)"}}},"ln_final":{"w":"(6656,)"},"unembed":{"W_U":"(6656, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-59]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6656)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        52, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 52, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6656)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 17920)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 6656)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 6656)\nhook_embed: (batch, seq_len, 6656)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-59]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6656)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 52, 128)","[hook_attn_scores, hook_pattern]":"(batch, 52, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6656)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 17920)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 6656)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6656)"},"hook_embed":"(batch, seq_len, 6656)"}}
{"name.default_alias":"llama-65b","name.official":"llama-65b-hf","name.aliases":["llama-65b"],"model_type":"llama","name.from_cfg":"llama-65b-hf","n_params.as_str":"50B","n_params.as_int":50331648000,"n_params.from_name":"65b","config.n_params":50331648000,"config.n_layers":80,"config.n_heads":64,"config.d_model":8192,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 80\nd_model: 8192\nn_ctx: 2048\nd_head: 128\nmodel_name: llama-65b-hf\nn_heads: 64\nd_mlp: 22016\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: llama-65b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgagj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 50331648000\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 8192)\nblocks:\n  '[0-79]':\n    ln1:\n      w: (8192,)\n    ln2:\n      w: (8192,)\n    attn:\n      '[W_Q, W_K, W_V]': (64, 8192, 128)\n      W_O: (64, 128, 8192)\n      '[b_Q, b_K, b_V]': (64, 128)\n      b_O: (8192,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (8192, 22016)\n      b_in: (22016,)\n      W_out: (22016, 8192)\n      b_out: (8192,)\nln_final:\n  w: (8192,)\nunembed:\n  W_U: (8192, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 8192)"},"blocks":{"[0-79]":{"ln1":{"w":"(8192,)"},"ln2":{"w":"(8192,)"},"attn":{"[W_Q, W_K, W_V]":"(64, 8192, 128)","W_O":"(64, 128, 8192)","[b_Q, b_K, b_V]":"(64, 128)","b_O":"(8192,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(8192, 22016)","b_in":"(22016,)","W_out":"(22016, 8192)","b_out":"(8192,)"}}},"ln_final":{"w":"(8192,)"},"unembed":{"W_U":"(8192, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-79]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 8192)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        64, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 64, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 8192)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 22016)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 8192)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 8192)\nhook_embed: (batch, seq_len, 8192)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-79]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 8192)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 64, 128)","[hook_attn_scores, hook_pattern]":"(batch, 64, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 8192)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 22016)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 8192)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 8192)"},"hook_embed":"(batch, seq_len, 8192)"}}
{"name.default_alias":"Llama-2-7b","name.official":null,"name.aliases":[],"model_type":"Llama-2","name.from_cfg":"Llama-2-7b-hf","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-7b-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-7b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"Llama-2-7b-chat","name.official":null,"name.aliases":[],"model_type":"Llama-2","name.from_cfg":"Llama-2-7b-chat-hf","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-7b-chat-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-7b-chat-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"Llama-2-13b","name.official":null,"name.aliases":[],"model_type":"Llama-2","name.from_cfg":"Llama-2-13b-hf","n_params.as_str":"9.9B","n_params.as_int":9856614400,"n_params.from_name":"13b","config.n_params":9856614400,"config.n_layers":40,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 40\nd_model: 5120\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-13b-hf\nn_heads: 40\nd_mlp: 13824\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-13b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 9856614400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (5120,)\n    ln2:\n      w: (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (5120, 13824)\n      b_in: (13824,)\n      W_out: (13824, 5120)\n      b_out: (5120,)\nln_final:\n  w: (5120,)\nunembed:\n  W_U: (5120, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 5120)"},"blocks":{"[0-39]":{"ln1":{"w":"(5120,)"},"ln2":{"w":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(5120, 13824)","b_in":"(13824,)","W_out":"(13824, 5120)","b_out":"(5120,)"}}},"ln_final":{"w":"(5120,)"},"unembed":{"W_U":"(5120, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13824)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 13824)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"Llama-2-13b-chat","name.official":null,"name.aliases":[],"model_type":"Llama-2","name.from_cfg":"Llama-2-13b-chat-hf","n_params.as_str":"9.9B","n_params.as_int":9856614400,"n_params.from_name":"13b","config.n_params":9856614400,"config.n_layers":40,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 40\nd_model: 5120\nn_ctx: 4096\nd_head: 128\nmodel_name: Llama-2-13b-chat-hf\nn_heads: 40\nd_mlp: 13824\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: meta-llama\/Llama-2-13b-chat-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  4Vdm0bflhj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 9856614400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (5120,)\n    ln2:\n      w: (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (5120, 13824)\n      b_in: (13824,)\n      W_out: (13824, 5120)\n      b_out: (5120,)\nln_final:\n  w: (5120,)\nunembed:\n  W_U: (5120, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 5120)"},"blocks":{"[0-39]":{"ln1":{"w":"(5120,)"},"ln2":{"w":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(5120, 13824)","b_in":"(13824,)","W_out":"(13824, 5120)","b_out":"(5120,)"}}},"ln_final":{"w":"(5120,)"},"unembed":{"W_U":"(5120, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13824)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 13824)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"CodeLlamallama-2-7b","name.official":null,"name.aliases":[],"model_type":"llama","name.from_cfg":"CodeLlama-7b-hf","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32016,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: CodeLlama-7b-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32016\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: CodeLlama-7b-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32016\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32016, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32016)\n  b_U: (32016,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32016, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32016)","b_U":"(32016,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"CodeLlama-7b-python","name.official":null,"name.aliases":[],"model_type":"CodeLlama","name.from_cfg":"CodeLlama-7b-Python-hf","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: CodeLlama-7b-Python-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: CodeLlama-7b-Python-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"CodeLlama-7b-instruct","name.official":null,"name.aliases":[],"model_type":"CodeLlama","name.from_cfg":"CodeLlama-7b-Instruct-hf","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32016,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"LlamaForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: CodeLlama-7b-Instruct-hf\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 32016\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: LlamaForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: CodeLlama-7b-Instruct-hf\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 32016\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 1000000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32016, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32016)\n  b_U: (32016,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32016, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32016)","b_U":"(32016,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"othello-gpt","name.official":"Baidicoot\/Othello-GPT-Transformer-Lens","name.aliases":["othello-gpt"],"model_type":null,"name.from_cfg":"Othello-GPT-Transformer-Lens","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":null,"config.n_params":25165824,"config.n_layers":8,"config.n_heads":8,"config.d_model":512,"config.d_vocab":61,"config.act_fn":"gelu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"mingpt","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 512\nn_ctx: 59\nd_head: 64\nmodel_name: Othello-GPT-Transformer-Lens\nn_heads: 8\nd_mlp: 2048\nact_fn: gelu\nd_vocab: 61\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: mingpt\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: null\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 61\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (61, 512)\npos_embed:\n  W_pos: (59, 512)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (8, 512, 64)\n      W_O: (8, 64, 512)\n      '[b_Q, b_K, b_V]': (8, 64)\n      b_O: (512,)\n      mask: (59, 59)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 61)\n  b_U: (61,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(61, 512)"},"pos_embed":{"W_pos":"(59, 512)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(8, 512, 64)","W_O":"(8, 64, 512)","[b_Q, b_K, b_V]":"(8, 64)","b_O":"(512,)","mask":"(59, 59)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 61)","b_U":"(61,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 8, 64)","[hook_attn_scores, hook_pattern]":"(batch, 8, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"bert-base-cased","name.official":"bert-base-cased","name.aliases":[],"model_type":"bert","name.from_cfg":"bert-base-cased","n_params.as_str":"85M","n_params.as_int":84934656,"n_params.from_name":null,"config.n_params":84934656,"config.n_layers":12,"config.n_heads":12,"config.d_model":768,"config.d_vocab":28996,"config.act_fn":"gelu","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"BertForMaskedLM","config.normalization_type":"LN","cfg":"n_layers: 12\nd_model: 768\nn_ctx: 512\nd_head: 64\nmodel_name: bert-base-cased\nn_heads: 12\nd_mlp: 3072\nact_fn: gelu\nd_vocab: 28996\neps: 1.0e-12\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BertForMaskedLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bert-base-cased\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: bidirectional\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 28996\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 84934656\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (28996, 768)\npos_embed:\n  W_pos: (512, 768)\nblocks:\n  '[0-11]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (12, 768, 64)\n      W_O: (12, 64, 768)\n      '[b_Q, b_K, b_V]': (12, 64)\n      b_O: (768,)\n      mask: (512, 512)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 28996)\n  b_U: (28996,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(28996, 768)"},"pos_embed":{"W_pos":"(512, 768)"},"blocks":{"[0-11]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(12, 768, 64)","W_O":"(12, 64, 768)","[b_Q, b_K, b_V]":"(12, 64)","b_O":"(768,)","mask":"(512, 512)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 28996)","b_U":"(28996,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-11]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-11]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 12, 64)","[hook_attn_scores, hook_pattern]":"(batch, 12, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"tiny-stories-1M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-1M","n_params.as_str":"393K","n_params.as_int":393216,"n_params.from_name":"1M","config.n_params":393216,"config.n_layers":8,"config.n_heads":16,"config.d_model":64,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 64\nn_ctx: 2048\nd_head: 4\nmodel_name: TinyStories-1M\nn_heads: 16\nd_mlp: 256\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-1M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZuT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 393216\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 64)\npos_embed:\n  W_pos: (2048, 64)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (64,)\n    ln2:\n      '[w, b]': (64,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 64, 4)\n      W_O: (16, 4, 64)\n      '[b_Q, b_K, b_V]': (16, 4)\n      b_O: (64,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (64, 256)\n      b_in: (256,)\n      W_out: (256, 64)\n      b_out: (64,)\nln_final:\n  '[w, b]': (64,)\nunembed:\n  W_U: (64, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 64)"},"pos_embed":{"W_pos":"(2048, 64)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(64,)"},"ln2":{"[w, b]":"(64,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 64, 4)","W_O":"(16, 4, 64)","[b_Q, b_K, b_V]":"(16, 4)","b_O":"(64,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(64, 256)","b_in":"(256,)","W_out":"(256, 64)","b_out":"(64,)"}}},"ln_final":{"[w, b]":"(64,)"},"unembed":{"W_U":"(64, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 4)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 256)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 64)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 64)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 64)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 4)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 256)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 64)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 64)"}}
{"name.default_alias":"tiny-stories-3M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-3M","n_params.as_str":"1.6M","n_params.as_int":1572864,"n_params.from_name":"3M","config.n_params":1572864,"config.n_layers":8,"config.n_heads":16,"config.d_model":128,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 128\nn_ctx: 2048\nd_head: 8\nmodel_name: TinyStories-3M\nn_heads: 16\nd_mlp: 512\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-3M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgasj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1572864\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 128)\npos_embed:\n  W_pos: (2048, 128)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (128,)\n    ln2:\n      '[w, b]': (128,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 128, 8)\n      W_O: (16, 8, 128)\n      '[b_Q, b_K, b_V]': (16, 8)\n      b_O: (128,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (128, 512)\n      b_in: (512,)\n      W_out: (512, 128)\n      b_out: (128,)\nln_final:\n  '[w, b]': (128,)\nunembed:\n  W_U: (128, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 128)"},"pos_embed":{"W_pos":"(2048, 128)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(128,)"},"ln2":{"[w, b]":"(128,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 128, 8)","W_O":"(16, 8, 128)","[b_Q, b_K, b_V]":"(16, 8)","b_O":"(128,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(128, 512)","b_in":"(512,)","W_out":"(512, 128)","b_out":"(128,)"}}},"ln_final":{"[w, b]":"(128,)"},"unembed":{"W_U":"(128, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 8)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 512)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 128)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 128)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 128)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 8)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 512)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 128)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 128)"}}
{"name.default_alias":"tiny-stories-8M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-8M","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":"8M","config.n_params":6291456,"config.n_layers":8,"config.n_heads":16,"config.d_model":256,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 256\nn_ctx: 2048\nd_head: 16\nmodel_name: TinyStories-8M\nn_heads: 16\nd_mlp: 1024\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-8M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZqT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 256)\npos_embed:\n  W_pos: (2048, 256)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (256,)\n    ln2:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 256, 16)\n      W_O: (16, 16, 256)\n      '[b_Q, b_K, b_V]': (16, 16)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (256, 1024)\n      b_in: (1024,)\n      W_out: (1024, 256)\n      b_out: (256,)\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 256)"},"pos_embed":{"W_pos":"(2048, 256)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(256,)"},"ln2":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 256, 16)","W_O":"(16, 16, 256)","[b_Q, b_K, b_V]":"(16, 16)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(256, 1024)","b_in":"(1024,)","W_out":"(1024, 256)","b_out":"(256,)"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 16)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 1024)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 16)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 1024)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 256)"}}
{"name.default_alias":"tiny-stories-28M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-28M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"28M","config.n_params":25165824,"config.n_layers":8,"config.n_heads":16,"config.d_model":512,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 512\nn_ctx: 2048\nd_head: 32\nmodel_name: TinyStories-28M\nn_heads: 16\nd_mlp: 2048\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-28M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 512)\npos_embed:\n  W_pos: (2048, 512)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 512, 32)\n      W_O: (16, 32, 512)\n      '[b_Q, b_K, b_V]': (16, 32)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 512)"},"pos_embed":{"W_pos":"(2048, 512)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 512, 32)","W_O":"(16, 32, 512)","[b_Q, b_K, b_V]":"(16, 32)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 32)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"tiny-stories-33M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-33M","n_params.as_str":"28M","n_params.as_int":28311552,"n_params.from_name":"33M","config.n_params":28311552,"config.n_layers":4,"config.n_heads":16,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 4\nd_model: 768\nn_ctx: 2048\nd_head: 48\nmodel_name: TinyStories-33M\nn_heads: 16\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-33M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 28311552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 768, 48)\n      W_O: (16, 48, 768)\n      '[b_Q, b_K, b_V]': (16, 48)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 768, 48)","W_O":"(16, 48, 768)","[b_Q, b_K, b_V]":"(16, 48)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 48)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 48)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"tiny-stories-instruct-1M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-1M","n_params.as_str":"393K","n_params.as_int":393216,"n_params.from_name":"1M","config.n_params":393216,"config.n_layers":8,"config.n_heads":16,"config.d_model":64,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 64\nn_ctx: 2048\nd_head: 4\nmodel_name: TinyStories-Instruct-1M\nn_heads: 16\nd_mlp: 256\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-1M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZuT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 393216\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 64)\npos_embed:\n  W_pos: (2048, 64)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (64,)\n    ln2:\n      '[w, b]': (64,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 64, 4)\n      W_O: (16, 4, 64)\n      '[b_Q, b_K, b_V]': (16, 4)\n      b_O: (64,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (64, 256)\n      b_in: (256,)\n      W_out: (256, 64)\n      b_out: (64,)\nln_final:\n  '[w, b]': (64,)\nunembed:\n  W_U: (64, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 64)"},"pos_embed":{"W_pos":"(2048, 64)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(64,)"},"ln2":{"[w, b]":"(64,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 64, 4)","W_O":"(16, 4, 64)","[b_Q, b_K, b_V]":"(16, 4)","b_O":"(64,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(64, 256)","b_in":"(256,)","W_out":"(256, 64)","b_out":"(64,)"}}},"ln_final":{"[w, b]":"(64,)"},"unembed":{"W_U":"(64, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 4)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 64)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 256)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 64)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 64)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 64)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 4)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 256)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 64)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 64)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 64)"}}
{"name.default_alias":"tiny-stories-instruct-3M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-3M","n_params.as_str":"1.6M","n_params.as_int":1572864,"n_params.from_name":"3M","config.n_params":1572864,"config.n_layers":8,"config.n_heads":16,"config.d_model":128,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 128\nn_ctx: 2048\nd_head: 8\nmodel_name: TinyStories-Instruct-3M\nn_heads: 16\nd_mlp: 512\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-3M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgasj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1572864\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 128)\npos_embed:\n  W_pos: (2048, 128)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (128,)\n    ln2:\n      '[w, b]': (128,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 128, 8)\n      W_O: (16, 8, 128)\n      '[b_Q, b_K, b_V]': (16, 8)\n      b_O: (128,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (128, 512)\n      b_in: (512,)\n      W_out: (512, 128)\n      b_out: (128,)\nln_final:\n  '[w, b]': (128,)\nunembed:\n  W_U: (128, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 128)"},"pos_embed":{"W_pos":"(2048, 128)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(128,)"},"ln2":{"[w, b]":"(128,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 128, 8)","W_O":"(16, 8, 128)","[b_Q, b_K, b_V]":"(16, 8)","b_O":"(128,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(128, 512)","b_in":"(512,)","W_out":"(512, 128)","b_out":"(128,)"}}},"ln_final":{"[w, b]":"(128,)"},"unembed":{"W_U":"(128, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 8)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 128)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 512)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 128)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 128)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 128)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 8)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 512)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 128)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 128)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 128)"}}
{"name.default_alias":"tiny-stories-instruct-8M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-8M","n_params.as_str":"6.3M","n_params.as_int":6291456,"n_params.from_name":"8M","config.n_params":6291456,"config.n_layers":8,"config.n_heads":16,"config.d_model":256,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 256\nn_ctx: 2048\nd_head: 16\nmodel_name: TinyStories-Instruct-8M\nn_heads: 16\nd_mlp: 1024\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-8M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZqT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6291456\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 256)\npos_embed:\n  W_pos: (2048, 256)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (256,)\n    ln2:\n      '[w, b]': (256,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 256, 16)\n      W_O: (16, 16, 256)\n      '[b_Q, b_K, b_V]': (16, 16)\n      b_O: (256,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (256, 1024)\n      b_in: (1024,)\n      W_out: (1024, 256)\n      b_out: (256,)\nln_final:\n  '[w, b]': (256,)\nunembed:\n  W_U: (256, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 256)"},"pos_embed":{"W_pos":"(2048, 256)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(256,)"},"ln2":{"[w, b]":"(256,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 256, 16)","W_O":"(16, 16, 256)","[b_Q, b_K, b_V]":"(16, 16)","b_O":"(256,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(256, 1024)","b_in":"(1024,)","W_out":"(1024, 256)","b_out":"(256,)"}}},"ln_final":{"[w, b]":"(256,)"},"unembed":{"W_U":"(256, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 16)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 256)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 1024)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 256)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 256)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 16)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 1024)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 256)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 256)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 256)"}}
{"name.default_alias":"tiny-stories-instruct-28M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-28M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"28M","config.n_params":25165824,"config.n_layers":8,"config.n_heads":16,"config.d_model":512,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 8\nd_model: 512\nn_ctx: 2048\nd_head: 32\nmodel_name: TinyStories-Instruct-28M\nn_heads: 16\nd_mlp: 2048\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-28M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgaoj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 512)\npos_embed:\n  W_pos: (2048, 512)\nblocks:\n  '[0-7]':\n    ln1:\n      '[w, b]': (512,)\n    ln2:\n      '[w, b]': (512,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 512, 32)\n      W_O: (16, 32, 512)\n      '[b_Q, b_K, b_V]': (16, 32)\n      b_O: (512,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (512, 2048)\n      b_in: (2048,)\n      W_out: (2048, 512)\n      b_out: (512,)\nln_final:\n  '[w, b]': (512,)\nunembed:\n  W_U: (512, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 512)"},"pos_embed":{"W_pos":"(2048, 512)"},"blocks":{"[0-7]":{"ln1":{"[w, b]":"(512,)"},"ln2":{"[w, b]":"(512,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 512, 32)","W_O":"(16, 32, 512)","[b_Q, b_K, b_V]":"(16, 32)","b_O":"(512,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(512, 2048)","b_in":"(2048,)","W_out":"(2048, 512)","b_out":"(512,)"}}},"ln_final":{"[w, b]":"(512,)"},"unembed":{"W_U":"(512, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-7]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 32)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 512)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 2048)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 512)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 512)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-7]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 32)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 2048)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 512)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 512)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 512)"}}
{"name.default_alias":"tiny-stories-instruct-33M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-33M","n_params.as_str":"28M","n_params.as_int":28311552,"n_params.from_name":"33M","config.n_params":28311552,"config.n_layers":4,"config.n_heads":16,"config.d_model":768,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 4\nd_model: 768\nn_ctx: 2048\nd_head: 48\nmodel_name: TinyStories-Instruct-33M\nn_heads: 16\nd_mlp: 3072\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-33M\nwindow_size: 256\nattn_types:\n- global\n- local\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  LrjmCHKPnT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 28311552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 768)\npos_embed:\n  W_pos: (2048, 768)\nblocks:\n  '[0-3]':\n    ln1:\n      '[w, b]': (768,)\n    ln2:\n      '[w, b]': (768,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 768, 48)\n      W_O: (16, 48, 768)\n      '[b_Q, b_K, b_V]': (16, 48)\n      b_O: (768,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (768, 3072)\n      b_in: (3072,)\n      W_out: (3072, 768)\n      b_out: (768,)\nln_final:\n  '[w, b]': (768,)\nunembed:\n  W_U: (768, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 768)"},"pos_embed":{"W_pos":"(2048, 768)"},"blocks":{"[0-3]":{"ln1":{"[w, b]":"(768,)"},"ln2":{"[w, b]":"(768,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 768, 48)","W_O":"(16, 48, 768)","[b_Q, b_K, b_V]":"(16, 48)","b_O":"(768,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(768, 3072)","b_in":"(3072,)","W_out":"(3072, 768)","b_out":"(768,)"}}},"ln_final":{"[w, b]":"(768,)"},"unembed":{"W_U":"(768, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-3]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 48)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 768)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 3072)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 768)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 768)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-3]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 48)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 3072)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 768)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 768)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 768)"}}
{"name.default_alias":"tiny-stories-1L-21M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-1Layer-21M","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":"21M","config.n_params":12582912,"config.n_layers":1,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 1\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-1Layer-21M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-1Layer-21M\nwindow_size: 256\nattn_types:\n- global\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"0":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"tiny-stories-2L-33M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-2Layers-33M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"33M","config.n_params":25165824,"config.n_layers":2,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 2\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-2Layers-33M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-2Layers-33M\nwindow_size: 256\nattn_types:\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"tiny-stories-instruct-1L-21M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-Instuct-1Layer-21M","n_params.as_str":"13M","n_params.as_int":12582912,"n_params.from_name":"21M","config.n_params":12582912,"config.n_layers":1,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 1\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-Instuct-1Layer-21M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instuct-1Layer-21M\nwindow_size: 256\nattn_types:\n- global\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 12582912\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '0':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"0":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '0':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"0":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"tiny-stories-instruct-2L-33M","name.official":null,"name.aliases":[],"model_type":"tiny-stories","name.from_cfg":"TinyStories-Instruct-2Layers-33M","n_params.as_str":"25M","n_params.as_int":25165824,"n_params.from_name":"33M","config.n_params":25165824,"config.n_layers":2,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":50257,"config.act_fn":"gelu_new","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPTNeoForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 2\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: TinyStories-Instruct-2Layers-33M\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_new\nd_vocab: 50257\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: false\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: GPTNeoForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: roneneldan\/TinyStories-Instruct-2Layers-33M\nwindow_size: 256\nattn_types:\n- global\n- local\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 50257\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 25165824\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50257, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '[0-1]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 50257)\n  b_U: (50257,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50257, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"[0-1]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 50257)","b_U":"(50257,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-1]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-1]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 1024)"}}
{"name.default_alias":"stablelm-base-alpha-3b","name.official":"stabilityai\/stablelm-base-alpha-3b","name.aliases":["stablelm-base-alpha-3b","stablelm-base-3b"],"model_type":"stablelm","name.from_cfg":"stablelm-base-alpha-3b","n_params.as_str":"3.2B","n_params.as_int":3221225472,"n_params.from_name":"3b","config.n_params":3221225472,"config.n_layers":16,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":50688,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-base-alpha-3b\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-base-alpha-3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 3221225472\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 4096)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 4096)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"stablelm-base-alpha-7b","name.official":"stabilityai\/stablelm-base-alpha-7b","name.aliases":["stablelm-base-alpha-7b","stablelm-base-7b"],"model_type":"stablelm","name.from_cfg":"stablelm-base-alpha-7b","n_params.as_str":"7.2B","n_params.as_int":7247757312,"n_params.from_name":"7b","config.n_params":7247757312,"config.n_layers":16,"config.n_heads":48,"config.d_model":6144,"config.d_vocab":50432,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 6144\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-base-alpha-7b\nn_heads: 48\nd_mlp: 24576\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-base-alpha-7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  MvA88\/3mhD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 7247757312\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 6144)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (6144,)\n    ln2:\n      '[w, b]': (6144,)\n    attn:\n      '[W_Q, W_K, W_V]': (48, 6144, 128)\n      W_O: (48, 128, 6144)\n      '[b_Q, b_K, b_V]': (48, 128)\n      b_O: (6144,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (6144, 24576)\n      b_in: (24576,)\n      W_out: (24576, 6144)\n      b_out: (6144,)\nln_final:\n  '[w, b]': (6144,)\nunembed:\n  W_U: (6144, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 6144)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(6144,)"},"ln2":{"[w, b]":"(6144,)"},"attn":{"[W_Q, W_K, W_V]":"(48, 6144, 128)","W_O":"(48, 128, 6144)","[b_Q, b_K, b_V]":"(48, 128)","b_O":"(6144,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(6144, 24576)","b_in":"(24576,)","W_out":"(24576, 6144)","b_out":"(6144,)"}}},"ln_final":{"[w, b]":"(6144,)"},"unembed":{"W_U":"(6144, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        48, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 48, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      6144)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 6144)\nhook_embed: (batch, seq_len, 6144)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 48, 128)","[hook_attn_scores, hook_pattern]":"(batch, 48, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 6144)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"hook_embed":"(batch, seq_len, 6144)"}}
{"name.default_alias":"stablelm-tuned-alpha-3b","name.official":"stabilityai\/stablelm-tuned-alpha-3b","name.aliases":["stablelm-tuned-alpha-3b","stablelm-tuned-3b"],"model_type":"stablelm","name.from_cfg":"stablelm-tuned-alpha-3b","n_params.as_str":"3.2B","n_params.as_int":3221225472,"n_params.from_name":"3b","config.n_params":3221225472,"config.n_layers":16,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":50688,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 4096\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-tuned-alpha-3b\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu\nd_vocab: 50688\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-tuned-alpha-3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50688\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 3221225472\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50688, 4096)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 50688)\n  b_U: (50688,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50688, 4096)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 50688)","b_U":"(50688,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"stablelm-tuned-alpha-7b","name.official":"stabilityai\/stablelm-tuned-alpha-7b","name.aliases":["stablelm-tuned-alpha-7b","stablelm-tuned-7b"],"model_type":"stablelm","name.from_cfg":"stablelm-tuned-alpha-7b","n_params.as_str":"7.2B","n_params.as_int":7247757312,"n_params.from_name":"7b","config.n_params":7247757312,"config.n_layers":16,"config.n_heads":48,"config.d_model":6144,"config.d_vocab":50432,"config.act_fn":"gelu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":true,"config.original_architecture":"GPTNeoXForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 16\nd_model: 6144\nn_ctx: 4096\nd_head: 128\nmodel_name: stablelm-tuned-alpha-7b\nn_heads: 48\nd_mlp: 24576\nact_fn: gelu\nd_vocab: 50432\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPTNeoXForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: stabilityai\/stablelm-tuned-alpha-7b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  MvA88\/3mhD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 50432\nparallel_attn_mlp: true\nrotary_dim: 32\nn_params: 7247757312\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (50432, 6144)\nblocks:\n  '[0-15]':\n    ln1:\n      '[w, b]': (6144,)\n    ln2:\n      '[w, b]': (6144,)\n    attn:\n      '[W_Q, W_K, W_V]': (48, 6144, 128)\n      W_O: (48, 128, 6144)\n      '[b_Q, b_K, b_V]': (48, 128)\n      b_O: (6144,)\n      mask: (4096, 4096)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (4096, 32)\n    mlp:\n      W_in: (6144, 24576)\n      b_in: (24576,)\n      W_out: (24576, 6144)\n      b_out: (6144,)\nln_final:\n  '[w, b]': (6144,)\nunembed:\n  W_U: (6144, 50432)\n  b_U: (50432,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(50432, 6144)"},"blocks":{"[0-15]":{"ln1":{"[w, b]":"(6144,)"},"ln2":{"[w, b]":"(6144,)"},"attn":{"[W_Q, W_K, W_V]":"(48, 6144, 128)","W_O":"(48, 128, 6144)","[b_Q, b_K, b_V]":"(48, 128)","b_O":"(6144,)","mask":"(4096, 4096)","IGNORE":"()","[rotary_sin, rotary_cos]":"(4096, 32)"},"mlp":{"W_in":"(6144, 24576)","b_in":"(24576,)","W_out":"(24576, 6144)","b_out":"(6144,)"}}},"ln_final":{"[w, b]":"(6144,)"},"unembed":{"W_U":"(6144, 50432)","b_U":"(50432,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-15]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        48, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 48, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 6144)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 24576)\n    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,\n      6144)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 6144)\nhook_embed: (batch, seq_len, 6144)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-15]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 48, 128)","[hook_attn_scores, hook_pattern]":"(batch, 48, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 24576)"},"[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 6144)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 6144)"},"hook_embed":"(batch, seq_len, 6144)"}}
{"name.default_alias":"mistral-7b","name.official":"mistralai\/Mistral-7B-v0.1","name.aliases":["mistral-7b"],"model_type":"mistral","name.from_cfg":"Mistral-7B-v0.1","n_params.as_str":"5.9B","n_params.as_int":5905580032,"n_params.from_name":"7b","config.n_params":5905580032,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"MistralForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Mistral-7B-v0.1\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: MistralForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: mistralai\/Mistral-7B-v0.1\nwindow_size: 4096\nattn_types:\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5905580032\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      W_Q: (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      b_Q: (32, 128)\n      b_O: (4096,)\n      '[_W_K, _W_V]': (8, 4096, 128)\n      '[_b_K, _b_V]': (8, 128)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 14336)\n      b_in: (14336,)\n      W_out: (14336, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"W_Q":"(32, 4096, 128)","W_O":"(32, 128, 4096)","b_Q":"(32, 128)","b_O":"(4096,)","[_W_K, _W_V]":"(8, 4096, 128)","[_b_K, _b_V]":"(8, 128)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 14336)","b_in":"(14336,)","W_out":"(14336, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 8, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 14336)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 32, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 8, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 14336)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"mistral-7b-instruct","name.official":"mistralai\/Mistral-7B-Instruct-v0.1","name.aliases":["mistral-7b-instruct"],"model_type":"mistral","name.from_cfg":"Mistral-7B-Instruct-v0.1","n_params.as_str":"5.9B","n_params.as_int":5905580032,"n_params.from_name":"7b","config.n_params":5905580032,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":32000,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"MistralForCausalLM","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Mistral-7B-Instruct-v0.1\nn_heads: 32\nd_mlp: 14336\nact_fn: silu\nd_vocab: 32000\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: true\noriginal_architecture: MistralForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: mistralai\/Mistral-7B-Instruct-v0.1\nwindow_size: 4096\nattn_types:\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\n- local\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: false\nd_vocab_out: 32000\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5905580032\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: 8\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (32000, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      W_Q: (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      b_Q: (32, 128)\n      b_O: (4096,)\n      '[_W_K, _W_V]': (8, 4096, 128)\n      '[_b_K, _b_V]': (8, 128)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 14336)\n      b_in: (14336,)\n      W_out: (14336, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 32000)\n  b_U: (32000,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(32000, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"W_Q":"(32, 4096, 128)","W_O":"(32, 128, 4096)","b_Q":"(32, 128)","b_O":"(4096,)","[_W_K, _W_V]":"(8, 4096, 128)","[_b_K, _b_V]":"(8, 128)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 14336)","b_in":"(14336,)","W_out":"(14336, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 32000)","b_U":"(32000,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_rot_q, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_k, hook_v, hook_rot_k]': (batch, seq_len, 8, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 14336)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_rot_q, hook_z]":"(batch, seq_len, 32, 128)","[hook_k, hook_v, hook_rot_k]":"(batch, seq_len, 8, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 14336)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"bloom-560m","name.official":"bigscience\/bloom-560m","name.aliases":["bloom-560m"],"model_type":"bloom","name.from_cfg":"bloom-560m","n_params.as_str":"302M","n_params.as_int":301989888,"n_params.from_name":"560m","config.n_params":301989888,"config.n_layers":24,"config.n_heads":16,"config.d_model":1024,"config.d_vocab":250880,"config.act_fn":"gelu_fast","config.positional_embedding_type":"alibi","config.parallel_attn_mlp":false,"config.original_architecture":"BloomForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1024\nn_ctx: 2048\nd_head: 64\nmodel_name: bloom-560m\nn_heads: 16\nd_mlp: 4096\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-560m\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZmT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 301989888\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (1024,)\n  W_E: (250880, 1024)\npos_embed:\n  W_pos: (2048, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1024,)\n    ln2:\n      '[w, b]': (1024,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1024, 64)\n      W_O: (16, 64, 1024)\n      '[b_Q, b_K, b_V]': (16, 64)\n      b_O: (1024,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1024, 4096)\n      b_in: (4096,)\n      W_out: (4096, 1024)\n      b_out: (1024,)\nln_final:\n  '[w, b]': (1024,)\nunembed:\n  W_U: (1024, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(1024,)"},"W_E":"(250880, 1024)"},"pos_embed":{"W_pos":"(2048, 1024)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1024,)"},"ln2":{"[w, b]":"(1024,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1024, 64)","W_O":"(16, 64, 1024)","[b_Q, b_K, b_V]":"(16, 64)","b_O":"(1024,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1024, 4096)","b_in":"(4096,)","W_out":"(4096, 1024)","b_out":"(1024,)"}}},"ln_final":{"[w, b]":"(1024,)"},"unembed":{"W_U":"(1024, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 1024)\nblocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1024)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 4096)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1024)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1024)\nhook_embed: (batch, seq_len, 1024)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"}},"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 64)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 4096)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1024)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1024)"},"hook_embed":"(batch, seq_len, 1024)"}}
{"name.default_alias":"bloom-1b1","name.official":"bigscience\/bloom-1b1","name.aliases":["bloom-1b1"],"model_type":"bloom","name.from_cfg":"bloom-1b1","n_params.as_str":"679M","n_params.as_int":679477248,"n_params.from_name":null,"config.n_params":679477248,"config.n_layers":24,"config.n_heads":16,"config.d_model":1536,"config.d_vocab":250880,"config.act_fn":"gelu_fast","config.positional_embedding_type":"alibi","config.parallel_attn_mlp":false,"config.original_architecture":"BloomForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 1536\nn_ctx: 2048\nd_head: 96\nmodel_name: bloom-1b1\nn_heads: 16\nd_mlp: 6144\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-1b1\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  MvA88\/3mlD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 679477248\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (1536,)\n  W_E: (250880, 1536)\npos_embed:\n  W_pos: (2048, 1536)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (1536,)\n    ln2:\n      '[w, b]': (1536,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 1536, 96)\n      W_O: (16, 96, 1536)\n      '[b_Q, b_K, b_V]': (16, 96)\n      b_O: (1536,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (1536, 6144)\n      b_in: (6144,)\n      W_out: (6144, 1536)\n      b_out: (1536,)\nln_final:\n  '[w, b]': (1536,)\nunembed:\n  W_U: (1536, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(1536,)"},"W_E":"(250880, 1536)"},"pos_embed":{"W_pos":"(2048, 1536)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(1536,)"},"ln2":{"[w, b]":"(1536,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 1536, 96)","W_O":"(16, 96, 1536)","[b_Q, b_K, b_V]":"(16, 96)","b_O":"(1536,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(1536, 6144)","b_in":"(6144,)","W_out":"(6144, 1536)","b_out":"(1536,)"}}},"ln_final":{"[w, b]":"(1536,)"},"unembed":{"W_U":"(1536, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 1536)\nblocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 96)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 1536)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 6144)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 1536)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 1536)\nhook_embed: (batch, seq_len, 1536)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"}},"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 96)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 6144)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 1536)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 1536)"},"hook_embed":"(batch, seq_len, 1536)"}}
{"name.default_alias":"bloom-1b7","name.official":"bigscience\/bloom-1b7","name.aliases":["bloom-1b7"],"model_type":"bloom","name.from_cfg":"bloom-1b7","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":null,"config.n_params":1207959552,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":250880,"config.act_fn":"gelu_fast","config.positional_embedding_type":"alibi","config.parallel_attn_mlp":false,"config.original_architecture":"BloomForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: bloom-1b7\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-1b7\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (2048,)\n  W_E: (250880, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(2048,)"},"W_E":"(250880, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"}},"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"bloom-3b","name.official":"bigscience\/bloom-3b","name.aliases":["bloom-3b"],"model_type":"bloom","name.from_cfg":"bloom-3b","n_params.as_str":"2.4B","n_params.as_int":2359296000,"n_params.from_name":"3b","config.n_params":2359296000,"config.n_layers":30,"config.n_heads":32,"config.d_model":2560,"config.d_vocab":250880,"config.act_fn":"gelu_fast","config.positional_embedding_type":"alibi","config.parallel_attn_mlp":false,"config.original_architecture":"BloomForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 30\nd_model: 2560\nn_ctx: 2048\nd_head: 80\nmodel_name: bloom-3b\nn_heads: 32\nd_mlp: 10240\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-3b\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  cjqgTtwwkD8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 2359296000\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (2560,)\n  W_E: (250880, 2560)\npos_embed:\n  W_pos: (2048, 2560)\nblocks:\n  '[0-29]':\n    ln1:\n      '[w, b]': (2560,)\n    ln2:\n      '[w, b]': (2560,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 2560, 80)\n      W_O: (32, 80, 2560)\n      '[b_Q, b_K, b_V]': (32, 80)\n      b_O: (2560,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2560, 10240)\n      b_in: (10240,)\n      W_out: (10240, 2560)\n      b_out: (2560,)\nln_final:\n  '[w, b]': (2560,)\nunembed:\n  W_U: (2560, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(2560,)"},"W_E":"(250880, 2560)"},"pos_embed":{"W_pos":"(2048, 2560)"},"blocks":{"[0-29]":{"ln1":{"[w, b]":"(2560,)"},"ln2":{"[w, b]":"(2560,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 2560, 80)","W_O":"(32, 80, 2560)","[b_Q, b_K, b_V]":"(32, 80)","b_O":"(2560,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2560, 10240)","b_in":"(10240,)","W_out":"(10240, 2560)","b_out":"(2560,)"}}},"ln_final":{"[w, b]":"(2560,)"},"unembed":{"W_U":"(2560, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 2560)\nblocks:\n  '[0-29]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 80)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2560)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 10240)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2560)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2560)\nhook_embed: (batch, seq_len, 2560)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"}},"blocks":{"[0-29]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 80)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 10240)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2560)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2560)"},"hook_embed":"(batch, seq_len, 2560)"}}
{"name.default_alias":"bloom-7b1","name.official":"bigscience\/bloom-7b1","name.aliases":["bloom-7b1"],"model_type":"bloom","name.from_cfg":"bloom-7b1","n_params.as_str":"6.0B","n_params.as_int":6039797760,"n_params.from_name":null,"config.n_params":6039797760,"config.n_layers":30,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":250880,"config.act_fn":"gelu_fast","config.positional_embedding_type":"alibi","config.parallel_attn_mlp":false,"config.original_architecture":"BloomForCausalLM","config.normalization_type":"LN","cfg":"n_layers: 30\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: bloom-7b1\nn_heads: 32\nd_mlp: 16384\nact_fn: gelu_fast\nd_vocab: 250880\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: BloomForCausalLM\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigscience\/bloom-7b1\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  mpmZmZmZiT8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: alibi\nfinal_rms: false\nd_vocab_out: 250880\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 6039797760\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: true\nrotary_base: 10000\ntrust_remote_code: false\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  ln:\n    '[w, b]': (4096,)\n  W_E: (250880, 4096)\npos_embed:\n  W_pos: (2048, 4096)\nblocks:\n  '[0-29]':\n    ln1:\n      '[w, b]': (4096,)\n    ln2:\n      '[w, b]': (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (4096, 16384)\n      b_in: (16384,)\n      W_out: (16384, 4096)\n      b_out: (4096,)\nln_final:\n  '[w, b]': (4096,)\nunembed:\n  W_U: (4096, 250880)\n  b_U: (250880,)\n","tensor_shapes.state_dict.raw__":{"embed":{"ln":{"[w, b]":"(4096,)"},"W_E":"(250880, 4096)"},"pos_embed":{"W_pos":"(2048, 4096)"},"blocks":{"[0-29]":{"ln1":{"[w, b]":"(4096,)"},"ln2":{"[w, b]":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(4096, 16384)","b_in":"(16384,)","W_out":"(16384, 4096)","b_out":"(4096,)"}}},"ln_final":{"[w, b]":"(4096,)"},"unembed":{"W_U":"(4096, 250880)","b_U":"(250880,)"}},"tensor_shapes.activation_cache":"embed:\n  ln:\n    hook_scale: (batch, seq_len, 1)\n    hook_normalized: (batch, seq_len, 4096)\nblocks:\n  '[0-29]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 16384)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"embed":{"ln":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"}},"blocks":{"[0-29]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 16384)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"santacoder","name.official":"bigcode\/santacoder","name.aliases":["santacoder"],"model_type":null,"name.from_cfg":"santacoder","n_params.as_str":"1.2B","n_params.as_int":1207959552,"n_params.from_name":null,"config.n_params":1207959552,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":49280,"config.act_fn":"gelu_fast","config.positional_embedding_type":"standard","config.parallel_attn_mlp":false,"config.original_architecture":"GPT2LMHeadCustomModel","config.normalization_type":"LN","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: santacoder\nn_heads: 16\nd_mlp: 8192\nact_fn: gelu_fast\nd_vocab: 49280\neps: 1.0e-05\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: GPT2LMHeadCustomModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: bigcode\/santacoder\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: LN\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: !!python\/object\/apply:numpy.core.multiarray.scalar\n- !!python\/object\/apply:numpy.dtype\n  args:\n  - f8\n  - false\n  - true\n  state: !!python\/tuple\n  - 3\n  - <\n  - null\n  - null\n  - null\n  - -1\n  - -1\n  - 0\n- !!binary |\n  CmP\/URgakj8=\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: standard\nfinal_rms: false\nd_vocab_out: 49280\nparallel_attn_mlp: false\nrotary_dim: null\nn_params: 1207959552\nuse_hook_tokens: false\ngated_mlp: false\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: null\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (49280, 2048)\npos_embed:\n  W_pos: (2048, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      '[w, b]': (2048,)\n    ln2:\n      '[w, b]': (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n    mlp:\n      W_in: (2048, 8192)\n      b_in: (8192,)\n      W_out: (8192, 2048)\n      b_out: (2048,)\nln_final:\n  '[w, b]': (2048,)\nunembed:\n  W_U: (2048, 49280)\n  b_U: (49280,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(49280, 2048)"},"pos_embed":{"W_pos":"(2048, 2048)"},"blocks":{"[0-23]":{"ln1":{"[w, b]":"(2048,)"},"ln2":{"[w, b]":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()"},"mlp":{"W_in":"(2048, 8192)","b_in":"(8192,)","W_out":"(8192, 2048)","b_out":"(2048,)"}}},"ln_final":{"[w, b]":"(2048,)"},"unembed":{"W_U":"(2048, 49280)","b_U":"(49280,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_post]': (batch, seq_len, 8192)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\n'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_post]":"(batch, seq_len, 8192)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"[hook_embed, hook_pos_embed]":"(batch, seq_len, 2048)"}}
{"name.default_alias":"qwen-1.8b","name.official":"Qwen\/Qwen-1_8B","name.aliases":["qwen-1.8b"],"model_type":"qwen","name.from_cfg":"Qwen-1_8B","n_params.as_str":"944M","n_params.as_int":943718400,"n_params.from_name":"1.8b","config.n_params":943718400,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":151936,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"QWenLMHeadModel","config.normalization_type":"RMS","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-1_8B\nn_heads: 16\nd_mlp: 5504\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-1_8B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 943718400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      w: (2048,)\n    ln2:\n      w: (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (2048, 5504)\n      b_in: (5504,)\n      W_out: (5504, 2048)\n      b_out: (2048,)\nln_final:\n  w: (2048,)\nunembed:\n  W_U: (2048, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 2048)"},"blocks":{"[0-23]":{"ln1":{"w":"(2048,)"},"ln2":{"w":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(2048, 5504)","b_in":"(5504,)","W_out":"(5504, 2048)","b_out":"(2048,)"}}},"ln_final":{"w":"(2048,)"},"unembed":{"W_U":"(2048, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 5504)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 5504)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"qwen-7b","name.official":"Qwen\/Qwen-7B","name.aliases":["qwen-7b"],"model_type":"qwen","name.from_cfg":"Qwen-7B","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":151936,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"QWenLMHeadModel","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-7B\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-7B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"qwen-14b","name.official":"Qwen\/Qwen-14B","name.aliases":["qwen-14b"],"model_type":"qwen","name.from_cfg":"Qwen-14B","n_params.as_str":"9.8B","n_params.as_int":9804185600,"n_params.from_name":"14b","config.n_params":9804185600,"config.n_layers":40,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":152064,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"QWenLMHeadModel","config.normalization_type":"RMS","cfg":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-14B\nn_heads: 40\nd_mlp: 13696\nact_fn: silu\nd_vocab: 152064\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-14B\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 152064\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 9804185600\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (152064, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (5120,)\n    ln2:\n      w: (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (5120, 13696)\n      b_in: (13696,)\n      W_out: (13696, 5120)\n      b_out: (5120,)\nln_final:\n  w: (5120,)\nunembed:\n  W_U: (5120, 152064)\n  b_U: (152064,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(152064, 5120)"},"blocks":{"[0-39]":{"ln1":{"w":"(5120,)"},"ln2":{"w":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(5120, 13696)","b_in":"(13696,)","W_out":"(13696, 5120)","b_out":"(5120,)"}}},"ln_final":{"w":"(5120,)"},"unembed":{"W_U":"(5120, 152064)","b_U":"(152064,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13696)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 13696)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
{"name.default_alias":"qwen-1.8b-chat","name.official":"Qwen\/Qwen-1_8B-Chat","name.aliases":["qwen-1.8b-chat"],"model_type":"qwen","name.from_cfg":"Qwen-1_8B-Chat","n_params.as_str":"944M","n_params.as_int":943718400,"n_params.from_name":"1.8b","config.n_params":943718400,"config.n_layers":24,"config.n_heads":16,"config.d_model":2048,"config.d_vocab":151936,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"QWenLMHeadModel","config.normalization_type":"RMS","cfg":"n_layers: 24\nd_model: 2048\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-1_8B-Chat\nn_heads: 16\nd_mlp: 5504\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-1_8B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 943718400\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 2048)\nblocks:\n  '[0-23]':\n    ln1:\n      w: (2048,)\n    ln2:\n      w: (2048,)\n    attn:\n      '[W_Q, W_K, W_V]': (16, 2048, 128)\n      W_O: (16, 128, 2048)\n      '[b_Q, b_K, b_V]': (16, 128)\n      b_O: (2048,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (2048, 5504)\n      b_in: (5504,)\n      W_out: (5504, 2048)\n      b_out: (2048,)\nln_final:\n  w: (2048,)\nunembed:\n  W_U: (2048, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 2048)"},"blocks":{"[0-23]":{"ln1":{"w":"(2048,)"},"ln2":{"w":"(2048,)"},"attn":{"[W_Q, W_K, W_V]":"(16, 2048, 128)","W_O":"(16, 128, 2048)","[b_Q, b_K, b_V]":"(16, 128)","b_O":"(2048,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(2048, 5504)","b_in":"(5504,)","W_out":"(5504, 2048)","b_out":"(2048,)"}}},"ln_final":{"w":"(2048,)"},"unembed":{"W_U":"(2048, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-23]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        16, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 2048)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 5504)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 2048)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 2048)\nhook_embed: (batch, seq_len, 2048)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-23]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 16, 128)","[hook_attn_scores, hook_pattern]":"(batch, 16, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 5504)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 2048)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 2048)"},"hook_embed":"(batch, seq_len, 2048)"}}
{"name.default_alias":"qwen-7b-chat","name.official":"Qwen\/Qwen-7B-Chat","name.aliases":["qwen-7b-chat"],"model_type":"qwen","name.from_cfg":"Qwen-7B-Chat","n_params.as_str":"5.0B","n_params.as_int":5033164800,"n_params.from_name":"7b","config.n_params":5033164800,"config.n_layers":32,"config.n_heads":32,"config.d_model":4096,"config.d_vocab":151936,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"QWenLMHeadModel","config.normalization_type":"RMS","cfg":"n_layers: 32\nd_model: 4096\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-7B-Chat\nn_heads: 32\nd_mlp: 11008\nact_fn: silu\nd_vocab: 151936\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-7B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 151936\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 5033164800\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (151936, 4096)\nblocks:\n  '[0-31]':\n    ln1:\n      w: (4096,)\n    ln2:\n      w: (4096,)\n    attn:\n      '[W_Q, W_K, W_V]': (32, 4096, 128)\n      W_O: (32, 128, 4096)\n      '[b_Q, b_K, b_V]': (32, 128)\n      b_O: (4096,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (4096, 11008)\n      b_in: (11008,)\n      W_out: (11008, 4096)\n      b_out: (4096,)\nln_final:\n  w: (4096,)\nunembed:\n  W_U: (4096, 151936)\n  b_U: (151936,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(151936, 4096)"},"blocks":{"[0-31]":{"ln1":{"w":"(4096,)"},"ln2":{"w":"(4096,)"},"attn":{"[W_Q, W_K, W_V]":"(32, 4096, 128)","W_O":"(32, 128, 4096)","[b_Q, b_K, b_V]":"(32, 128)","b_O":"(4096,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(4096, 11008)","b_in":"(11008,)","W_out":"(11008, 4096)","b_out":"(4096,)"}}},"ln_final":{"w":"(4096,)"},"unembed":{"W_U":"(4096, 151936)","b_U":"(151936,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-31]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        32, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 4096)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 4096)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 4096)\nhook_embed: (batch, seq_len, 4096)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-31]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 32, 128)","[hook_attn_scores, hook_pattern]":"(batch, 32, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 11008)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 4096)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 4096)"},"hook_embed":"(batch, seq_len, 4096)"}}
{"name.default_alias":"qwen-14b-chat","name.official":"Qwen\/Qwen-14B-Chat","name.aliases":["qwen-14b-chat"],"model_type":"qwen","name.from_cfg":"Qwen-14B-Chat","n_params.as_str":"9.8B","n_params.as_int":9804185600,"n_params.from_name":"14b","config.n_params":9804185600,"config.n_layers":40,"config.n_heads":40,"config.d_model":5120,"config.d_vocab":152064,"config.act_fn":"silu","config.positional_embedding_type":"rotary","config.parallel_attn_mlp":false,"config.original_architecture":"QWenLMHeadModel","config.normalization_type":"RMS","cfg":"n_layers: 40\nd_model: 5120\nn_ctx: 2048\nd_head: 128\nmodel_name: Qwen-14B-Chat\nn_heads: 40\nd_mlp: 13696\nact_fn: silu\nd_vocab: 152064\neps: 1.0e-06\nuse_attn_result: false\nuse_attn_scale: true\nuse_split_qkv_input: false\nuse_hook_mlp_in: false\nuse_attn_in: false\nuse_local_attn: false\noriginal_architecture: QWenLMHeadModel\nfrom_checkpoint: false\ncheckpoint_index: null\ncheckpoint_label_type: null\ncheckpoint_value: null\ntokenizer_name: Qwen\/Qwen-14B-Chat\nwindow_size: null\nattn_types: null\ninit_mode: gpt2\nnormalization_type: RMS\ndevice: cpu\nn_devices: 1\nattention_dir: causal\nattn_only: false\nseed: null\ninitializer_range: 0.02\ninit_weights: false\nscale_attn_by_inverse_layer_idx: false\npositional_embedding_type: rotary\nfinal_rms: true\nd_vocab_out: 152064\nparallel_attn_mlp: false\nrotary_dim: 128\nn_params: 9804185600\nuse_hook_tokens: false\ngated_mlp: true\ndefault_prepend_bos: true\ndtype: torch.float32\ntokenizer_prepends_bos: true\nn_key_value_heads: null\npost_embedding_ln: false\nrotary_base: 10000\ntrust_remote_code: true\nrotary_adjacent_pairs: false\n","tensor_shapes.state_dict":"embed:\n  W_E: (152064, 5120)\nblocks:\n  '[0-39]':\n    ln1:\n      w: (5120,)\n    ln2:\n      w: (5120,)\n    attn:\n      '[W_Q, W_K, W_V]': (40, 5120, 128)\n      W_O: (40, 128, 5120)\n      '[b_Q, b_K, b_V]': (40, 128)\n      b_O: (5120,)\n      mask: (2048, 2048)\n      IGNORE: ()\n      '[rotary_sin, rotary_cos]': (2048, 128)\n    mlp:\n      '[W_in, W_gate]': (5120, 13696)\n      b_in: (13696,)\n      W_out: (13696, 5120)\n      b_out: (5120,)\nln_final:\n  w: (5120,)\nunembed:\n  W_U: (5120, 152064)\n  b_U: (152064,)\n","tensor_shapes.state_dict.raw__":{"embed":{"W_E":"(152064, 5120)"},"blocks":{"[0-39]":{"ln1":{"w":"(5120,)"},"ln2":{"w":"(5120,)"},"attn":{"[W_Q, W_K, W_V]":"(40, 5120, 128)","W_O":"(40, 128, 5120)","[b_Q, b_K, b_V]":"(40, 128)","b_O":"(5120,)","mask":"(2048, 2048)","IGNORE":"()","[rotary_sin, rotary_cos]":"(2048, 128)"},"mlp":{"[W_in, W_gate]":"(5120, 13696)","b_in":"(13696,)","W_out":"(13696, 5120)","b_out":"(5120,)"}}},"ln_final":{"w":"(5120,)"},"unembed":{"W_U":"(5120, 152064)","b_U":"(152064,)"}},"tensor_shapes.activation_cache":"blocks:\n  '[0-39]':\n    ln1:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    attn:\n      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,\n        40, 128)\n      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)\n    ln2:\n      hook_scale: (batch, seq_len, 1)\n      hook_normalized: (batch, seq_len, 5120)\n    mlp:\n      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13696)\n    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,\n      seq_len, 5120)\nln_final:\n  hook_scale: (batch, seq_len, 1)\n  hook_normalized: (batch, seq_len, 5120)\nhook_embed: (batch, seq_len, 5120)\n","tensor_shapes.activation_cache.raw__":{"blocks":{"[0-39]":{"ln1":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"attn":{"[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]":"(batch, seq_len, 40, 128)","[hook_attn_scores, hook_pattern]":"(batch, 40, seq_len, seq_len)"},"ln2":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"mlp":{"[hook_pre, hook_pre_linear, hook_post]":"(batch, seq_len, 13696)"},"[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]":"(batch, seq_len, 5120)"}},"ln_final":{"hook_scale":"(batch, seq_len, 1)","hook_normalized":"(batch, seq_len, 5120)"},"hook_embed":"(batch, seq_len, 5120)"}}
