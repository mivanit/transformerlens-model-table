name.default_alias,name.official,name.aliases,model_type,name.from_cfg,n_params.as_str,n_params.as_int,n_params.from_name,config.n_params,config.n_layers,config.n_heads,config.d_model,config.d_vocab,config.act_fn,config.positional_embedding_type,config.parallel_attn_mlp,config.original_architecture,config.normalization_type,cfg,tensor_shapes.state_dict,tensor_shapes.state_dict.raw__,tensor_shapes.activation_cache,tensor_shapes.activation_cache.raw__
gpt2-small,gpt2,['gpt2-small'],gpt2,gpt2,85M,84934656,,84934656,12,12,768,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 12
d_model: 768
n_ctx: 1024
d_head: 64
model_name: gpt2
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: gpt2
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
gpt2-medium,gpt2-medium,[],gpt2,gpt2-medium,302M,301989888,,301989888,24,16,1024,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 24
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: gpt2-medium
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: gpt2-medium
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
gpt2-large,gpt2-large,[],gpt2,gpt2-large,708M,707788800,,707788800,36,20,1280,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 36
d_model: 1280
n_ctx: 1024
d_head: 64
model_name: gpt2-large
n_heads: 20
d_mlp: 5120
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: gpt2-large
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bfllj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 707788800
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1280)
pos_embed:
  W_pos: (1024, 1280)
blocks:
  '[0-35]':
    ln1:
      '[w, b]': (1280,)
    ln2:
      '[w, b]': (1280,)
    attn:
      '[W_Q, W_K, W_V]': (20, 1280, 64)
      W_O: (20, 64, 1280)
      '[b_Q, b_K, b_V]': (20, 64)
      b_O: (1280,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1280, 5120)
      b_in: (5120,)
      W_out: (5120, 1280)
      b_out: (1280,)
ln_final:
  '[w, b]': (1280,)
unembed:
  W_U: (1280, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1280)'}, 'pos_embed': {'W_pos': '(1024, 1280)'}, 'blocks': {'[0-35]': {'ln1': {'[w, b]': '(1280,)'}, 'ln2': {'[w, b]': '(1280,)'}, 'attn': {'[W_Q, W_K, W_V]': '(20, 1280, 64)', 'W_O': '(20, 64, 1280)', '[b_Q, b_K, b_V]': '(20, 64)', 'b_O': '(1280,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1280, 5120)', 'b_in': '(5120,)', 'W_out': '(5120, 1280)', 'b_out': '(1280,)'}}}, 'ln_final': {'[w, b]': '(1280,)'}, 'unembed': {'W_U': '(1280, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-35]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1280)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1280)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 5120)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1280)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1280)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)
","{'blocks': {'[0-35]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 20, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 20, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 5120)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1280)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1280)'}"
gpt2-xl,gpt2-xl,[],gpt2,gpt2-xl,1.5B,1474560000,,1474560000,48,25,1600,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 48
d_model: 1600
n_ctx: 1024
d_head: 64
model_name: gpt2-xl
n_heads: 25
d_mlp: 6400
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: gpt2-xl
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  exSuR+F6lD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 1474560000
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1600)
pos_embed:
  W_pos: (1024, 1600)
blocks:
  '[0-47]':
    ln1:
      '[w, b]': (1600,)
    ln2:
      '[w, b]': (1600,)
    attn:
      '[W_Q, W_K, W_V]': (25, 1600, 64)
      W_O: (25, 64, 1600)
      '[b_Q, b_K, b_V]': (25, 64)
      b_O: (1600,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1600, 6400)
      b_in: (6400,)
      W_out: (6400, 1600)
      b_out: (1600,)
ln_final:
  '[w, b]': (1600,)
unembed:
  W_U: (1600, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1600)'}, 'pos_embed': {'W_pos': '(1024, 1600)'}, 'blocks': {'[0-47]': {'ln1': {'[w, b]': '(1600,)'}, 'ln2': {'[w, b]': '(1600,)'}, 'attn': {'[W_Q, W_K, W_V]': '(25, 1600, 64)', 'W_O': '(25, 64, 1600)', '[b_Q, b_K, b_V]': '(25, 64)', 'b_O': '(1600,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1600, 6400)', 'b_in': '(6400,)', 'W_out': '(6400, 1600)', 'b_out': '(1600,)'}}}, 'ln_final': {'[w, b]': '(1600,)'}, 'unembed': {'W_U': '(1600, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-47]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1600)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 25, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 25, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1600)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 6400)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1600)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1600)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1600)
","{'blocks': {'[0-47]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1600)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 25, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 25, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1600)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 6400)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1600)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1600)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1600)'}"
distillgpt2,distilgpt2,"['distillgpt2', 'distill-gpt2', 'distil-gpt2', 'gpt2-xs']",gpt2,distilgpt2,42M,42467328,,42467328,6,12,768,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 6
d_model: 768
n_ctx: 1024
d_head: 64
model_name: distilgpt2
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: distilgpt2
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 42467328
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
opt-125m,facebook/opt-125m,"['opt-125m', 'opt-small', 'opt']",opt,opt-125m,85M,84934656,125m,84934656,12,12,768,50272,relu,standard,False,OPTForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: opt-125m
n_heads: 12
d_mlp: 3072
act_fn: relu
d_vocab: 50272
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: OPTForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: facebook/opt-125m
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50272
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50272, 768)
pos_embed:
  W_pos: (2048, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50272)
  b_U: (50272,)
","{'embed': {'W_E': '(50272, 768)'}, 'pos_embed': {'W_pos': '(2048, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50272)', 'b_U': '(50272,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
opt-1.3b,facebook/opt-1.3b,"['opt-1.3b', 'opt-medium']",opt,opt-1.3b,1.2B,1207959552,1.3b,1207959552,24,32,2048,50272,relu,standard,False,OPTForCausalLM,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 64
model_name: opt-1.3b
n_heads: 32
d_mlp: 8192
act_fn: relu
d_vocab: 50272
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: OPTForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: facebook/opt-1.3b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50272
parallel_attn_mlp: false
rotary_dim: null
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50272, 2048)
pos_embed:
  W_pos: (2048, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (32, 2048, 64)
      W_O: (32, 64, 2048)
      '[b_Q, b_K, b_V]': (32, 64)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50272)
  b_U: (50272,)
","{'embed': {'W_E': '(50272, 2048)'}, 'pos_embed': {'W_pos': '(2048, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 2048, 64)', 'W_O': '(32, 64, 2048)', '[b_Q, b_K, b_V]': '(32, 64)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50272)', 'b_U': '(50272,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 32, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 2048)'}"
opt-2.7b,facebook/opt-2.7b,"['opt-2.7b', 'opt-large']",opt,opt-2.7b,2.5B,2516582400,2.7b,2516582400,32,32,2560,50272,relu,standard,False,OPTForCausalLM,LN,"n_layers: 32
d_model: 2560
n_ctx: 2048
d_head: 80
model_name: opt-2.7b
n_heads: 32
d_mlp: 10240
act_fn: relu
d_vocab: 50272
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: OPTForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: facebook/opt-2.7b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  cjqgTtwwkD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50272
parallel_attn_mlp: false
rotary_dim: null
n_params: 2516582400
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50272, 2560)
pos_embed:
  W_pos: (2048, 2560)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (2560,)
    ln2:
      '[w, b]': (2560,)
    attn:
      '[W_Q, W_K, W_V]': (32, 2560, 80)
      W_O: (32, 80, 2560)
      '[b_Q, b_K, b_V]': (32, 80)
      b_O: (2560,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (2560, 10240)
      b_in: (10240,)
      W_out: (10240, 2560)
      b_out: (2560,)
ln_final:
  '[w, b]': (2560,)
unembed:
  W_U: (2560, 50272)
  b_U: (50272,)
","{'embed': {'W_E': '(50272, 2560)'}, 'pos_embed': {'W_pos': '(2048, 2560)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(2560,)'}, 'ln2': {'[w, b]': '(2560,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 2560, 80)', 'W_O': '(32, 80, 2560)', '[b_Q, b_K, b_V]': '(32, 80)', 'b_O': '(2560,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(2560, 10240)', 'b_in': '(10240,)', 'W_out': '(10240, 2560)', 'b_out': '(2560,)'}}}, 'ln_final': {'[w, b]': '(2560,)'}, 'unembed': {'W_U': '(2560, 50272)', 'b_U': '(50272,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 80)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 10240)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2560)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2560)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 2560)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 32, 80)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 10240)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2560)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 2560)'}"
opt-6.7b,facebook/opt-6.7b,"['opt-6.7b', 'opt-xl']",opt,opt-6.7b,6.4B,6442450944,6.7b,6442450944,32,32,4096,50272,relu,standard,False,OPTForCausalLM,LN,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: opt-6.7b
n_heads: 32
d_mlp: 16384
act_fn: relu
d_vocab: 50272
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: OPTForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: facebook/opt-6.7b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50272
parallel_attn_mlp: false
rotary_dim: null
n_params: 6442450944
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50272, 4096)
pos_embed:
  W_pos: (2048, 4096)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50272)
  b_U: (50272,)
","{'embed': {'W_E': '(50272, 4096)'}, 'pos_embed': {'W_pos': '(2048, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50272)', 'b_U': '(50272,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 4096)'}"
opt-13b,facebook/opt-13b,"['opt-13b', 'opt-xxl']",opt,opt-13b,13B,12582912000,13b,12582912000,40,40,5120,50272,relu,standard,False,OPTForCausalLM,LN,"n_layers: 40
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: opt-13b
n_heads: 40
d_mlp: 20480
act_fn: relu
d_vocab: 50272
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: OPTForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: facebook/opt-13b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50272
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912000
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50272, 5120)
pos_embed:
  W_pos: (2048, 5120)
blocks:
  '[0-39]':
    ln1:
      '[w, b]': (5120,)
    ln2:
      '[w, b]': (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (5120, 20480)
      b_in: (20480,)
      W_out: (20480, 5120)
      b_out: (5120,)
ln_final:
  '[w, b]': (5120,)
unembed:
  W_U: (5120, 50272)
  b_U: (50272,)
","{'embed': {'W_E': '(50272, 5120)'}, 'pos_embed': {'W_pos': '(2048, 5120)'}, 'blocks': {'[0-39]': {'ln1': {'[w, b]': '(5120,)'}, 'ln2': {'[w, b]': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(5120, 20480)', 'b_in': '(20480,)', 'W_out': '(20480, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'[w, b]': '(5120,)'}, 'unembed': {'W_U': '(5120, 50272)', 'b_U': '(50272,)'}}","blocks:
  '[0-39]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 20480)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 5120)
","{'blocks': {'[0-39]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 20480)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 5120)'}"
opt-30b,facebook/opt-30b,"['opt-30b', 'opt-xxxl']",opt,opt-30b,30B,29595009024,30b,29595009024,48,56,7168,50272,relu,standard,False,OPTForCausalLM,LN,"n_layers: 48
d_model: 7168
n_ctx: 2048
d_head: 128
model_name: opt-30b
n_heads: 56
d_mlp: 28672
act_fn: relu
d_vocab: 50272
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: OPTForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: facebook/opt-30b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  hfkfUg5agz8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50272
parallel_attn_mlp: false
rotary_dim: null
n_params: 29595009024
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50272, 7168)
pos_embed:
  W_pos: (2048, 7168)
blocks:
  '[0-47]':
    ln1:
      '[w, b]': (7168,)
    ln2:
      '[w, b]': (7168,)
    attn:
      '[W_Q, W_K, W_V]': (56, 7168, 128)
      W_O: (56, 128, 7168)
      '[b_Q, b_K, b_V]': (56, 128)
      b_O: (7168,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (7168, 28672)
      b_in: (28672,)
      W_out: (28672, 7168)
      b_out: (7168,)
ln_final:
  '[w, b]': (7168,)
unembed:
  W_U: (7168, 50272)
  b_U: (50272,)
","{'embed': {'W_E': '(50272, 7168)'}, 'pos_embed': {'W_pos': '(2048, 7168)'}, 'blocks': {'[0-47]': {'ln1': {'[w, b]': '(7168,)'}, 'ln2': {'[w, b]': '(7168,)'}, 'attn': {'[W_Q, W_K, W_V]': '(56, 7168, 128)', 'W_O': '(56, 128, 7168)', '[b_Q, b_K, b_V]': '(56, 128)', 'b_O': '(7168,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(7168, 28672)', 'b_in': '(28672,)', 'W_out': '(28672, 7168)', 'b_out': '(7168,)'}}}, 'ln_final': {'[w, b]': '(7168,)'}, 'unembed': {'W_U': '(7168, 50272)', 'b_U': '(50272,)'}}","blocks:
  '[0-47]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 7168)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 56, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 56, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 7168)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 28672)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 7168)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 7168)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 7168)
","{'blocks': {'[0-47]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 7168)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 56, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 56, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 7168)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 28672)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 7168)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 7168)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 7168)'}"
opt-66b,facebook/opt-66b,"['opt-66b', 'opt-xxxxl']",opt,opt-66b,65B,65229815808,66b,65229815808,64,72,9216,50272,relu,standard,False,OPTForCausalLM,LN,"n_layers: 64
d_model: 9216
n_ctx: 2048
d_head: 128
model_name: opt-66b
n_heads: 72
d_mlp: 36864
act_fn: relu
d_vocab: 50272
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: OPTForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: facebook/opt-66b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  ERERERERgT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50272
parallel_attn_mlp: false
rotary_dim: null
n_params: 65229815808
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50272, 9216)
pos_embed:
  W_pos: (2048, 9216)
blocks:
  '[0-63]':
    ln1:
      '[w, b]': (9216,)
    ln2:
      '[w, b]': (9216,)
    attn:
      '[W_Q, W_K, W_V]': (72, 9216, 128)
      W_O: (72, 128, 9216)
      '[b_Q, b_K, b_V]': (72, 128)
      b_O: (9216,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (9216, 36864)
      b_in: (36864,)
      W_out: (36864, 9216)
      b_out: (9216,)
ln_final:
  '[w, b]': (9216,)
unembed:
  W_U: (9216, 50272)
  b_U: (50272,)
","{'embed': {'W_E': '(50272, 9216)'}, 'pos_embed': {'W_pos': '(2048, 9216)'}, 'blocks': {'[0-63]': {'ln1': {'[w, b]': '(9216,)'}, 'ln2': {'[w, b]': '(9216,)'}, 'attn': {'[W_Q, W_K, W_V]': '(72, 9216, 128)', 'W_O': '(72, 128, 9216)', '[b_Q, b_K, b_V]': '(72, 128)', 'b_O': '(9216,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(9216, 36864)', 'b_in': '(36864,)', 'W_out': '(36864, 9216)', 'b_out': '(9216,)'}}}, 'ln_final': {'[w, b]': '(9216,)'}, 'unembed': {'W_U': '(9216, 50272)', 'b_U': '(50272,)'}}","blocks:
  '[0-63]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 9216)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 72, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 72, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 9216)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 36864)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 9216)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 9216)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 9216)
","{'blocks': {'[0-63]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 9216)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 72, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 72, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 9216)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 36864)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 9216)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 9216)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 9216)'}"
gpt-neo-125M,,[],gpt-neo,gpt-neo-125M,85M,84934656,125M,84934656,12,12,768,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: gpt-neo-125M
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neo-125M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (2048, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(2048, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
gpt-neo-1.3B,,[],gpt-neo,gpt-neo-1.3B,1.2B,1207959552,1.3B,1207959552,24,16,2048,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: gpt-neo-1.3B
n_heads: 16
d_mlp: 8192
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neo-1.3B
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 2048)
pos_embed:
  W_pos: (2048, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 2048)'}, 'pos_embed': {'W_pos': '(2048, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 2048)'}"
gpt-neo-2.7B,,[],gpt-neo,gpt-neo-2.7B,2.5B,2516582400,2.7B,2516582400,32,20,2560,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 32
d_model: 2560
n_ctx: 2048
d_head: 128
model_name: gpt-neo-2.7B
n_heads: 20
d_mlp: 10240
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neo-2.7B
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  cjqgTtwwkD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 2516582400
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 2560)
pos_embed:
  W_pos: (2048, 2560)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (2560,)
    ln2:
      '[w, b]': (2560,)
    attn:
      '[W_Q, W_K, W_V]': (20, 2560, 128)
      W_O: (20, 128, 2560)
      '[b_Q, b_K, b_V]': (20, 128)
      b_O: (2560,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (2560, 10240)
      b_in: (10240,)
      W_out: (10240, 2560)
      b_out: (2560,)
ln_final:
  '[w, b]': (2560,)
unembed:
  W_U: (2560, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 2560)'}, 'pos_embed': {'W_pos': '(2048, 2560)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(2560,)'}, 'ln2': {'[w, b]': '(2560,)'}, 'attn': {'[W_Q, W_K, W_V]': '(20, 2560, 128)', 'W_O': '(20, 128, 2560)', '[b_Q, b_K, b_V]': '(20, 128)', 'b_O': '(2560,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(2560, 10240)', 'b_in': '(10240,)', 'W_out': '(10240, 2560)', 'b_out': '(2560,)'}}}, 'ln_final': {'[w, b]': '(2560,)'}, 'unembed': {'W_U': '(2560, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 10240)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2560)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2560)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 2560)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 20, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 20, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 10240)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2560)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 2560)'}"
gpt-j-6B,,[],gpt-j,gpt-j-6B,5.6B,5637144576,6B,5637144576,28,16,4096,50400,gelu_new,rotary,True,GPTJForCausalLM,LN,"n_layers: 28
d_model: 4096
n_ctx: 2048
d_head: 256
model_name: gpt-j-6B
n_heads: 16
d_mlp: 16384
act_fn: gelu_new
d_vocab: 50400
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTJForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-j-6B
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50400
parallel_attn_mlp: true
rotary_dim: 64
n_params: 5637144576
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: true
","embed:
  W_E: (50400, 4096)
blocks:
  '[0-27]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (16, 4096, 256)
      W_O: (16, 256, 4096)
      '[b_Q, b_K, b_V]': (16, 256)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 64)
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50400)
  b_U: (50400,)
","{'embed': {'W_E': '(50400, 4096)'}, 'blocks': {'[0-27]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 4096, 256)', 'W_O': '(16, 256, 4096)', '[b_Q, b_K, b_V]': '(16, 256)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 64)'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50400)', 'b_U': '(50400,)'}}","blocks:
  '[0-27]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 256)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-27]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 256)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
gpt-neox-20b,EleutherAI/gpt-neox-20b,"['gpt-neox-20b', 'gpt-neox', 'neox']",gpt-neo,gpt-neox-20b,20B,19931332608,20b,19931332608,44,64,6144,50432,gelu_fast,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 44
d_model: 6144
n_ctx: 2048
d_head: 96
model_name: gpt-neox-20b
n_heads: 64
d_mlp: 24576
act_fn: gelu_fast
d_vocab: 50432
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  MvA88/3mhD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50432
parallel_attn_mlp: true
rotary_dim: 24
n_params: 19931332608
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50432, 6144)
blocks:
  '[0-43]':
    ln1:
      '[w, b]': (6144,)
    ln2:
      '[w, b]': (6144,)
    attn:
      '[W_Q, W_K, W_V]': (64, 6144, 96)
      W_O: (64, 96, 6144)
      '[b_Q, b_K, b_V]': (64, 96)
      b_O: (6144,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 24)
    mlp:
      W_in: (6144, 24576)
      b_in: (24576,)
      W_out: (24576, 6144)
      b_out: (6144,)
ln_final:
  '[w, b]': (6144,)
unembed:
  W_U: (6144, 50432)
  b_U: (50432,)
","{'embed': {'W_E': '(50432, 6144)'}, 'blocks': {'[0-43]': {'ln1': {'[w, b]': '(6144,)'}, 'ln2': {'[w, b]': '(6144,)'}, 'attn': {'[W_Q, W_K, W_V]': '(64, 6144, 96)', 'W_O': '(64, 96, 6144)', '[b_Q, b_K, b_V]': '(64, 96)', 'b_O': '(6144,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 24)'}, 'mlp': {'W_in': '(6144, 24576)', 'b_in': '(24576,)', 'W_out': '(24576, 6144)', 'b_out': '(6144,)'}}}, 'ln_final': {'[w, b]': '(6144,)'}, 'unembed': {'W_U': '(6144, 50432)', 'b_U': '(50432,)'}}","blocks:
  '[0-43]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6144)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        64, 96)
      '[hook_attn_scores, hook_pattern]': (batch, 64, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6144)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 24576)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      6144)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 6144)
hook_embed: (batch, seq_len, 6144)
","{'blocks': {'[0-43]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 64, 96)', '[hook_attn_scores, hook_pattern]': '(batch, 64, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 24576)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 6144)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'hook_embed': '(batch, seq_len, 6144)'}"
stanford-gpt2-small-a,stanford-crfm/alias-gpt2-small-x21,"['stanford-gpt2-small-a', 'alias-gpt2-small-x21', 'gpt2-mistral-small-a', 'gpt2-stanford-small-a']",gpt2,alias-gpt2-small-x21,85M,84934656,,84934656,12,12,768,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 12
d_model: 768
n_ctx: 1024
d_head: 64
model_name: alias-gpt2-small-x21
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/alias-gpt2-small-x21
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
stanford-gpt2-small-b,stanford-crfm/battlestar-gpt2-small-x49,"['stanford-gpt2-small-b', 'battlestar-gpt2-small-x49', 'gpt2-mistral-small-b', 'gpt2-mistral-small-b']",gpt2,battlestar-gpt2-small-x49,85M,84934656,,84934656,12,12,768,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 12
d_model: 768
n_ctx: 1024
d_head: 64
model_name: battlestar-gpt2-small-x49
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/battlestar-gpt2-small-x49
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
stanford-gpt2-small-c,stanford-crfm/caprica-gpt2-small-x81,"['stanford-gpt2-small-c', 'caprica-gpt2-small-x81', 'gpt2-mistral-small-c', 'gpt2-stanford-small-c']",gpt2,caprica-gpt2-small-x81,85M,84934656,,84934656,12,12,768,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 12
d_model: 768
n_ctx: 1024
d_head: 64
model_name: caprica-gpt2-small-x81
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/caprica-gpt2-small-x81
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
stanford-gpt2-small-d,stanford-crfm/darkmatter-gpt2-small-x343,"['stanford-gpt2-small-d', 'darkmatter-gpt2-small-x343', 'gpt2-mistral-small-d', 'gpt2-mistral-small-d']",gpt2,darkmatter-gpt2-small-x343,85M,84934656,,84934656,12,12,768,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 12
d_model: 768
n_ctx: 1024
d_head: 64
model_name: darkmatter-gpt2-small-x343
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/darkmatter-gpt2-small-x343
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
stanford-gpt2-small-e,stanford-crfm/expanse-gpt2-small-x777,"['stanford-gpt2-small-e', 'expanse-gpt2-small-x777', 'gpt2-mistral-small-e', 'gpt2-mistral-small-e']",gpt2,expanse-gpt2-small-x777,85M,84934656,,84934656,12,12,768,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 12
d_model: 768
n_ctx: 1024
d_head: 64
model_name: expanse-gpt2-small-x777
n_heads: 12
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/expanse-gpt2-small-x777
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
stanford-gpt2-medium-a,stanford-crfm/arwen-gpt2-medium-x21,"['stanford-gpt2-medium-a', 'arwen-gpt2-medium-x21', 'gpt2-medium-small-a', 'gpt2-stanford-medium-a']",gpt2,arwen-gpt2-medium-x21,302M,301989888,,301989888,24,16,1024,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 24
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: arwen-gpt2-medium-x21
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/arwen-gpt2-medium-x21
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
stanford-gpt2-medium-b,stanford-crfm/beren-gpt2-medium-x49,"['stanford-gpt2-medium-b', 'beren-gpt2-medium-x49', 'gpt2-medium-small-b', 'gpt2-stanford-medium-b']",gpt2,beren-gpt2-medium-x49,302M,301989888,,301989888,24,16,1024,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 24
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: beren-gpt2-medium-x49
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/beren-gpt2-medium-x49
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
stanford-gpt2-medium-c,stanford-crfm/celebrimbor-gpt2-medium-x81,"['stanford-gpt2-medium-c', 'celebrimbor-gpt2-medium-x81', 'gpt2-medium-small-c', 'gpt2-medium-small-c']",gpt2,celebrimbor-gpt2-medium-x81,302M,301989888,,301989888,24,16,1024,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 24
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: celebrimbor-gpt2-medium-x81
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/celebrimbor-gpt2-medium-x81
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
stanford-gpt2-medium-d,stanford-crfm/durin-gpt2-medium-x343,"['stanford-gpt2-medium-d', 'durin-gpt2-medium-x343', 'gpt2-medium-small-d', 'gpt2-stanford-medium-d']",gpt2,durin-gpt2-medium-x343,302M,301989888,,301989888,24,16,1024,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 24
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: durin-gpt2-medium-x343
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/durin-gpt2-medium-x343
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
stanford-gpt2-medium-e,stanford-crfm/eowyn-gpt2-medium-x777,"['stanford-gpt2-medium-e', 'eowyn-gpt2-medium-x777', 'gpt2-medium-small-e', 'gpt2-stanford-medium-e']",gpt2,eowyn-gpt2-medium-x777,302M,301989888,,301989888,24,16,1024,50257,gelu_new,standard,False,GPT2LMHeadModel,LN,"n_layers: 24
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: eowyn-gpt2-medium-x777
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stanford-crfm/eowyn-gpt2-medium-x777
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: true
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
pythia-14m,EleutherAI/pythia-14m,['pythia-14m'],pythia,pythia-14m,1.2M,1179648,14m,1179648,6,4,128,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 6
d_model: 128
n_ctx: 2048
d_head: 32
model_name: pythia-14m
n_heads: 4
d_mlp: 512
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-14m
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgasj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 8
n_params: 1179648
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 128)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (128,)
    ln2:
      '[w, b]': (128,)
    attn:
      '[W_Q, W_K, W_V]': (4, 128, 32)
      W_O: (4, 32, 128)
      '[b_Q, b_K, b_V]': (4, 32)
      b_O: (128,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 8)
    mlp:
      W_in: (128, 512)
      b_in: (512,)
      W_out: (512, 128)
      b_out: (128,)
ln_final:
  '[w, b]': (128,)
unembed:
  W_U: (128, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 128)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(128,)'}, 'ln2': {'[w, b]': '(128,)'}, 'attn': {'[W_Q, W_K, W_V]': '(4, 128, 32)', 'W_O': '(4, 32, 128)', '[b_Q, b_K, b_V]': '(4, 32)', 'b_O': '(128,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 8)'}, 'mlp': {'W_in': '(128, 512)', 'b_in': '(512,)', 'W_out': '(512, 128)', 'b_out': '(128,)'}}}, 'ln_final': {'[w, b]': '(128,)'}, 'unembed': {'W_U': '(128, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 128)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        4, 32)
      '[hook_attn_scores, hook_pattern]': (batch, 4, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 128)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 512)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      128)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 128)
hook_embed: (batch, seq_len, 128)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 4, 32)', '[hook_attn_scores, hook_pattern]': '(batch, 4, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 512)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 128)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, 'hook_embed': '(batch, seq_len, 128)'}"
pythia-31m,EleutherAI/pythia-31m,['pythia-31m'],pythia,pythia-31m,4.7M,4718592,31m,4718592,6,8,256,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 6
d_model: 256
n_ctx: 2048
d_head: 32
model_name: pythia-31m
n_heads: 8
d_mlp: 1024
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-31m
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZqT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 8
n_params: 4718592
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 256)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (256,)
    ln2:
      '[w, b]': (256,)
    attn:
      '[W_Q, W_K, W_V]': (8, 256, 32)
      W_O: (8, 32, 256)
      '[b_Q, b_K, b_V]': (8, 32)
      b_O: (256,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 8)
    mlp:
      W_in: (256, 1024)
      b_in: (1024,)
      W_out: (1024, 256)
      b_out: (256,)
ln_final:
  '[w, b]': (256,)
unembed:
  W_U: (256, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 256)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(256,)'}, 'ln2': {'[w, b]': '(256,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 256, 32)', 'W_O': '(8, 32, 256)', '[b_Q, b_K, b_V]': '(8, 32)', 'b_O': '(256,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 8)'}, 'mlp': {'W_in': '(256, 1024)', 'b_in': '(1024,)', 'W_out': '(1024, 256)', 'b_out': '(256,)'}}}, 'ln_final': {'[w, b]': '(256,)'}, 'unembed': {'W_U': '(256, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 256)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 32)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 256)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 1024)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      256)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 256)
hook_embed: (batch, seq_len, 256)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 32)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 1024)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 256)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'hook_embed': '(batch, seq_len, 256)'}"
pythia-70m,EleutherAI/pythia-70m,"['pythia-70m', 'pythia', 'EleutherAI/pythia-19m', 'pythia-19m']",pythia,pythia-70m,19M,18874368,70m,18874368,6,8,512,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 6
d_model: 512
n_ctx: 2048
d_head: 64
model_name: pythia-70m
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-70m
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 18874368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 512)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 512)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
hook_embed: (batch, seq_len, 512)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'hook_embed': '(batch, seq_len, 512)'}"
pythia-160m,EleutherAI/pythia-160m,"['pythia-160m', 'EleutherAI/pythia-125m', 'pythia-125m']",pythia,pythia-160m,85M,84934656,160m,84934656,12,12,768,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: pythia-160m
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-160m
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
hook_embed: (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'hook_embed': '(batch, seq_len, 768)'}"
pythia-410m,EleutherAI/pythia-410m,"['pythia-410m', 'EleutherAI/pythia-350m', 'pythia-350m']",pythia,pythia-410m,302M,301989888,410m,301989888,24,16,1024,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: pythia-410m
n_heads: 16
d_mlp: 4096
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-410m
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
hook_embed: (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'hook_embed': '(batch, seq_len, 1024)'}"
pythia-1b,EleutherAI/pythia-1b,"['pythia-1b', 'EleutherAI/pythia-800m', 'pythia-800m']",pythia,pythia-1b,805M,805306368,1b,805306368,16,8,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 2048
n_ctx: 2048
d_head: 256
model_name: pythia-1b
n_heads: 8
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 64
n_params: 805306368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (8, 2048, 256)
      W_O: (8, 256, 2048)
      '[b_Q, b_K, b_V]': (8, 256)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 64)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 2048, 256)', 'W_O': '(8, 256, 2048)', '[b_Q, b_K, b_V]': '(8, 256)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 64)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 256)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 256)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-1.4b,EleutherAI/pythia-1.4b,"['pythia-1.4b', 'EleutherAI/pythia-1.3b', 'pythia-1.3b']",pythia,pythia-1.4b,1.2B,1207959552,1.4b,1207959552,24,16,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: pythia-1.4b
n_heads: 16
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1.4b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 32
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-2.8b,EleutherAI/pythia-2.8b,"['pythia-2.8b', 'EleutherAI/pythia-2.7b', 'pythia-2.7b']",pythia,pythia-2.8b,2.5B,2516582400,2.8b,2516582400,32,32,2560,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 2560
n_ctx: 2048
d_head: 80
model_name: pythia-2.8b
n_heads: 32
d_mlp: 10240
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-2.8b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  cjqgTtwwkD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 20
n_params: 2516582400
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2560)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (2560,)
    ln2:
      '[w, b]': (2560,)
    attn:
      '[W_Q, W_K, W_V]': (32, 2560, 80)
      W_O: (32, 80, 2560)
      '[b_Q, b_K, b_V]': (32, 80)
      b_O: (2560,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 20)
    mlp:
      W_in: (2560, 10240)
      b_in: (10240,)
      W_out: (10240, 2560)
      b_out: (2560,)
ln_final:
  '[w, b]': (2560,)
unembed:
  W_U: (2560, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2560)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(2560,)'}, 'ln2': {'[w, b]': '(2560,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 2560, 80)', 'W_O': '(32, 80, 2560)', '[b_Q, b_K, b_V]': '(32, 80)', 'b_O': '(2560,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 20)'}, 'mlp': {'W_in': '(2560, 10240)', 'b_in': '(10240,)', 'W_out': '(10240, 2560)', 'b_out': '(2560,)'}}}, 'ln_final': {'[w, b]': '(2560,)'}, 'unembed': {'W_U': '(2560, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 80)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 10240)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2560)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2560)
hook_embed: (batch, seq_len, 2560)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 80)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 10240)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2560)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'hook_embed': '(batch, seq_len, 2560)'}"
pythia-6.9b,EleutherAI/pythia-6.9b,"['pythia-6.9b', 'EleutherAI/pythia-6.7b', 'pythia-6.7b']",pythia,pythia-6.9b,6.4B,6442450944,6.9b,6442450944,32,32,4096,50432,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: pythia-6.9b
n_heads: 32
d_mlp: 16384
act_fn: gelu
d_vocab: 50432
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-6.9b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50432
parallel_attn_mlp: true
rotary_dim: 32
n_params: 6442450944
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50432, 4096)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50432)
  b_U: (50432,)
","{'embed': {'W_E': '(50432, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50432)', 'b_U': '(50432,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
pythia-12b,EleutherAI/pythia-12b,"['pythia-12b', 'EleutherAI/pythia-13b', 'pythia-13b']",pythia,pythia-12b,11B,11324620800,12b,11324620800,36,40,5120,50688,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 36
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: pythia-12b
n_heads: 40
d_mlp: 20480
act_fn: gelu
d_vocab: 50688
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-12b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50688
parallel_attn_mlp: true
rotary_dim: 32
n_params: 11324620800
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50688, 5120)
blocks:
  '[0-35]':
    ln1:
      '[w, b]': (5120,)
    ln2:
      '[w, b]': (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (5120, 20480)
      b_in: (20480,)
      W_out: (20480, 5120)
      b_out: (5120,)
ln_final:
  '[w, b]': (5120,)
unembed:
  W_U: (5120, 50688)
  b_U: (50688,)
","{'embed': {'W_E': '(50688, 5120)'}, 'blocks': {'[0-35]': {'ln1': {'[w, b]': '(5120,)'}, 'ln2': {'[w, b]': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(5120, 20480)', 'b_in': '(20480,)', 'W_out': '(20480, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'[w, b]': '(5120,)'}, 'unembed': {'W_U': '(5120, 50688)', 'b_U': '(50688,)'}}","blocks:
  '[0-35]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 20480)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-35]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 20480)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
pythia-70m-deduped,EleutherAI/pythia-70m-deduped,"['pythia-70m-deduped', 'EleutherAI/pythia-19m-deduped', 'pythia-19m-deduped']",pythia,pythia-70m-deduped,19M,18874368,70m,18874368,6,8,512,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 6
d_model: 512
n_ctx: 2048
d_head: 64
model_name: pythia-70m-deduped
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-70m-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 18874368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 512)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 512)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
hook_embed: (batch, seq_len, 512)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'hook_embed': '(batch, seq_len, 512)'}"
pythia-160m-deduped,EleutherAI/pythia-160m-deduped,"['pythia-160m-deduped', 'EleutherAI/pythia-125m-deduped', 'pythia-125m-deduped']",pythia,pythia-160m-deduped,85M,84934656,160m,84934656,12,12,768,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: pythia-160m-deduped
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-160m-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
hook_embed: (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'hook_embed': '(batch, seq_len, 768)'}"
pythia-410m-deduped,EleutherAI/pythia-410m-deduped,"['pythia-410m-deduped', 'EleutherAI/pythia-350m-deduped', 'pythia-350m-deduped']",pythia,pythia-410m-deduped,302M,301989888,410m,301989888,24,16,1024,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: pythia-410m-deduped
n_heads: 16
d_mlp: 4096
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-410m-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
hook_embed: (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'hook_embed': '(batch, seq_len, 1024)'}"
pythia-1b-deduped,EleutherAI/pythia-1b-deduped,"['pythia-1b-deduped', 'EleutherAI/pythia-800m-deduped', 'pythia-800m-deduped']",pythia,pythia-1b-deduped,805M,805306368,1b,805306368,16,8,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 2048
n_ctx: 2048
d_head: 256
model_name: pythia-1b-deduped
n_heads: 8
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1b-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 64
n_params: 805306368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (8, 2048, 256)
      W_O: (8, 256, 2048)
      '[b_Q, b_K, b_V]': (8, 256)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 64)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 2048, 256)', 'W_O': '(8, 256, 2048)', '[b_Q, b_K, b_V]': '(8, 256)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 64)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 256)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 256)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-1.4b-deduped,EleutherAI/pythia-1.4b-deduped,"['pythia-1.4b-deduped', 'EleutherAI/pythia-1.3b-deduped', 'pythia-1.3b-deduped']",pythia,pythia-1.4b-deduped,1.2B,1207959552,1.4b,1207959552,24,16,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: pythia-1.4b-deduped
n_heads: 16
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1.4b-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 32
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-2.8b-deduped,EleutherAI/pythia-2.8b-deduped,"['pythia-2.8b-deduped', 'EleutherAI/pythia-2.7b-deduped', 'pythia-2.7b-deduped']",pythia,pythia-2.8b-deduped,2.5B,2516582400,2.8b,2516582400,32,32,2560,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 2560
n_ctx: 2048
d_head: 80
model_name: pythia-2.8b-deduped
n_heads: 32
d_mlp: 10240
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-2.8b-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  cjqgTtwwkD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 20
n_params: 2516582400
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2560)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (2560,)
    ln2:
      '[w, b]': (2560,)
    attn:
      '[W_Q, W_K, W_V]': (32, 2560, 80)
      W_O: (32, 80, 2560)
      '[b_Q, b_K, b_V]': (32, 80)
      b_O: (2560,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 20)
    mlp:
      W_in: (2560, 10240)
      b_in: (10240,)
      W_out: (10240, 2560)
      b_out: (2560,)
ln_final:
  '[w, b]': (2560,)
unembed:
  W_U: (2560, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2560)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(2560,)'}, 'ln2': {'[w, b]': '(2560,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 2560, 80)', 'W_O': '(32, 80, 2560)', '[b_Q, b_K, b_V]': '(32, 80)', 'b_O': '(2560,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 20)'}, 'mlp': {'W_in': '(2560, 10240)', 'b_in': '(10240,)', 'W_out': '(10240, 2560)', 'b_out': '(2560,)'}}}, 'ln_final': {'[w, b]': '(2560,)'}, 'unembed': {'W_U': '(2560, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 80)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 10240)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2560)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2560)
hook_embed: (batch, seq_len, 2560)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 80)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 10240)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2560)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'hook_embed': '(batch, seq_len, 2560)'}"
pythia-6.9b-deduped,EleutherAI/pythia-6.9b-deduped,"['pythia-6.9b-deduped', 'EleutherAI/pythia-6.7b-deduped', 'pythia-6.7b-deduped']",pythia,pythia-6.9b-deduped,6.4B,6442450944,6.9b,6442450944,32,32,4096,50432,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: pythia-6.9b-deduped
n_heads: 32
d_mlp: 16384
act_fn: gelu
d_vocab: 50432
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-6.9b-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50432
parallel_attn_mlp: true
rotary_dim: 32
n_params: 6442450944
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50432, 4096)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50432)
  b_U: (50432,)
","{'embed': {'W_E': '(50432, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50432)', 'b_U': '(50432,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
pythia-12b-deduped,EleutherAI/pythia-12b-deduped,"['pythia-12b-deduped', 'EleutherAI/pythia-13b-deduped', 'pythia-13b-deduped']",pythia,pythia-12b-deduped,11B,11324620800,12b,11324620800,36,40,5120,50688,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 36
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: pythia-12b-deduped
n_heads: 40
d_mlp: 20480
act_fn: gelu
d_vocab: 50688
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-12b-deduped
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50688
parallel_attn_mlp: true
rotary_dim: 32
n_params: 11324620800
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50688, 5120)
blocks:
  '[0-35]':
    ln1:
      '[w, b]': (5120,)
    ln2:
      '[w, b]': (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (5120, 20480)
      b_in: (20480,)
      W_out: (20480, 5120)
      b_out: (5120,)
ln_final:
  '[w, b]': (5120,)
unembed:
  W_U: (5120, 50688)
  b_U: (50688,)
","{'embed': {'W_E': '(50688, 5120)'}, 'blocks': {'[0-35]': {'ln1': {'[w, b]': '(5120,)'}, 'ln2': {'[w, b]': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(5120, 20480)', 'b_in': '(20480,)', 'W_out': '(20480, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'[w, b]': '(5120,)'}, 'unembed': {'W_U': '(5120, 50688)', 'b_U': '(50688,)'}}","blocks:
  '[0-35]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 20480)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-35]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 20480)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
pythia-70m-v0,EleutherAI/pythia-70m-v0,"['pythia-70m-v0', 'pythia-v0', 'EleutherAI/pythia-19m-v0', 'pythia-19m-v0']",pythia,pythia-70m-v0,19M,18874368,70m,18874368,6,8,512,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 6
d_model: 512
n_ctx: 2048
d_head: 64
model_name: pythia-70m-v0
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-70m-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 18874368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 512)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 512)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
hook_embed: (batch, seq_len, 512)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'hook_embed': '(batch, seq_len, 512)'}"
pythia-160m-v0,EleutherAI/pythia-160m-v0,"['pythia-160m-v0', 'EleutherAI/pythia-125m-v0', 'pythia-125m-v0']",pythia,pythia-160m-v0,85M,84934656,160m,84934656,12,12,768,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: pythia-160m-v0
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-160m-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
hook_embed: (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'hook_embed': '(batch, seq_len, 768)'}"
pythia-410m-v0,EleutherAI/pythia-410m-v0,"['pythia-410m-v0', 'EleutherAI/pythia-350m-v0', 'pythia-350m-v0']",pythia,pythia-410m-v0,302M,301989888,410m,301989888,24,16,1024,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: pythia-410m-v0
n_heads: 16
d_mlp: 4096
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-410m-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
hook_embed: (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'hook_embed': '(batch, seq_len, 1024)'}"
pythia-1b-v0,EleutherAI/pythia-1b-v0,"['pythia-1b-v0', 'EleutherAI/pythia-800m-v0', 'pythia-800m-v0']",pythia,pythia-1b-v0,805M,805306368,1b,805306368,16,8,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 2048
n_ctx: 2048
d_head: 256
model_name: pythia-1b-v0
n_heads: 8
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1b-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 64
n_params: 805306368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (8, 2048, 256)
      W_O: (8, 256, 2048)
      '[b_Q, b_K, b_V]': (8, 256)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 64)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 2048, 256)', 'W_O': '(8, 256, 2048)', '[b_Q, b_K, b_V]': '(8, 256)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 64)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 256)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 256)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-1.4b-v0,EleutherAI/pythia-1.4b-v0,"['pythia-1.4b-v0', 'EleutherAI/pythia-1.3b-v0', 'pythia-1.3b-v0']",pythia,pythia-1.4b-v0,1.2B,1207959552,1.4b,1207959552,24,16,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: pythia-1.4b-v0
n_heads: 16
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1.4b-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 32
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-2.8b-v0,EleutherAI/pythia-2.8b-v0,"['pythia-2.8b-v0', 'EleutherAI/pythia-2.7b-v0', 'pythia-2.7b-v0']",pythia,pythia-2.8b-v0,2.5B,2516582400,2.8b,2516582400,32,32,2560,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 2560
n_ctx: 2048
d_head: 80
model_name: pythia-2.8b-v0
n_heads: 32
d_mlp: 10240
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-2.8b-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  cjqgTtwwkD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 20
n_params: 2516582400
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2560)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (2560,)
    ln2:
      '[w, b]': (2560,)
    attn:
      '[W_Q, W_K, W_V]': (32, 2560, 80)
      W_O: (32, 80, 2560)
      '[b_Q, b_K, b_V]': (32, 80)
      b_O: (2560,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 20)
    mlp:
      W_in: (2560, 10240)
      b_in: (10240,)
      W_out: (10240, 2560)
      b_out: (2560,)
ln_final:
  '[w, b]': (2560,)
unembed:
  W_U: (2560, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2560)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(2560,)'}, 'ln2': {'[w, b]': '(2560,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 2560, 80)', 'W_O': '(32, 80, 2560)', '[b_Q, b_K, b_V]': '(32, 80)', 'b_O': '(2560,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 20)'}, 'mlp': {'W_in': '(2560, 10240)', 'b_in': '(10240,)', 'W_out': '(10240, 2560)', 'b_out': '(2560,)'}}}, 'ln_final': {'[w, b]': '(2560,)'}, 'unembed': {'W_U': '(2560, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 80)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 10240)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2560)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2560)
hook_embed: (batch, seq_len, 2560)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 80)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 10240)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2560)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'hook_embed': '(batch, seq_len, 2560)'}"
pythia-6.9b-v0,EleutherAI/pythia-6.9b-v0,"['pythia-6.9b-v0', 'EleutherAI/pythia-6.7b-v0', 'pythia-6.7b-v0']",pythia,pythia-6.9b-v0,6.4B,6442450944,6.9b,6442450944,32,32,4096,50432,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: pythia-6.9b-v0
n_heads: 32
d_mlp: 16384
act_fn: gelu
d_vocab: 50432
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-6.9b-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50432
parallel_attn_mlp: true
rotary_dim: 32
n_params: 6442450944
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50432, 4096)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50432)
  b_U: (50432,)
","{'embed': {'W_E': '(50432, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50432)', 'b_U': '(50432,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
pythia-12b-v0,EleutherAI/pythia-12b-v0,"['pythia-12b-v0', 'EleutherAI/pythia-13b-v0', 'pythia-13b-v0']",pythia,pythia-12b-v0,11B,11324620800,12b,11324620800,36,40,5120,50688,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 36
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: pythia-12b-v0
n_heads: 40
d_mlp: 20480
act_fn: gelu
d_vocab: 50688
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-12b-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50688
parallel_attn_mlp: true
rotary_dim: 32
n_params: 11324620800
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50688, 5120)
blocks:
  '[0-35]':
    ln1:
      '[w, b]': (5120,)
    ln2:
      '[w, b]': (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (5120, 20480)
      b_in: (20480,)
      W_out: (20480, 5120)
      b_out: (5120,)
ln_final:
  '[w, b]': (5120,)
unembed:
  W_U: (5120, 50688)
  b_U: (50688,)
","{'embed': {'W_E': '(50688, 5120)'}, 'blocks': {'[0-35]': {'ln1': {'[w, b]': '(5120,)'}, 'ln2': {'[w, b]': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(5120, 20480)', 'b_in': '(20480,)', 'W_out': '(20480, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'[w, b]': '(5120,)'}, 'unembed': {'W_U': '(5120, 50688)', 'b_U': '(50688,)'}}","blocks:
  '[0-35]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 20480)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-35]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 20480)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
pythia-70m-deduped-v0,EleutherAI/pythia-70m-deduped-v0,"['pythia-70m-deduped-v0', 'EleutherAI/pythia-19m-deduped-v0', 'pythia-19m-deduped-v0']",pythia,pythia-70m-deduped-v0,19M,18874368,70m,18874368,6,8,512,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 6
d_model: 512
n_ctx: 2048
d_head: 64
model_name: pythia-70m-deduped-v0
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-70m-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 18874368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 512)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 512)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
hook_embed: (batch, seq_len, 512)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'hook_embed': '(batch, seq_len, 512)'}"
pythia-160m-deduped-v0,EleutherAI/pythia-160m-deduped-v0,"['pythia-160m-deduped-v0', 'EleutherAI/pythia-125m-deduped-v0', 'pythia-125m-deduped-v0']",pythia,pythia-160m-deduped-v0,85M,84934656,160m,84934656,12,12,768,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: pythia-160m-deduped-v0
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-160m-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
hook_embed: (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'hook_embed': '(batch, seq_len, 768)'}"
pythia-410m-deduped-v0,EleutherAI/pythia-410m-deduped-v0,"['pythia-410m-deduped-v0', 'EleutherAI/pythia-350m-deduped-v0', 'pythia-350m-deduped-v0']",pythia,pythia-410m-deduped-v0,302M,301989888,410m,301989888,24,16,1024,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: pythia-410m-deduped-v0
n_heads: 16
d_mlp: 4096
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-410m-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
hook_embed: (batch, seq_len, 1024)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'hook_embed': '(batch, seq_len, 1024)'}"
pythia-1b-deduped-v0,EleutherAI/pythia-1b-deduped-v0,"['pythia-1b-deduped-v0', 'EleutherAI/pythia-800m-deduped-v0', 'pythia-800m-deduped-v0']",pythia,pythia-1b-deduped-v0,805M,805306368,1b,805306368,16,8,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 2048
n_ctx: 2048
d_head: 256
model_name: pythia-1b-deduped-v0
n_heads: 8
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1b-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 64
n_params: 805306368
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (8, 2048, 256)
      W_O: (8, 256, 2048)
      '[b_Q, b_K, b_V]': (8, 256)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 64)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 2048, 256)', 'W_O': '(8, 256, 2048)', '[b_Q, b_K, b_V]': '(8, 256)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 64)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        8, 256)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 8, 256)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-1.4b-deduped-v0,EleutherAI/pythia-1.4b-deduped-v0,"['pythia-1.4b-deduped-v0', 'EleutherAI/pythia-1.3b-deduped-v0', 'pythia-1.3b-deduped-v0']",pythia,pythia-1.4b-deduped-v0,1.2B,1207959552,1.4b,1207959552,24,16,2048,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: pythia-1.4b-deduped-v0
n_heads: 16
d_mlp: 8192
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-1.4b-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 32
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
pythia-2.8b-deduped-v0,EleutherAI/pythia-2.8b-deduped-v0,"['pythia-2.8b-deduped-v0', 'EleutherAI/pythia-2.7b-deduped-v0', 'pythia-2.7b-deduped-v0']",pythia,pythia-2.8b-deduped-v0,2.5B,2516582400,2.8b,2516582400,32,32,2560,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 2560
n_ctx: 2048
d_head: 80
model_name: pythia-2.8b-deduped-v0
n_heads: 32
d_mlp: 10240
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-2.8b-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  cjqgTtwwkD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 20
n_params: 2516582400
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 2560)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (2560,)
    ln2:
      '[w, b]': (2560,)
    attn:
      '[W_Q, W_K, W_V]': (32, 2560, 80)
      W_O: (32, 80, 2560)
      '[b_Q, b_K, b_V]': (32, 80)
      b_O: (2560,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 20)
    mlp:
      W_in: (2560, 10240)
      b_in: (10240,)
      W_out: (10240, 2560)
      b_out: (2560,)
ln_final:
  '[w, b]': (2560,)
unembed:
  W_U: (2560, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 2560)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(2560,)'}, 'ln2': {'[w, b]': '(2560,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 2560, 80)', 'W_O': '(32, 80, 2560)', '[b_Q, b_K, b_V]': '(32, 80)', 'b_O': '(2560,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 20)'}, 'mlp': {'W_in': '(2560, 10240)', 'b_in': '(10240,)', 'W_out': '(10240, 2560)', 'b_out': '(2560,)'}}}, 'ln_final': {'[w, b]': '(2560,)'}, 'unembed': {'W_U': '(2560, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 80)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 10240)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      2560)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2560)
hook_embed: (batch, seq_len, 2560)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 80)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 10240)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2560)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'hook_embed': '(batch, seq_len, 2560)'}"
pythia-6.9b-deduped-v0,EleutherAI/pythia-6.9b-deduped-v0,"['pythia-6.9b-deduped-v0', 'EleutherAI/pythia-6.7b-deduped-v0', 'pythia-6.7b-deduped-v0']",pythia,pythia-6.9b-deduped-v0,6.4B,6442450944,6.9b,6442450944,32,32,4096,50432,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: pythia-6.9b-deduped-v0
n_heads: 32
d_mlp: 16384
act_fn: gelu
d_vocab: 50432
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-6.9b-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50432
parallel_attn_mlp: true
rotary_dim: 32
n_params: 6442450944
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50432, 4096)
blocks:
  '[0-31]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50432)
  b_U: (50432,)
","{'embed': {'W_E': '(50432, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50432)', 'b_U': '(50432,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
pythia-12b-deduped-v0,EleutherAI/pythia-12b-deduped-v0,"['pythia-12b-deduped-v0', 'EleutherAI/pythia-13b-deduped-v0', 'pythia-13b-deduped-v0']",pythia,pythia-12b-deduped-v0,11B,11324620800,12b,11324620800,36,40,5120,50688,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 36
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: pythia-12b-deduped-v0
n_heads: 40
d_mlp: 20480
act_fn: gelu
d_vocab: 50688
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-12b-deduped-v0
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50688
parallel_attn_mlp: true
rotary_dim: 32
n_params: 11324620800
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50688, 5120)
blocks:
  '[0-35]':
    ln1:
      '[w, b]': (5120,)
    ln2:
      '[w, b]': (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 32)
    mlp:
      W_in: (5120, 20480)
      b_in: (20480,)
      W_out: (20480, 5120)
      b_out: (5120,)
ln_final:
  '[w, b]': (5120,)
unembed:
  W_U: (5120, 50688)
  b_U: (50688,)
","{'embed': {'W_E': '(50688, 5120)'}, 'blocks': {'[0-35]': {'ln1': {'[w, b]': '(5120,)'}, 'ln2': {'[w, b]': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 32)'}, 'mlp': {'W_in': '(5120, 20480)', 'b_in': '(20480,)', 'W_out': '(20480, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'[w, b]': '(5120,)'}, 'unembed': {'W_U': '(5120, 50688)', 'b_U': '(50688,)'}}","blocks:
  '[0-35]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 20480)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-35]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 20480)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
pythia-160m-seed1,EleutherAI/pythia-160m-seed1,"['pythia-160m-seed1', 'EleutherAI/pythia-125m-seed1', 'pythia-125m-seed1']",pythia,pythia-160m-seed1,85M,84934656,160m,84934656,12,12,768,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: pythia-160m-seed1
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-160m-seed1
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
hook_embed: (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'hook_embed': '(batch, seq_len, 768)'}"
pythia-160m-seed2,EleutherAI/pythia-160m-seed2,"['pythia-160m-seed2', 'EleutherAI/pythia-125m-seed2', 'pythia-125m-seed2']",pythia,pythia-160m-seed2,85M,84934656,160m,84934656,12,12,768,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: pythia-160m-seed2
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-160m-seed2
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
hook_embed: (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'hook_embed': '(batch, seq_len, 768)'}"
pythia-160m-seed3,EleutherAI/pythia-160m-seed3,"['pythia-160m-seed3', 'EleutherAI/pythia-125m-seed3', 'pythia-125m-seed3']",pythia,pythia-160m-seed3,85M,84934656,160m,84934656,12,12,768,50304,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 12
d_model: 768
n_ctx: 2048
d_head: 64
model_name: pythia-160m-seed3
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 50304
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/pythia-160m-seed3
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50304
parallel_attn_mlp: true
rotary_dim: 16
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50304, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 16)
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50304)
  b_U: (50304,)
","{'embed': {'W_E': '(50304, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 16)'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50304)', 'b_U': '(50304,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
hook_embed: (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'hook_embed': '(batch, seq_len, 768)'}"
solu-1l-pile,NeelNanda/SoLU_1L_v9_old,"['solu-1l-pile', 'solu-1l-old']",solu,SoLU_1L_v9_old,13M,12582912,,12582912,1,16,1024,50278,solu_ln,standard,False,neel-solu-old,LN,"n_layers: 1
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: SoLU_1L_v9_old
n_heads: 16
d_mlp: 4096
act_fn: solu_ln
d_vocab: 50278
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel-solu-old
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: true
d_vocab_out: 50278
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50278, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '0':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (4096,)
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  w: (1024,)
unembed:
  W_U: (1024, 50278)
  b_U: (50278,)
","{'embed': {'W_E': '(50278, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'0': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(4096,)'}, 'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'w': '(1024,)'}, 'unembed': {'W_U': '(1024, 50278)', 'b_U': '(50278,)'}}","blocks:
  '0':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 4096)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'0': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
solu-2l-pile,NeelNanda/SoLU_2L_v10_old,"['solu-2l-pile', 'solu-2l-old']",solu,SoLU_2L_v10_old,13M,12812288,,12812288,2,11,736,50278,solu_ln,standard,False,neel-solu-old,LNPre,"n_layers: 2
d_model: 736
n_ctx: 1024
d_head: 64
model_name: SoLU_2L_v10_old
n_heads: 11
d_mlp: 2944
act_fn: solu_ln
d_vocab: 50278
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel-solu-old
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LNPre
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  l3rLbzQynj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: true
d_vocab_out: 50278
parallel_attn_mlp: false
rotary_dim: null
n_params: 12812288
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50278, 736)
pos_embed:
  W_pos: (1024, 736)
blocks:
  '[0-1]':
    attn:
      '[W_Q, W_K, W_V]': (11, 736, 64)
      W_O: (11, 64, 736)
      '[b_Q, b_K, b_V]': (11, 64)
      b_O: (736,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (736, 2944)
      b_in: (2944,)
      W_out: (2944, 736)
      b_out: (736,)
unembed:
  W_U: (736, 50278)
  b_U: (50278,)
","{'embed': {'W_E': '(50278, 736)'}, 'pos_embed': {'W_pos': '(1024, 736)'}, 'blocks': {'[0-1]': {'attn': {'[W_Q, W_K, W_V]': '(11, 736, 64)', 'W_O': '(11, 64, 736)', '[b_Q, b_K, b_V]': '(11, 64)', 'b_O': '(736,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(736, 2944)', 'b_in': '(2944,)', 'W_out': '(2944, 736)', 'b_out': '(736,)'}}}, 'unembed': {'W_U': '(736, 50278)', 'b_U': '(50278,)'}}","blocks:
  '[0-1]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 736)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 11, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 11, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 736)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2944)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2944)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 736)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 736)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 736)
","{'blocks': {'[0-1]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 736)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 11, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 11, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 736)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2944)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2944)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 736)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 736)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 736)'}"
solu-4l-pile,NeelNanda/SoLU_4L_v11_old,"['solu-4l-pile', 'solu-4l-old']",solu,SoLU_4L_v11_old,13M,12582912,,12582912,4,8,512,50278,solu_ln,standard,False,neel-solu-old,LNPre,"n_layers: 4
d_model: 512
n_ctx: 1024
d_head: 64
model_name: SoLU_4L_v11_old
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 50278
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel-solu-old
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LNPre
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: true
d_vocab_out: 50278
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50278, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-3]':
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
unembed:
  W_U: (512, 50278)
  b_U: (50278,)
","{'embed': {'W_E': '(50278, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-3]': {'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'unembed': {'W_U': '(512, 50278)', 'b_U': '(50278,)'}}","blocks:
  '[0-3]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2048)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-3]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
solu-6l-pile,NeelNanda/SoLU_6L_v13_old,"['solu-6l-pile', 'solu-6l-old']",solu,SoLU_6L_v13_old,42M,42467328,,42467328,6,12,768,50278,solu_ln,standard,False,neel-solu-old,LNPre,"n_layers: 6
d_model: 768
n_ctx: 1024
d_head: 64
model_name: SoLU_6L_v13_old
n_heads: 12
d_mlp: 3072
act_fn: solu_ln
d_vocab: 50278
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel-solu-old
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LNPre
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: true
d_vocab_out: 50278
parallel_attn_mlp: false
rotary_dim: null
n_params: 42467328
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50278, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-5]':
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
unembed:
  W_U: (768, 50278)
  b_U: (50278,)
","{'embed': {'W_E': '(50278, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-5]': {'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'unembed': {'W_U': '(768, 50278)', 'b_U': '(50278,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 3072)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 3072)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
solu-8l-pile,NeelNanda/SoLU_8L_v21_old,"['solu-8l-pile', 'solu-8l-old']",solu,SoLU_8L_v21_old,101M,100663296,,100663296,8,16,1024,50278,solu_ln,standard,False,neel-solu-old,LNPre,"n_layers: 8
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: SoLU_8L_v21_old
n_heads: 16
d_mlp: 4096
act_fn: solu_ln
d_vocab: 50278
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel-solu-old
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LNPre
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50278
parallel_attn_mlp: false
rotary_dim: null
n_params: 100663296
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50278, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-7]':
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
unembed:
  W_U: (1024, 50278)
  b_U: (50278,)
","{'embed': {'W_E': '(50278, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-7]': {'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'unembed': {'W_U': '(1024, 50278)', 'b_U': '(50278,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 4096)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
solu-10l-pile,NeelNanda/SoLU_10L_v22_old,"['solu-10l-pile', 'solu-10l-old']",solu,SoLU_10L_v22_old,197M,196608000,,196608000,10,20,1280,50278,solu_ln,standard,False,neel-solu-old,LNPre,"n_layers: 10
d_model: 1280
n_ctx: 1024
d_head: 64
model_name: SoLU_10L_v22_old
n_heads: 20
d_mlp: 5120
act_fn: solu_ln
d_vocab: 50278
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel-solu-old
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LNPre
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bfllj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50278
parallel_attn_mlp: false
rotary_dim: null
n_params: 196608000
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50278, 1280)
pos_embed:
  W_pos: (1024, 1280)
blocks:
  '[0-9]':
    attn:
      '[W_Q, W_K, W_V]': (20, 1280, 64)
      W_O: (20, 64, 1280)
      '[b_Q, b_K, b_V]': (20, 64)
      b_O: (1280,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (1280, 5120)
      b_in: (5120,)
      W_out: (5120, 1280)
      b_out: (1280,)
unembed:
  W_U: (1280, 50278)
  b_U: (50278,)
","{'embed': {'W_E': '(50278, 1280)'}, 'pos_embed': {'W_pos': '(1024, 1280)'}, 'blocks': {'[0-9]': {'attn': {'[W_Q, W_K, W_V]': '(20, 1280, 64)', 'W_O': '(20, 64, 1280)', '[b_Q, b_K, b_V]': '(20, 64)', 'b_O': '(1280,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1280, 5120)', 'b_in': '(5120,)', 'W_out': '(5120, 1280)', 'b_out': '(1280,)'}}}, 'unembed': {'W_U': '(1280, 50278)', 'b_U': '(50278,)'}}","blocks:
  '[0-9]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1280)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1280)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 5120)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 5120)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1280)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1280)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)
","{'blocks': {'[0-9]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 20, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 20, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 5120)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1280)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1280)'}"
solu-12l-pile,NeelNanda/SoLU_12L_v23_old,"['solu-12l-pile', 'solu-12l-old']",solu,SoLU_12L_v23_old,340M,339738624,,339738624,12,24,1536,50278,solu_ln,standard,False,neel-solu-old,LN,"n_layers: 12
d_model: 1536
n_ctx: 1024
d_head: 64
model_name: SoLU_12L_v23_old
n_heads: 24
d_mlp: 6144
act_fn: solu_ln
d_vocab: 50278
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel-solu-old
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  MvA88/3mlD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50278
parallel_attn_mlp: false
rotary_dim: null
n_params: 339738624
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50278, 1536)
pos_embed:
  W_pos: (1024, 1536)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (1536,)
    ln2:
      '[w, b]': (1536,)
    attn:
      '[W_Q, W_K, W_V]': (24, 1536, 64)
      W_O: (24, 64, 1536)
      '[b_Q, b_K, b_V]': (24, 64)
      b_O: (1536,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (6144,)
      W_in: (1536, 6144)
      b_in: (6144,)
      W_out: (6144, 1536)
      b_out: (1536,)
ln_final:
  '[w, b]': (1536,)
unembed:
  W_U: (1536, 50278)
  b_U: (50278,)
","{'embed': {'W_E': '(50278, 1536)'}, 'pos_embed': {'W_pos': '(1024, 1536)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(1536,)'}, 'ln2': {'[w, b]': '(1536,)'}, 'attn': {'[W_Q, W_K, W_V]': '(24, 1536, 64)', 'W_O': '(24, 64, 1536)', '[b_Q, b_K, b_V]': '(24, 64)', 'b_O': '(1536,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(6144,)'}, 'W_in': '(1536, 6144)', 'b_in': '(6144,)', 'W_out': '(6144, 1536)', 'b_out': '(1536,)'}}}, 'ln_final': {'[w, b]': '(1536,)'}, 'unembed': {'W_U': '(1536, 50278)', 'b_U': '(50278,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1536)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 24, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 24, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1536)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 6144)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 6144)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1536)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1536)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1536)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 24, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 24, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 6144)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1536)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1536)'}"
solu-1l,NeelNanda/SoLU_1L512W_C4_Code,"['solu-1l', 'solu-1l-new', 'solu-1l-c4-code']",solu,SoLU_1L512W_C4_Code,3.1M,3145728,,3145728,1,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 1
d_model: 512
n_ctx: 1024
d_head: 64
model_name: SoLU_1L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 3145728
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '0':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (2048,)
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'0': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(2048,)'}, 'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '0':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2048)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'0': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
solu-2l,NeelNanda/SoLU_2L512W_C4_Code,"['solu-2l', 'solu-2l-new', 'solu-2l-c4-code']",solu,SoLU_2L512W_C4_Code,6.3M,6291456,,6291456,2,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 2
d_model: 512
n_ctx: 1024
d_head: 64
model_name: SoLU_2L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 6291456
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-1]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (2048,)
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-1]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(2048,)'}, 'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-1]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2048)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-1]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
solu-3l,NeelNanda/SoLU_3L512W_C4_Code,"['solu-3l', 'solu-3l-new', 'solu-3l-c4-code']",solu,SoLU_3L512W_C4_Code,9.4M,9437184,,9437184,3,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 3
d_model: 512
n_ctx: 1024
d_head: 64
model_name: SoLU_3L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 9437184
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-2]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (2048,)
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-2]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(2048,)'}, 'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-2]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2048)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-2]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
solu-4l,NeelNanda/SoLU_4L512W_C4_Code,"['solu-4l', 'solu-4l-new', 'solu-4l-c4-code']",solu,SoLU_4L512W_C4_Code,13M,12582912,,12582912,4,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 4
d_model: 512
n_ctx: 1024
d_head: 64
model_name: SoLU_4L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-3]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (2048,)
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-3]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(2048,)'}, 'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-3]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2048)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-3]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
solu-6l,NeelNanda/SoLU_6L768W_C4_Code,"['solu-6l', 'solu-6l-new', 'solu-6l-c4-code']",solu,SoLU_6L768W_C4_Code,42M,42467328,,42467328,6,12,768,48262,solu_ln,standard,False,neel,LN,"n_layers: 6
d_model: 768
n_ctx: 1024
d_head: 64
model_name: SoLU_6L768W_C4_Code
n_heads: 12
d_mlp: 3072
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 42467328
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 768)
pos_embed:
  W_pos: (1024, 768)
blocks:
  '[0-5]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (3072,)
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 768)'}, 'pos_embed': {'W_pos': '(1024, 768)'}, 'blocks': {'[0-5]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(3072,)'}, 'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-5]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 3072)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-5]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 3072)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
solu-8l,NeelNanda/SoLU_8L1024W_C4_Code,"['solu-8l', 'solu-8l-new', 'solu-8l-c4-code']",solu,SoLU_8L1024W_C4_Code,101M,100663296,,100663296,8,16,1024,48262,solu_ln,standard,False,neel,LN,"n_layers: 8
d_model: 1024
n_ctx: 1024
d_head: 64
model_name: SoLU_8L1024W_C4_Code
n_heads: 16
d_mlp: 4096
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 100663296
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 1024)
pos_embed:
  W_pos: (1024, 1024)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (4096,)
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 1024)'}, 'pos_embed': {'W_pos': '(1024, 1024)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(4096,)'}, 'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 4096)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
solu-10l,NeelNanda/SoLU_10L1280W_C4_Code,"['solu-10l', 'solu-10l-new', 'solu-10l-c4-code']",solu,SoLU_10L1280W_C4_Code,197M,196608000,,196608000,10,20,1280,48262,solu_ln,standard,False,neel,LN,"n_layers: 10
d_model: 1280
n_ctx: 1024
d_head: 64
model_name: SoLU_10L1280W_C4_Code
n_heads: 20
d_mlp: 5120
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bfllj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 196608000
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 1280)
pos_embed:
  W_pos: (1024, 1280)
blocks:
  '[0-9]':
    ln1:
      '[w, b]': (1280,)
    ln2:
      '[w, b]': (1280,)
    attn:
      '[W_Q, W_K, W_V]': (20, 1280, 64)
      W_O: (20, 64, 1280)
      '[b_Q, b_K, b_V]': (20, 64)
      b_O: (1280,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (5120,)
      W_in: (1280, 5120)
      b_in: (5120,)
      W_out: (5120, 1280)
      b_out: (1280,)
ln_final:
  '[w, b]': (1280,)
unembed:
  W_U: (1280, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 1280)'}, 'pos_embed': {'W_pos': '(1024, 1280)'}, 'blocks': {'[0-9]': {'ln1': {'[w, b]': '(1280,)'}, 'ln2': {'[w, b]': '(1280,)'}, 'attn': {'[W_Q, W_K, W_V]': '(20, 1280, 64)', 'W_O': '(20, 64, 1280)', '[b_Q, b_K, b_V]': '(20, 64)', 'b_O': '(1280,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(5120,)'}, 'W_in': '(1280, 5120)', 'b_in': '(5120,)', 'W_out': '(5120, 1280)', 'b_out': '(1280,)'}}}, 'ln_final': {'[w, b]': '(1280,)'}, 'unembed': {'W_U': '(1280, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-9]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1280)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 20, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 20, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1280)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 5120)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 5120)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1280)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1280)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1280)
","{'blocks': {'[0-9]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 20, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 20, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 5120)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1280)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1280)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1280)'}"
solu-12l,NeelNanda/SoLU_12L1536W_C4_Code,"['solu-12l', 'solu-12l-new', 'solu-12l-c4-code']",solu,SoLU_12L1536W_C4_Code,340M,339738624,,339738624,12,24,1536,48262,solu_ln,standard,False,neel,LN,"n_layers: 12
d_model: 1536
n_ctx: 1024
d_head: 64
model_name: SoLU_12L1536W_C4_Code
n_heads: 24
d_mlp: 6144
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  MvA88/3mlD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 339738624
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 1536)
pos_embed:
  W_pos: (1024, 1536)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (1536,)
    ln2:
      '[w, b]': (1536,)
    attn:
      '[W_Q, W_K, W_V]': (24, 1536, 64)
      W_O: (24, 64, 1536)
      '[b_Q, b_K, b_V]': (24, 64)
      b_O: (1536,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (6144,)
      W_in: (1536, 6144)
      b_in: (6144,)
      W_out: (6144, 1536)
      b_out: (1536,)
ln_final:
  '[w, b]': (1536,)
unembed:
  W_U: (1536, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 1536)'}, 'pos_embed': {'W_pos': '(1024, 1536)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(1536,)'}, 'ln2': {'[w, b]': '(1536,)'}, 'attn': {'[W_Q, W_K, W_V]': '(24, 1536, 64)', 'W_O': '(24, 64, 1536)', '[b_Q, b_K, b_V]': '(24, 64)', 'b_O': '(1536,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(6144,)'}, 'W_in': '(1536, 6144)', 'b_in': '(6144,)', 'W_out': '(6144, 1536)', 'b_out': '(1536,)'}}}, 'ln_final': {'[w, b]': '(1536,)'}, 'unembed': {'W_U': '(1536, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1536)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 24, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 24, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1536)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 6144)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 6144)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1536)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1536)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1536)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 24, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 24, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 6144)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1536)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1536)'}"
gelu-1l,NeelNanda/GELU_1L512W_C4_Code,"['gelu-1l', 'gelu-1l-new', 'gelu-1l-c4-code']",gelu,GELU_1L512W_C4_Code,3.1M,3145728,,3145728,1,8,512,48262,gelu,standard,False,neel,LN,"n_layers: 1
d_model: 512
n_ctx: 1024
d_head: 64
model_name: GELU_1L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 3145728
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '0':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'0': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '0':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'0': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
gelu-2l,NeelNanda/GELU_2L512W_C4_Code,"['gelu-2l', 'gelu-2l-new', 'gelu-2l-c4-code']",gelu,GELU_2L512W_C4_Code,6.3M,6291456,,6291456,2,8,512,48262,gelu,standard,False,neel,LN,"n_layers: 2
d_model: 512
n_ctx: 1024
d_head: 64
model_name: GELU_2L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 6291456
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-1]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-1]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-1]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-1]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
gelu-3l,NeelNanda/GELU_3L512W_C4_Code,"['gelu-3l', 'gelu-3l-new', 'gelu-3l-c4-code']",gelu,GELU_3L512W_C4_Code,9.4M,9437184,,9437184,3,8,512,48262,gelu,standard,False,neel,LN,"n_layers: 3
d_model: 512
n_ctx: 1024
d_head: 64
model_name: GELU_3L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 9437184
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-2]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-2]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-2]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-2]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
gelu-4l,NeelNanda/GELU_4L512W_C4_Code,"['gelu-4l', 'gelu-4l-new', 'gelu-4l-c4-code']",gelu,GELU_4L512W_C4_Code,13M,12582912,,12582912,4,8,512,48262,gelu,standard,False,neel,LN,"n_layers: 4
d_model: 512
n_ctx: 1024
d_head: 64
model_name: GELU_4L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-3]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-3]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-3]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-3]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
attn-only-1l,NeelNanda/Attn_Only_1L512W_C4_Code,"['attn-only-1l', 'attn-only-1l-new', 'attn-only-1l-c4-code']",attn-only,Attn_Only_1L512W_C4_Code,1.0M,1048576,,1048576,1,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 1
d_model: 512
n_ctx: 1024
d_head: 64
model_name: Attn_Only_1L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: true
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 1048576
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '0':
    ln1:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'0': {'ln1': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '0':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'0': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, '[hook_resid_pre, hook_attn_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
attn-only-2l,NeelNanda/Attn_Only_2L512W_C4_Code,"['attn-only-2l', 'attn-only-2l-new', 'attn-only-2l-c4-code']",attn-only,Attn_Only_2L512W_C4_Code,2.1M,2097152,,2097152,2,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 2
d_model: 512
n_ctx: 1024
d_head: 64
model_name: Attn_Only_2L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: true
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 2097152
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-1]':
    ln1:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-1]': {'ln1': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-1]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-1]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, '[hook_resid_pre, hook_attn_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
attn-only-3l,NeelNanda/Attn_Only_3L512W_C4_Code,"['attn-only-3l', 'attn-only-3l-new', 'attn-only-3l-c4-code']",attn-only,Attn_Only_3L512W_C4_Code,3.1M,3145728,,3145728,3,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 3
d_model: 512
n_ctx: 1024
d_head: 64
model_name: Attn_Only_3L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: true
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 3145728
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-2]':
    ln1:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-2]': {'ln1': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-2]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-2]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, '[hook_resid_pre, hook_attn_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
attn-only-4l,NeelNanda/Attn_Only_4L512W_C4_Code,"['attn-only-4l', 'attn-only-4l-new', 'attn-only-4l-c4-code']",attn-only,Attn_Only_4L512W_C4_Code,4.2M,4194304,,4194304,4,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 4
d_model: 512
n_ctx: 1024
d_head: 64
model_name: Attn_Only_4L512W_C4_Code
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: true
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 4194304
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-3]':
    ln1:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-3]': {'ln1': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-3]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-3]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, '[hook_resid_pre, hook_attn_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
attn-only-2l-demo,NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr,"['attn-only-2l-demo', 'attn-only-2l-shortformer-6b-big-lr', 'attn-only-2l-induction-demo', 'attn-only-demo']",attn-only,Attn-Only-2L512W-Shortformer-6B-big-lr,2.1M,2097152,,2097152,2,8,512,50277,solu_ln,shortformer,False,neel,,"n_layers: 2
d_model: 512
n_ctx: 1024
d_head: 64
model_name: Attn-Only-2L512W-Shortformer-6B-big-lr
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 50277
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: EleutherAI/gpt-neox-20b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: null
device: cuda
n_devices: 1
attention_dir: causal
attn_only: true
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: shortformer
final_rms: false
d_vocab_out: 50277
parallel_attn_mlp: false
rotary_dim: null
n_params: 2097152
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50277, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-1]':
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
unembed:
  W_U: (512, 50277)
  b_U: (50277,)
","{'embed': {'W_E': '(50277, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-1]': {'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}}}, 'unembed': {'W_U': '(512, 50277)', 'b_U': '(50277,)'}}","blocks:
  '[0-1]':
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-1]': {'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, '[hook_resid_pre, hook_attn_out, hook_resid_post]': '(batch, seq_len, 512)'}}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
solu-1l-wiki,NeelNanda/SoLU_1L512W_Wiki_Finetune,"['solu-1l-wiki', 'solu-1l-wiki-finetune', 'solu-1l-finetune']",solu,SoLU_1L512W_Wiki_Finetune,3.1M,3145728,,3145728,1,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 1
d_model: 512
n_ctx: 1024
d_head: 64
model_name: SoLU_1L512W_Wiki_Finetune
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 3145728
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '0':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (2048,)
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'0': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(2048,)'}, 'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '0':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2048)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'0': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
solu-4l-wiki,NeelNanda/SoLU_4L512W_Wiki_Finetune,"['solu-4l-wiki', 'solu-4l-wiki-finetune', 'solu-4l-finetune']",solu,SoLU_4L512W_Wiki_Finetune,13M,12582912,,12582912,4,8,512,48262,solu_ln,standard,False,neel,LN,"n_layers: 4
d_model: 512
n_ctx: 1024
d_head: 64
model_name: SoLU_4L512W_Wiki_Finetune
n_heads: 8
d_mlp: 2048
act_fn: solu_ln
d_vocab: 48262
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: NeelNanda/gpt-neox-tokenizer-digits
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 48262
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (48262, 512)
pos_embed:
  W_pos: (1024, 512)
blocks:
  '[0-3]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (1024, 1024)
      IGNORE: ()
    mlp:
      ln:
        '[w, b]': (2048,)
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 48262)
  b_U: (48262,)
","{'embed': {'W_E': '(48262, 512)'}, 'pos_embed': {'W_pos': '(1024, 512)'}, 'blocks': {'[0-3]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(1024, 1024)', 'IGNORE': '()'}, 'mlp': {'ln': {'[w, b]': '(2048,)'}, 'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 48262)', 'b_U': '(48262,)'}}","blocks:
  '[0-3]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      ln:
        hook_scale: (batch, seq_len, 1)
        hook_normalized: (batch, seq_len, 2048)
      '[hook_pre, hook_mid, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-3]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_pre, hook_mid, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
redwood_attn_2l,ArthurConmy/redwood_attn_2l,['redwood_attn_2l'],,redwood_attn_2l,524k,524288,,524288,2,8,256,50259,gelu_new,shortformer,False,neel,LN,"n_layers: 2
d_model: 256
n_ctx: 2048
d_head: 32
model_name: redwood_attn_2l
n_heads: 8
d_mlp: -1
act_fn: gelu_new
d_vocab: 50259
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: neel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: ArthurConmy/redwood_tokenizer
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: true
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZqT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: shortformer
final_rms: false
d_vocab_out: 50259
parallel_attn_mlp: false
rotary_dim: null
n_params: 524288
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50259, 256)
pos_embed:
  W_pos: (2048, 256)
blocks:
  '[0-1]':
    ln1:
      '[w, b]': (256,)
    attn:
      '[W_Q, W_K, W_V]': (8, 256, 32)
      W_O: (8, 32, 256)
      '[b_Q, b_K, b_V]': (8, 32)
      b_O: (256,)
      mask: (2048, 2048)
      IGNORE: ()
ln_final:
  '[w, b]': (256,)
unembed:
  W_U: (256, 50259)
  b_U: (50259,)
","{'embed': {'W_E': '(50259, 256)'}, 'pos_embed': {'W_pos': '(2048, 256)'}, 'blocks': {'[0-1]': {'ln1': {'[w, b]': '(256,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 256, 32)', 'W_O': '(8, 32, 256)', '[b_Q, b_K, b_V]': '(8, 32)', 'b_O': '(256,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}}}, 'ln_final': {'[w, b]': '(256,)'}, 'unembed': {'W_U': '(256, 50259)', 'b_U': '(50259,)'}}","blocks:
  '[0-1]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 256)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 32)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    '[hook_resid_pre, hook_attn_out, hook_resid_post]': (batch, seq_len, 256)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 256)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)
","{'blocks': {'[0-1]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 32)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, '[hook_resid_pre, hook_attn_out, hook_resid_post]': '(batch, seq_len, 256)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 256)'}"
llama-7b,llama-7b-hf,['llama-7b'],llama,llama-7b-hf,5.0B,5033164800,7b,5033164800,32,32,4096,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: llama-7b-hf
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 32000
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: llama-7b-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
llama-13b,llama-13b-hf,['llama-13b'],llama,llama-13b-hf,9.9B,9856614400,13b,9856614400,40,40,5120,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 40
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: llama-13b-hf
n_heads: 40
d_mlp: 13824
act_fn: silu
d_vocab: 32000
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: llama-13b-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 9856614400
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 5120)
blocks:
  '[0-39]':
    ln1:
      w: (5120,)
    ln2:
      w: (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (5120, 13824)
      b_in: (13824,)
      W_out: (13824, 5120)
      b_out: (5120,)
ln_final:
  w: (5120,)
unembed:
  W_U: (5120, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 5120)'}, 'blocks': {'[0-39]': {'ln1': {'w': '(5120,)'}, 'ln2': {'w': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(5120, 13824)', 'b_in': '(13824,)', 'W_out': '(13824, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'w': '(5120,)'}, 'unembed': {'W_U': '(5120, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-39]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13824)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-39]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 13824)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
llama-30b,llama-30b-hf,['llama-30b'],llama,llama-30b-hf,25B,24945623040,30b,24945623040,60,52,6656,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 60
d_model: 6656
n_ctx: 2048
d_head: 128
model_name: llama-30b-hf
n_heads: 52
d_mlp: 17920
act_fn: silu
d_vocab: 32000
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: llama-30b-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  sJGnGhEVhD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 24945623040
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 6656)
blocks:
  '[0-59]':
    ln1:
      w: (6656,)
    ln2:
      w: (6656,)
    attn:
      '[W_Q, W_K, W_V]': (52, 6656, 128)
      W_O: (52, 128, 6656)
      '[b_Q, b_K, b_V]': (52, 128)
      b_O: (6656,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (6656, 17920)
      b_in: (17920,)
      W_out: (17920, 6656)
      b_out: (6656,)
ln_final:
  w: (6656,)
unembed:
  W_U: (6656, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 6656)'}, 'blocks': {'[0-59]': {'ln1': {'w': '(6656,)'}, 'ln2': {'w': '(6656,)'}, 'attn': {'[W_Q, W_K, W_V]': '(52, 6656, 128)', 'W_O': '(52, 128, 6656)', '[b_Q, b_K, b_V]': '(52, 128)', 'b_O': '(6656,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(6656, 17920)', 'b_in': '(17920,)', 'W_out': '(17920, 6656)', 'b_out': '(6656,)'}}}, 'ln_final': {'w': '(6656,)'}, 'unembed': {'W_U': '(6656, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-59]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6656)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        52, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 52, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6656)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 17920)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 6656)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 6656)
hook_embed: (batch, seq_len, 6656)
","{'blocks': {'[0-59]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6656)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 52, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 52, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6656)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 17920)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 6656)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6656)'}, 'hook_embed': '(batch, seq_len, 6656)'}"
llama-65b,llama-65b-hf,['llama-65b'],llama,llama-65b-hf,50B,50331648000,65b,50331648000,80,64,8192,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 80
d_model: 8192
n_ctx: 2048
d_head: 128
model_name: llama-65b-hf
n_heads: 64
d_mlp: 22016
act_fn: silu
d_vocab: 32000
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: llama-65b-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgagj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 50331648000
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 8192)
blocks:
  '[0-79]':
    ln1:
      w: (8192,)
    ln2:
      w: (8192,)
    attn:
      '[W_Q, W_K, W_V]': (64, 8192, 128)
      W_O: (64, 128, 8192)
      '[b_Q, b_K, b_V]': (64, 128)
      b_O: (8192,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (8192, 22016)
      b_in: (22016,)
      W_out: (22016, 8192)
      b_out: (8192,)
ln_final:
  w: (8192,)
unembed:
  W_U: (8192, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 8192)'}, 'blocks': {'[0-79]': {'ln1': {'w': '(8192,)'}, 'ln2': {'w': '(8192,)'}, 'attn': {'[W_Q, W_K, W_V]': '(64, 8192, 128)', 'W_O': '(64, 128, 8192)', '[b_Q, b_K, b_V]': '(64, 128)', 'b_O': '(8192,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(8192, 22016)', 'b_in': '(22016,)', 'W_out': '(22016, 8192)', 'b_out': '(8192,)'}}}, 'ln_final': {'w': '(8192,)'}, 'unembed': {'W_U': '(8192, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-79]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 8192)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        64, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 64, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 8192)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 22016)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 8192)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 8192)
hook_embed: (batch, seq_len, 8192)
","{'blocks': {'[0-79]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 8192)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 64, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 64, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 8192)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 22016)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 8192)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 8192)'}, 'hook_embed': '(batch, seq_len, 8192)'}"
Llama-2-7b,,[],Llama-2,Llama-2-7b-hf,5.0B,5033164800,7b,5033164800,32,32,4096,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 4096
d_head: 128
model_name: Llama-2-7b-hf
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 32000
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: meta-llama/Llama-2-7b-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
Llama-2-7b-chat,,[],Llama-2,Llama-2-7b-chat-hf,5.0B,5033164800,7b,5033164800,32,32,4096,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 4096
d_head: 128
model_name: Llama-2-7b-chat-hf
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 32000
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: meta-llama/Llama-2-7b-chat-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
Llama-2-13b,,[],Llama-2,Llama-2-13b-hf,9.9B,9856614400,13b,9856614400,40,40,5120,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 40
d_model: 5120
n_ctx: 4096
d_head: 128
model_name: Llama-2-13b-hf
n_heads: 40
d_mlp: 13824
act_fn: silu
d_vocab: 32000
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: meta-llama/Llama-2-13b-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 9856614400
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 5120)
blocks:
  '[0-39]':
    ln1:
      w: (5120,)
    ln2:
      w: (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 128)
    mlp:
      '[W_in, W_gate]': (5120, 13824)
      b_in: (13824,)
      W_out: (13824, 5120)
      b_out: (5120,)
ln_final:
  w: (5120,)
unembed:
  W_U: (5120, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 5120)'}, 'blocks': {'[0-39]': {'ln1': {'w': '(5120,)'}, 'ln2': {'w': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 128)'}, 'mlp': {'[W_in, W_gate]': '(5120, 13824)', 'b_in': '(13824,)', 'W_out': '(13824, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'w': '(5120,)'}, 'unembed': {'W_U': '(5120, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-39]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13824)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-39]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 13824)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
Llama-2-13b-chat,,[],Llama-2,Llama-2-13b-chat-hf,9.9B,9856614400,13b,9856614400,40,40,5120,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 40
d_model: 5120
n_ctx: 4096
d_head: 128
model_name: Llama-2-13b-chat-hf
n_heads: 40
d_mlp: 13824
act_fn: silu
d_vocab: 32000
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: meta-llama/Llama-2-13b-chat-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  4Vdm0bflhj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 9856614400
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 5120)
blocks:
  '[0-39]':
    ln1:
      w: (5120,)
    ln2:
      w: (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 128)
    mlp:
      '[W_in, W_gate]': (5120, 13824)
      b_in: (13824,)
      W_out: (13824, 5120)
      b_out: (5120,)
ln_final:
  w: (5120,)
unembed:
  W_U: (5120, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 5120)'}, 'blocks': {'[0-39]': {'ln1': {'w': '(5120,)'}, 'ln2': {'w': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 128)'}, 'mlp': {'[W_in, W_gate]': '(5120, 13824)', 'b_in': '(13824,)', 'W_out': '(13824, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'w': '(5120,)'}, 'unembed': {'W_U': '(5120, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-39]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13824)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-39]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 13824)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
CodeLlamallama-2-7b,,[],llama,CodeLlama-7b-hf,5.0B,5033164800,7b,5033164800,32,32,4096,32016,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 4096
d_head: 128
model_name: CodeLlama-7b-hf
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 32016
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: CodeLlama-7b-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32016
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 1000000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32016, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32016)
  b_U: (32016,)
","{'embed': {'W_E': '(32016, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32016)', 'b_U': '(32016,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
CodeLlama-7b-python,,[],CodeLlama,CodeLlama-7b-Python-hf,5.0B,5033164800,7b,5033164800,32,32,4096,32000,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 4096
d_head: 128
model_name: CodeLlama-7b-Python-hf
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 32000
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: CodeLlama-7b-Python-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 1000000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32000)', 'b_U': '(32000,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
CodeLlama-7b-instruct,,[],CodeLlama,CodeLlama-7b-Instruct-hf,5.0B,5033164800,7b,5033164800,32,32,4096,32016,silu,rotary,False,LlamaForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 4096
d_head: 128
model_name: CodeLlama-7b-Instruct-hf
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 32016
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: LlamaForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: CodeLlama-7b-Instruct-hf
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 32016
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 1000000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32016, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32016)
  b_U: (32016,)
","{'embed': {'W_E': '(32016, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32016)', 'b_U': '(32016,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
othello-gpt,Baidicoot/Othello-GPT-Transformer-Lens,['othello-gpt'],,Othello-GPT-Transformer-Lens,25M,25165824,,25165824,8,8,512,61,gelu,standard,False,mingpt,LN,"n_layers: 8
d_model: 512
n_ctx: 59
d_head: 64
model_name: Othello-GPT-Transformer-Lens
n_heads: 8
d_mlp: 2048
act_fn: gelu
d_vocab: 61
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: mingpt
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: null
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 61
parallel_attn_mlp: false
rotary_dim: null
n_params: 25165824
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (61, 512)
pos_embed:
  W_pos: (59, 512)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (8, 512, 64)
      W_O: (8, 64, 512)
      '[b_Q, b_K, b_V]': (8, 64)
      b_O: (512,)
      mask: (59, 59)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 61)
  b_U: (61,)
","{'embed': {'W_E': '(61, 512)'}, 'pos_embed': {'W_pos': '(59, 512)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(8, 512, 64)', 'W_O': '(8, 64, 512)', '[b_Q, b_K, b_V]': '(8, 64)', 'b_O': '(512,)', 'mask': '(59, 59)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 61)', 'b_U': '(61,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 8, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 8, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 8, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 8, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
bert-base-cased,bert-base-cased,[],bert,bert-base-cased,85M,84934656,,84934656,12,12,768,28996,gelu,standard,False,BertForMaskedLM,LN,"n_layers: 12
d_model: 768
n_ctx: 512
d_head: 64
model_name: bert-base-cased
n_heads: 12
d_mlp: 3072
act_fn: gelu
d_vocab: 28996
eps: 1.0e-12
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: BertForMaskedLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: bert-base-cased
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: bidirectional
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 28996
parallel_attn_mlp: false
rotary_dim: null
n_params: 84934656
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (28996, 768)
pos_embed:
  W_pos: (512, 768)
blocks:
  '[0-11]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (12, 768, 64)
      W_O: (12, 64, 768)
      '[b_Q, b_K, b_V]': (12, 64)
      b_O: (768,)
      mask: (512, 512)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 28996)
  b_U: (28996,)
","{'embed': {'W_E': '(28996, 768)'}, 'pos_embed': {'W_pos': '(512, 768)'}, 'blocks': {'[0-11]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(12, 768, 64)', 'W_O': '(12, 64, 768)', '[b_Q, b_K, b_V]': '(12, 64)', 'b_O': '(768,)', 'mask': '(512, 512)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 28996)', 'b_U': '(28996,)'}}","blocks:
  '[0-11]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 12, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 12, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-11]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 12, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 12, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
tiny-stories-1M,,[],tiny-stories,TinyStories-1M,393k,393216,1M,393216,8,16,64,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 64
n_ctx: 2048
d_head: 4
model_name: TinyStories-1M
n_heads: 16
d_mlp: 256
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-1M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZuT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 393216
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 64)
pos_embed:
  W_pos: (2048, 64)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (64,)
    ln2:
      '[w, b]': (64,)
    attn:
      '[W_Q, W_K, W_V]': (16, 64, 4)
      W_O: (16, 4, 64)
      '[b_Q, b_K, b_V]': (16, 4)
      b_O: (64,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (64, 256)
      b_in: (256,)
      W_out: (256, 64)
      b_out: (64,)
ln_final:
  '[w, b]': (64,)
unembed:
  W_U: (64, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 64)'}, 'pos_embed': {'W_pos': '(2048, 64)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(64,)'}, 'ln2': {'[w, b]': '(64,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 64, 4)', 'W_O': '(16, 4, 64)', '[b_Q, b_K, b_V]': '(16, 4)', 'b_O': '(64,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(64, 256)', 'b_in': '(256,)', 'W_out': '(256, 64)', 'b_out': '(64,)'}}}, 'ln_final': {'[w, b]': '(64,)'}, 'unembed': {'W_U': '(64, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 64)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 4)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 64)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 256)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 64)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 64)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 64)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 64)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 4)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 64)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 256)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 64)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 64)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 64)'}"
tiny-stories-3M,,[],tiny-stories,TinyStories-3M,1.6M,1572864,3M,1572864,8,16,128,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 128
n_ctx: 2048
d_head: 8
model_name: TinyStories-3M
n_heads: 16
d_mlp: 512
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-3M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgasj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 1572864
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 128)
pos_embed:
  W_pos: (2048, 128)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (128,)
    ln2:
      '[w, b]': (128,)
    attn:
      '[W_Q, W_K, W_V]': (16, 128, 8)
      W_O: (16, 8, 128)
      '[b_Q, b_K, b_V]': (16, 8)
      b_O: (128,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (128, 512)
      b_in: (512,)
      W_out: (512, 128)
      b_out: (128,)
ln_final:
  '[w, b]': (128,)
unembed:
  W_U: (128, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 128)'}, 'pos_embed': {'W_pos': '(2048, 128)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(128,)'}, 'ln2': {'[w, b]': '(128,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 128, 8)', 'W_O': '(16, 8, 128)', '[b_Q, b_K, b_V]': '(16, 8)', 'b_O': '(128,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(128, 512)', 'b_in': '(512,)', 'W_out': '(512, 128)', 'b_out': '(128,)'}}}, 'ln_final': {'[w, b]': '(128,)'}, 'unembed': {'W_U': '(128, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 128)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 8)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 128)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 512)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 128)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 128)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 128)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 8)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 512)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 128)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 128)'}"
tiny-stories-8M,,[],tiny-stories,TinyStories-8M,6.3M,6291456,8M,6291456,8,16,256,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 256
n_ctx: 2048
d_head: 16
model_name: TinyStories-8M
n_heads: 16
d_mlp: 1024
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-8M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZqT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 6291456
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 256)
pos_embed:
  W_pos: (2048, 256)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (256,)
    ln2:
      '[w, b]': (256,)
    attn:
      '[W_Q, W_K, W_V]': (16, 256, 16)
      W_O: (16, 16, 256)
      '[b_Q, b_K, b_V]': (16, 16)
      b_O: (256,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (256, 1024)
      b_in: (1024,)
      W_out: (1024, 256)
      b_out: (256,)
ln_final:
  '[w, b]': (256,)
unembed:
  W_U: (256, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 256)'}, 'pos_embed': {'W_pos': '(2048, 256)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(256,)'}, 'ln2': {'[w, b]': '(256,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 256, 16)', 'W_O': '(16, 16, 256)', '[b_Q, b_K, b_V]': '(16, 16)', 'b_O': '(256,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(256, 1024)', 'b_in': '(1024,)', 'W_out': '(1024, 256)', 'b_out': '(256,)'}}}, 'ln_final': {'[w, b]': '(256,)'}, 'unembed': {'W_U': '(256, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 256)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 16)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 256)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 1024)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 256)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 256)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 16)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 1024)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 256)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 256)'}"
tiny-stories-28M,,[],tiny-stories,TinyStories-28M,25M,25165824,28M,25165824,8,16,512,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 512
n_ctx: 2048
d_head: 32
model_name: TinyStories-28M
n_heads: 16
d_mlp: 2048
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-28M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 25165824
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 512)
pos_embed:
  W_pos: (2048, 512)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (16, 512, 32)
      W_O: (16, 32, 512)
      '[b_Q, b_K, b_V]': (16, 32)
      b_O: (512,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 512)'}, 'pos_embed': {'W_pos': '(2048, 512)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 512, 32)', 'W_O': '(16, 32, 512)', '[b_Q, b_K, b_V]': '(16, 32)', 'b_O': '(512,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 32)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 32)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
tiny-stories-33M,,[],tiny-stories,TinyStories-33M,28M,28311552,33M,28311552,4,16,768,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 4
d_model: 768
n_ctx: 2048
d_head: 48
model_name: TinyStories-33M
n_heads: 16
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-33M
window_size: 256
attn_types:
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 28311552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (2048, 768)
blocks:
  '[0-3]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (16, 768, 48)
      W_O: (16, 48, 768)
      '[b_Q, b_K, b_V]': (16, 48)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(2048, 768)'}, 'blocks': {'[0-3]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 768, 48)', 'W_O': '(16, 48, 768)', '[b_Q, b_K, b_V]': '(16, 48)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-3]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 48)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-3]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 48)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
tiny-stories-instruct-1M,,[],tiny-stories,TinyStories-Instruct-1M,393k,393216,1M,393216,8,16,64,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 64
n_ctx: 2048
d_head: 4
model_name: TinyStories-Instruct-1M
n_heads: 16
d_mlp: 256
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-Instruct-1M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZuT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 393216
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 64)
pos_embed:
  W_pos: (2048, 64)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (64,)
    ln2:
      '[w, b]': (64,)
    attn:
      '[W_Q, W_K, W_V]': (16, 64, 4)
      W_O: (16, 4, 64)
      '[b_Q, b_K, b_V]': (16, 4)
      b_O: (64,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (64, 256)
      b_in: (256,)
      W_out: (256, 64)
      b_out: (64,)
ln_final:
  '[w, b]': (64,)
unembed:
  W_U: (64, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 64)'}, 'pos_embed': {'W_pos': '(2048, 64)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(64,)'}, 'ln2': {'[w, b]': '(64,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 64, 4)', 'W_O': '(16, 4, 64)', '[b_Q, b_K, b_V]': '(16, 4)', 'b_O': '(64,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(64, 256)', 'b_in': '(256,)', 'W_out': '(256, 64)', 'b_out': '(64,)'}}}, 'ln_final': {'[w, b]': '(64,)'}, 'unembed': {'W_U': '(64, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 64)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 4)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 64)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 256)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 64)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 64)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 64)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 64)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 4)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 64)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 256)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 64)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 64)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 64)'}"
tiny-stories-instruct-3M,,[],tiny-stories,TinyStories-Instruct-3M,1.6M,1572864,3M,1572864,8,16,128,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 128
n_ctx: 2048
d_head: 8
model_name: TinyStories-Instruct-3M
n_heads: 16
d_mlp: 512
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-Instruct-3M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgasj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 1572864
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 128)
pos_embed:
  W_pos: (2048, 128)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (128,)
    ln2:
      '[w, b]': (128,)
    attn:
      '[W_Q, W_K, W_V]': (16, 128, 8)
      W_O: (16, 8, 128)
      '[b_Q, b_K, b_V]': (16, 8)
      b_O: (128,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (128, 512)
      b_in: (512,)
      W_out: (512, 128)
      b_out: (128,)
ln_final:
  '[w, b]': (128,)
unembed:
  W_U: (128, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 128)'}, 'pos_embed': {'W_pos': '(2048, 128)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(128,)'}, 'ln2': {'[w, b]': '(128,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 128, 8)', 'W_O': '(16, 8, 128)', '[b_Q, b_K, b_V]': '(16, 8)', 'b_O': '(128,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(128, 512)', 'b_in': '(512,)', 'W_out': '(512, 128)', 'b_out': '(128,)'}}}, 'ln_final': {'[w, b]': '(128,)'}, 'unembed': {'W_U': '(128, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 128)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 8)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 128)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 512)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 128)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 128)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 128)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 8)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 512)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 128)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 128)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 128)'}"
tiny-stories-instruct-8M,,[],tiny-stories,TinyStories-Instruct-8M,6.3M,6291456,8M,6291456,8,16,256,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 256
n_ctx: 2048
d_head: 16
model_name: TinyStories-Instruct-8M
n_heads: 16
d_mlp: 1024
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-Instruct-8M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZqT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 6291456
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 256)
pos_embed:
  W_pos: (2048, 256)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (256,)
    ln2:
      '[w, b]': (256,)
    attn:
      '[W_Q, W_K, W_V]': (16, 256, 16)
      W_O: (16, 16, 256)
      '[b_Q, b_K, b_V]': (16, 16)
      b_O: (256,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (256, 1024)
      b_in: (1024,)
      W_out: (1024, 256)
      b_out: (256,)
ln_final:
  '[w, b]': (256,)
unembed:
  W_U: (256, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 256)'}, 'pos_embed': {'W_pos': '(2048, 256)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(256,)'}, 'ln2': {'[w, b]': '(256,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 256, 16)', 'W_O': '(16, 16, 256)', '[b_Q, b_K, b_V]': '(16, 16)', 'b_O': '(256,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(256, 1024)', 'b_in': '(1024,)', 'W_out': '(1024, 256)', 'b_out': '(256,)'}}}, 'ln_final': {'[w, b]': '(256,)'}, 'unembed': {'W_U': '(256, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 256)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 16)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 256)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 1024)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 256)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 256)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 256)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 16)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 1024)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 256)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 256)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 256)'}"
tiny-stories-instruct-28M,,[],tiny-stories,TinyStories-Instruct-28M,25M,25165824,28M,25165824,8,16,512,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 8
d_model: 512
n_ctx: 2048
d_head: 32
model_name: TinyStories-Instruct-28M
n_heads: 16
d_mlp: 2048
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-Instruct-28M
window_size: 256
attn_types:
- global
- local
- global
- local
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgaoj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 25165824
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 512)
pos_embed:
  W_pos: (2048, 512)
blocks:
  '[0-7]':
    ln1:
      '[w, b]': (512,)
    ln2:
      '[w, b]': (512,)
    attn:
      '[W_Q, W_K, W_V]': (16, 512, 32)
      W_O: (16, 32, 512)
      '[b_Q, b_K, b_V]': (16, 32)
      b_O: (512,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (512, 2048)
      b_in: (2048,)
      W_out: (2048, 512)
      b_out: (512,)
ln_final:
  '[w, b]': (512,)
unembed:
  W_U: (512, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 512)'}, 'pos_embed': {'W_pos': '(2048, 512)'}, 'blocks': {'[0-7]': {'ln1': {'[w, b]': '(512,)'}, 'ln2': {'[w, b]': '(512,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 512, 32)', 'W_O': '(16, 32, 512)', '[b_Q, b_K, b_V]': '(16, 32)', 'b_O': '(512,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(512, 2048)', 'b_in': '(2048,)', 'W_out': '(2048, 512)', 'b_out': '(512,)'}}}, 'ln_final': {'[w, b]': '(512,)'}, 'unembed': {'W_U': '(512, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-7]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 32)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 512)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 2048)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 512)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 512)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 512)
","{'blocks': {'[0-7]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 32)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 2048)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 512)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 512)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 512)'}"
tiny-stories-instruct-33M,,[],tiny-stories,TinyStories-Instruct-33M,28M,28311552,33M,28311552,4,16,768,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 4
d_model: 768
n_ctx: 2048
d_head: 48
model_name: TinyStories-Instruct-33M
n_heads: 16
d_mlp: 3072
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-Instruct-33M
window_size: 256
attn_types:
- global
- local
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  LrjmCHKPnT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 28311552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 768)
pos_embed:
  W_pos: (2048, 768)
blocks:
  '[0-3]':
    ln1:
      '[w, b]': (768,)
    ln2:
      '[w, b]': (768,)
    attn:
      '[W_Q, W_K, W_V]': (16, 768, 48)
      W_O: (16, 48, 768)
      '[b_Q, b_K, b_V]': (16, 48)
      b_O: (768,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (768, 3072)
      b_in: (3072,)
      W_out: (3072, 768)
      b_out: (768,)
ln_final:
  '[w, b]': (768,)
unembed:
  W_U: (768, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 768)'}, 'pos_embed': {'W_pos': '(2048, 768)'}, 'blocks': {'[0-3]': {'ln1': {'[w, b]': '(768,)'}, 'ln2': {'[w, b]': '(768,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 768, 48)', 'W_O': '(16, 48, 768)', '[b_Q, b_K, b_V]': '(16, 48)', 'b_O': '(768,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(768, 3072)', 'b_in': '(3072,)', 'W_out': '(3072, 768)', 'b_out': '(768,)'}}}, 'ln_final': {'[w, b]': '(768,)'}, 'unembed': {'W_U': '(768, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-3]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 48)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 768)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 3072)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 768)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 768)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 768)
","{'blocks': {'[0-3]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 48)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 3072)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 768)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 768)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 768)'}"
tiny-stories-1L-21M,,[],tiny-stories,TinyStories-1Layer-21M,13M,12582912,21M,12582912,1,16,1024,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 1
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: TinyStories-1Layer-21M
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-1Layer-21M
window_size: 256
attn_types:
- global
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (2048, 1024)
blocks:
  '0':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(2048, 1024)'}, 'blocks': {'0': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '0':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'0': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
tiny-stories-2L-33M,,[],tiny-stories,TinyStories-2Layers-33M,25M,25165824,33M,25165824,2,16,1024,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 2
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: TinyStories-2Layers-33M
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-2Layers-33M
window_size: 256
attn_types:
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 25165824
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (2048, 1024)
blocks:
  '[0-1]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(2048, 1024)'}, 'blocks': {'[0-1]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-1]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-1]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
tiny-stories-instruct-1L-21M,,[],tiny-stories,TinyStories-Instuct-1Layer-21M,13M,12582912,21M,12582912,1,16,1024,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 1
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: TinyStories-Instuct-1Layer-21M
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-Instuct-1Layer-21M
window_size: 256
attn_types:
- global
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 12582912
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (2048, 1024)
blocks:
  '0':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(2048, 1024)'}, 'blocks': {'0': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '0':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'0': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
tiny-stories-instruct-2L-33M,,[],tiny-stories,TinyStories-Instruct-2Layers-33M,25M,25165824,33M,25165824,2,16,1024,50257,gelu_new,standard,False,GPTNeoForCausalLM,LN,"n_layers: 2
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: TinyStories-Instruct-2Layers-33M
n_heads: 16
d_mlp: 4096
act_fn: gelu_new
d_vocab: 50257
eps: 1.0e-05
use_attn_result: false
use_attn_scale: false
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: GPTNeoForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: roneneldan/TinyStories-Instruct-2Layers-33M
window_size: 256
attn_types:
- global
- local
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 50257
parallel_attn_mlp: false
rotary_dim: null
n_params: 25165824
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50257, 1024)
pos_embed:
  W_pos: (2048, 1024)
blocks:
  '[0-1]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 50257)
  b_U: (50257,)
","{'embed': {'W_E': '(50257, 1024)'}, 'pos_embed': {'W_pos': '(2048, 1024)'}, 'blocks': {'[0-1]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 50257)', 'b_U': '(50257,)'}}","blocks:
  '[0-1]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 1024)
","{'blocks': {'[0-1]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 1024)'}"
stablelm-base-alpha-3b,stabilityai/stablelm-base-alpha-3b,"['stablelm-base-alpha-3b', 'stablelm-base-3b']",stablelm,stablelm-base-alpha-3b,3.2B,3221225472,3b,3221225472,16,32,4096,50688,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 4096
n_ctx: 4096
d_head: 128
model_name: stablelm-base-alpha-3b
n_heads: 32
d_mlp: 16384
act_fn: gelu
d_vocab: 50688
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stabilityai/stablelm-base-alpha-3b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50688
parallel_attn_mlp: true
rotary_dim: 32
n_params: 3221225472
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50688, 4096)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 32)
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50688)
  b_U: (50688,)
","{'embed': {'W_E': '(50688, 4096)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 32)'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50688)', 'b_U': '(50688,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
stablelm-base-alpha-7b,stabilityai/stablelm-base-alpha-7b,"['stablelm-base-alpha-7b', 'stablelm-base-7b']",stablelm,stablelm-base-alpha-7b,7.2B,7247757312,7b,7247757312,16,48,6144,50432,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 6144
n_ctx: 4096
d_head: 128
model_name: stablelm-base-alpha-7b
n_heads: 48
d_mlp: 24576
act_fn: gelu
d_vocab: 50432
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stabilityai/stablelm-base-alpha-7b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  MvA88/3mhD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50432
parallel_attn_mlp: true
rotary_dim: 32
n_params: 7247757312
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50432, 6144)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (6144,)
    ln2:
      '[w, b]': (6144,)
    attn:
      '[W_Q, W_K, W_V]': (48, 6144, 128)
      W_O: (48, 128, 6144)
      '[b_Q, b_K, b_V]': (48, 128)
      b_O: (6144,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 32)
    mlp:
      W_in: (6144, 24576)
      b_in: (24576,)
      W_out: (24576, 6144)
      b_out: (6144,)
ln_final:
  '[w, b]': (6144,)
unembed:
  W_U: (6144, 50432)
  b_U: (50432,)
","{'embed': {'W_E': '(50432, 6144)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(6144,)'}, 'ln2': {'[w, b]': '(6144,)'}, 'attn': {'[W_Q, W_K, W_V]': '(48, 6144, 128)', 'W_O': '(48, 128, 6144)', '[b_Q, b_K, b_V]': '(48, 128)', 'b_O': '(6144,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 32)'}, 'mlp': {'W_in': '(6144, 24576)', 'b_in': '(24576,)', 'W_out': '(24576, 6144)', 'b_out': '(6144,)'}}}, 'ln_final': {'[w, b]': '(6144,)'}, 'unembed': {'W_U': '(6144, 50432)', 'b_U': '(50432,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6144)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        48, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 48, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6144)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 24576)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      6144)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 6144)
hook_embed: (batch, seq_len, 6144)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 48, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 48, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 24576)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 6144)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'hook_embed': '(batch, seq_len, 6144)'}"
stablelm-tuned-alpha-3b,stabilityai/stablelm-tuned-alpha-3b,"['stablelm-tuned-alpha-3b', 'stablelm-tuned-3b']",stablelm,stablelm-tuned-alpha-3b,3.2B,3221225472,3b,3221225472,16,32,4096,50688,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 4096
n_ctx: 4096
d_head: 128
model_name: stablelm-tuned-alpha-3b
n_heads: 32
d_mlp: 16384
act_fn: gelu
d_vocab: 50688
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stabilityai/stablelm-tuned-alpha-3b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50688
parallel_attn_mlp: true
rotary_dim: 32
n_params: 3221225472
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50688, 4096)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 32)
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 50688)
  b_U: (50688,)
","{'embed': {'W_E': '(50688, 4096)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 32)'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 50688)', 'b_U': '(50688,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
stablelm-tuned-alpha-7b,stabilityai/stablelm-tuned-alpha-7b,"['stablelm-tuned-alpha-7b', 'stablelm-tuned-7b']",stablelm,stablelm-tuned-alpha-7b,7.2B,7247757312,7b,7247757312,16,48,6144,50432,gelu,rotary,True,GPTNeoXForCausalLM,LN,"n_layers: 16
d_model: 6144
n_ctx: 4096
d_head: 128
model_name: stablelm-tuned-alpha-7b
n_heads: 48
d_mlp: 24576
act_fn: gelu
d_vocab: 50432
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPTNeoXForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: stabilityai/stablelm-tuned-alpha-7b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  MvA88/3mhD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 50432
parallel_attn_mlp: true
rotary_dim: 32
n_params: 7247757312
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (50432, 6144)
blocks:
  '[0-15]':
    ln1:
      '[w, b]': (6144,)
    ln2:
      '[w, b]': (6144,)
    attn:
      '[W_Q, W_K, W_V]': (48, 6144, 128)
      W_O: (48, 128, 6144)
      '[b_Q, b_K, b_V]': (48, 128)
      b_O: (6144,)
      mask: (4096, 4096)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (4096, 32)
    mlp:
      W_in: (6144, 24576)
      b_in: (24576,)
      W_out: (24576, 6144)
      b_out: (6144,)
ln_final:
  '[w, b]': (6144,)
unembed:
  W_U: (6144, 50432)
  b_U: (50432,)
","{'embed': {'W_E': '(50432, 6144)'}, 'blocks': {'[0-15]': {'ln1': {'[w, b]': '(6144,)'}, 'ln2': {'[w, b]': '(6144,)'}, 'attn': {'[W_Q, W_K, W_V]': '(48, 6144, 128)', 'W_O': '(48, 128, 6144)', '[b_Q, b_K, b_V]': '(48, 128)', 'b_O': '(6144,)', 'mask': '(4096, 4096)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(4096, 32)'}, 'mlp': {'W_in': '(6144, 24576)', 'b_in': '(24576,)', 'W_out': '(24576, 6144)', 'b_out': '(6144,)'}}}, 'ln_final': {'[w, b]': '(6144,)'}, 'unembed': {'W_U': '(6144, 50432)', 'b_U': '(50432,)'}}","blocks:
  '[0-15]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6144)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        48, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 48, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 6144)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 24576)
    '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': (batch, seq_len,
      6144)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 6144)
hook_embed: (batch, seq_len, 6144)
","{'blocks': {'[0-15]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 48, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 48, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 24576)'}, '[hook_resid_pre, hook_attn_out, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 6144)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 6144)'}, 'hook_embed': '(batch, seq_len, 6144)'}"
mistral-7b,mistralai/Mistral-7B-v0.1,['mistral-7b'],mistral,Mistral-7B-v0.1,5.9B,5905580032,7b,5905580032,32,32,4096,32000,silu,rotary,False,MistralForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: Mistral-7B-v0.1
n_heads: 32
d_mlp: 14336
act_fn: silu
d_vocab: 32000
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: MistralForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: mistralai/Mistral-7B-v0.1
window_size: 4096
attn_types:
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5905580032
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: 8
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      W_Q: (32, 4096, 128)
      W_O: (32, 128, 4096)
      b_Q: (32, 128)
      b_O: (4096,)
      '[_W_K, _W_V]': (8, 4096, 128)
      '[_b_K, _b_V]': (8, 128)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (4096, 14336)
      b_in: (14336,)
      W_out: (14336, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'W_Q': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', 'b_Q': '(32, 128)', 'b_O': '(4096,)', '[_W_K, _W_V]': '(8, 4096, 128)', '[_b_K, _b_V]': '(8, 128)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 14336)', 'b_in': '(14336,)', 'W_out': '(14336, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32000)', 'b_U': '(32000,)'}}",,
mistral-7b-instruct,mistralai/Mistral-7B-Instruct-v0.1,['mistral-7b-instruct'],mistral,Mistral-7B-Instruct-v0.1,5.9B,5905580032,7b,5905580032,32,32,4096,32000,silu,rotary,False,MistralForCausalLM,RMS,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: Mistral-7B-Instruct-v0.1
n_heads: 32
d_mlp: 14336
act_fn: silu
d_vocab: 32000
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: true
original_architecture: MistralForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: mistralai/Mistral-7B-Instruct-v0.1
window_size: 4096
attn_types:
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
- local
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: false
d_vocab_out: 32000
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5905580032
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: 8
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  W_E: (32000, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      W_Q: (32, 4096, 128)
      W_O: (32, 128, 4096)
      b_Q: (32, 128)
      b_O: (4096,)
      '[_W_K, _W_V]': (8, 4096, 128)
      '[_b_K, _b_V]': (8, 128)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (4096, 14336)
      b_in: (14336,)
      W_out: (14336, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 32000)
  b_U: (32000,)
","{'embed': {'W_E': '(32000, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'W_Q': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', 'b_Q': '(32, 128)', 'b_O': '(4096,)', '[_W_K, _W_V]': '(8, 4096, 128)', '[_b_K, _b_V]': '(8, 128)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 14336)', 'b_in': '(14336,)', 'W_out': '(14336, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 32000)', 'b_U': '(32000,)'}}",,
bloom-560m,bigscience/bloom-560m,['bloom-560m'],bloom,bloom-560m,302M,301989888,560m,301989888,24,16,1024,250880,gelu_fast,alibi,False,BloomForCausalLM,LN,"n_layers: 24
d_model: 1024
n_ctx: 2048
d_head: 64
model_name: bloom-560m
n_heads: 16
d_mlp: 4096
act_fn: gelu_fast
d_vocab: 250880
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: BloomForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: bigscience/bloom-560m
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZmT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: alibi
final_rms: false
d_vocab_out: 250880
parallel_attn_mlp: false
rotary_dim: null
n_params: 301989888
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: true
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  ln:
    '[w, b]': (1024,)
  W_E: (250880, 1024)
pos_embed:
  W_pos: (2048, 1024)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1024,)
    ln2:
      '[w, b]': (1024,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1024, 64)
      W_O: (16, 64, 1024)
      '[b_Q, b_K, b_V]': (16, 64)
      b_O: (1024,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (1024, 4096)
      b_in: (4096,)
      W_out: (4096, 1024)
      b_out: (1024,)
ln_final:
  '[w, b]': (1024,)
unembed:
  W_U: (1024, 250880)
  b_U: (250880,)
","{'embed': {'ln': {'[w, b]': '(1024,)'}, 'W_E': '(250880, 1024)'}, 'pos_embed': {'W_pos': '(2048, 1024)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1024,)'}, 'ln2': {'[w, b]': '(1024,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1024, 64)', 'W_O': '(16, 64, 1024)', '[b_Q, b_K, b_V]': '(16, 64)', 'b_O': '(1024,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1024, 4096)', 'b_in': '(4096,)', 'W_out': '(4096, 1024)', 'b_out': '(1024,)'}}}, 'ln_final': {'[w, b]': '(1024,)'}, 'unembed': {'W_U': '(1024, 250880)', 'b_U': '(250880,)'}}","embed:
  ln:
    hook_scale: (batch, seq_len, 1)
    hook_normalized: (batch, seq_len, 1024)
blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 64)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1024)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 4096)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1024)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1024)
hook_embed: (batch, seq_len, 1024)
","{'embed': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}}, 'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 64)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 4096)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1024)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1024)'}, 'hook_embed': '(batch, seq_len, 1024)'}"
bloom-1b1,bigscience/bloom-1b1,['bloom-1b1'],bloom,bloom-1b1,679M,679477248,,679477248,24,16,1536,250880,gelu_fast,alibi,False,BloomForCausalLM,LN,"n_layers: 24
d_model: 1536
n_ctx: 2048
d_head: 96
model_name: bloom-1b1
n_heads: 16
d_mlp: 6144
act_fn: gelu_fast
d_vocab: 250880
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: BloomForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: bigscience/bloom-1b1
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  MvA88/3mlD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: alibi
final_rms: false
d_vocab_out: 250880
parallel_attn_mlp: false
rotary_dim: null
n_params: 679477248
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: true
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  ln:
    '[w, b]': (1536,)
  W_E: (250880, 1536)
pos_embed:
  W_pos: (2048, 1536)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (1536,)
    ln2:
      '[w, b]': (1536,)
    attn:
      '[W_Q, W_K, W_V]': (16, 1536, 96)
      W_O: (16, 96, 1536)
      '[b_Q, b_K, b_V]': (16, 96)
      b_O: (1536,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (1536, 6144)
      b_in: (6144,)
      W_out: (6144, 1536)
      b_out: (1536,)
ln_final:
  '[w, b]': (1536,)
unembed:
  W_U: (1536, 250880)
  b_U: (250880,)
","{'embed': {'ln': {'[w, b]': '(1536,)'}, 'W_E': '(250880, 1536)'}, 'pos_embed': {'W_pos': '(2048, 1536)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(1536,)'}, 'ln2': {'[w, b]': '(1536,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 1536, 96)', 'W_O': '(16, 96, 1536)', '[b_Q, b_K, b_V]': '(16, 96)', 'b_O': '(1536,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(1536, 6144)', 'b_in': '(6144,)', 'W_out': '(6144, 1536)', 'b_out': '(1536,)'}}}, 'ln_final': {'[w, b]': '(1536,)'}, 'unembed': {'W_U': '(1536, 250880)', 'b_U': '(250880,)'}}","embed:
  ln:
    hook_scale: (batch, seq_len, 1)
    hook_normalized: (batch, seq_len, 1536)
blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1536)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 96)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 1536)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 6144)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 1536)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 1536)
hook_embed: (batch, seq_len, 1536)
","{'embed': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}}, 'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 96)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 6144)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 1536)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 1536)'}, 'hook_embed': '(batch, seq_len, 1536)'}"
bloom-1b7,bigscience/bloom-1b7,['bloom-1b7'],bloom,bloom-1b7,1.2B,1207959552,,1207959552,24,16,2048,250880,gelu_fast,alibi,False,BloomForCausalLM,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: bloom-1b7
n_heads: 16
d_mlp: 8192
act_fn: gelu_fast
d_vocab: 250880
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: BloomForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: bigscience/bloom-1b7
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: alibi
final_rms: false
d_vocab_out: 250880
parallel_attn_mlp: false
rotary_dim: null
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: true
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  ln:
    '[w, b]': (2048,)
  W_E: (250880, 2048)
pos_embed:
  W_pos: (2048, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 250880)
  b_U: (250880,)
","{'embed': {'ln': {'[w, b]': '(2048,)'}, 'W_E': '(250880, 2048)'}, 'pos_embed': {'W_pos': '(2048, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 250880)', 'b_U': '(250880,)'}}","embed:
  ln:
    hook_scale: (batch, seq_len, 1)
    hook_normalized: (batch, seq_len, 2048)
blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'embed': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}}, 'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
bloom-3b,bigscience/bloom-3b,['bloom-3b'],bloom,bloom-3b,2.4B,2359296000,3b,2359296000,30,32,2560,250880,gelu_fast,alibi,False,BloomForCausalLM,LN,"n_layers: 30
d_model: 2560
n_ctx: 2048
d_head: 80
model_name: bloom-3b
n_heads: 32
d_mlp: 10240
act_fn: gelu_fast
d_vocab: 250880
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: BloomForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: bigscience/bloom-3b
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  cjqgTtwwkD8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: alibi
final_rms: false
d_vocab_out: 250880
parallel_attn_mlp: false
rotary_dim: null
n_params: 2359296000
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: true
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  ln:
    '[w, b]': (2560,)
  W_E: (250880, 2560)
pos_embed:
  W_pos: (2048, 2560)
blocks:
  '[0-29]':
    ln1:
      '[w, b]': (2560,)
    ln2:
      '[w, b]': (2560,)
    attn:
      '[W_Q, W_K, W_V]': (32, 2560, 80)
      W_O: (32, 80, 2560)
      '[b_Q, b_K, b_V]': (32, 80)
      b_O: (2560,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (2560, 10240)
      b_in: (10240,)
      W_out: (10240, 2560)
      b_out: (2560,)
ln_final:
  '[w, b]': (2560,)
unembed:
  W_U: (2560, 250880)
  b_U: (250880,)
","{'embed': {'ln': {'[w, b]': '(2560,)'}, 'W_E': '(250880, 2560)'}, 'pos_embed': {'W_pos': '(2048, 2560)'}, 'blocks': {'[0-29]': {'ln1': {'[w, b]': '(2560,)'}, 'ln2': {'[w, b]': '(2560,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 2560, 80)', 'W_O': '(32, 80, 2560)', '[b_Q, b_K, b_V]': '(32, 80)', 'b_O': '(2560,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(2560, 10240)', 'b_in': '(10240,)', 'W_out': '(10240, 2560)', 'b_out': '(2560,)'}}}, 'ln_final': {'[w, b]': '(2560,)'}, 'unembed': {'W_U': '(2560, 250880)', 'b_U': '(250880,)'}}","embed:
  ln:
    hook_scale: (batch, seq_len, 1)
    hook_normalized: (batch, seq_len, 2560)
blocks:
  '[0-29]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 80)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2560)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 10240)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2560)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2560)
hook_embed: (batch, seq_len, 2560)
","{'embed': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}}, 'blocks': {'[0-29]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 32, 80)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 10240)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2560)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2560)'}, 'hook_embed': '(batch, seq_len, 2560)'}"
bloom-7b1,bigscience/bloom-7b1,['bloom-7b1'],bloom,bloom-7b1,6.0B,6039797760,,6039797760,30,32,4096,250880,gelu_fast,alibi,False,BloomForCausalLM,LN,"n_layers: 30
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: bloom-7b1
n_heads: 32
d_mlp: 16384
act_fn: gelu_fast
d_vocab: 250880
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: BloomForCausalLM
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: bigscience/bloom-7b1
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  mpmZmZmZiT8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: alibi
final_rms: false
d_vocab_out: 250880
parallel_attn_mlp: false
rotary_dim: null
n_params: 6039797760
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: true
rotary_base: 10000
trust_remote_code: false
rotary_adjacent_pairs: false
","embed:
  ln:
    '[w, b]': (4096,)
  W_E: (250880, 4096)
pos_embed:
  W_pos: (2048, 4096)
blocks:
  '[0-29]':
    ln1:
      '[w, b]': (4096,)
    ln2:
      '[w, b]': (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (4096, 16384)
      b_in: (16384,)
      W_out: (16384, 4096)
      b_out: (4096,)
ln_final:
  '[w, b]': (4096,)
unembed:
  W_U: (4096, 250880)
  b_U: (250880,)
","{'embed': {'ln': {'[w, b]': '(4096,)'}, 'W_E': '(250880, 4096)'}, 'pos_embed': {'W_pos': '(2048, 4096)'}, 'blocks': {'[0-29]': {'ln1': {'[w, b]': '(4096,)'}, 'ln2': {'[w, b]': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(4096, 16384)', 'b_in': '(16384,)', 'W_out': '(16384, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'[w, b]': '(4096,)'}, 'unembed': {'W_U': '(4096, 250880)', 'b_U': '(250880,)'}}","embed:
  ln:
    hook_scale: (batch, seq_len, 1)
    hook_normalized: (batch, seq_len, 4096)
blocks:
  '[0-29]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 16384)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'embed': {'ln': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}}, 'blocks': {'[0-29]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 16384)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
santacoder,bigcode/santacoder,['santacoder'],,santacoder,1.2B,1207959552,,1207959552,24,16,2048,49280,gelu_fast,standard,False,GPT2LMHeadCustomModel,LN,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: santacoder
n_heads: 16
d_mlp: 8192
act_fn: gelu_fast
d_vocab: 49280
eps: 1.0e-05
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: GPT2LMHeadCustomModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: bigcode/santacoder
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: LN
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: !!python/object/apply:numpy.core.multiarray.scalar
- !!python/object/apply:numpy.dtype
  args:
  - f8
  - false
  - true
  state: !!python/tuple
  - 3
  - <
  - null
  - null
  - null
  - -1
  - -1
  - 0
- !!binary |
  CmP/URgakj8=
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: standard
final_rms: false
d_vocab_out: 49280
parallel_attn_mlp: false
rotary_dim: null
n_params: 1207959552
use_hook_tokens: false
gated_mlp: false
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: null
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: true
rotary_adjacent_pairs: false
","embed:
  W_E: (49280, 2048)
pos_embed:
  W_pos: (2048, 2048)
blocks:
  '[0-23]':
    ln1:
      '[w, b]': (2048,)
    ln2:
      '[w, b]': (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
    mlp:
      W_in: (2048, 8192)
      b_in: (8192,)
      W_out: (8192, 2048)
      b_out: (2048,)
ln_final:
  '[w, b]': (2048,)
unembed:
  W_U: (2048, 49280)
  b_U: (49280,)
","{'embed': {'W_E': '(49280, 2048)'}, 'pos_embed': {'W_pos': '(2048, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'[w, b]': '(2048,)'}, 'ln2': {'[w, b]': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()'}, 'mlp': {'W_in': '(2048, 8192)', 'b_in': '(8192,)', 'W_out': '(8192, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'[w, b]': '(2048,)'}, 'unembed': {'W_U': '(2048, 49280)', 'b_U': '(49280,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_z]': (batch, seq_len, 16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_post]': (batch, seq_len, 8192)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
'[hook_embed, hook_pos_embed]': (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_post]': '(batch, seq_len, 8192)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, '[hook_embed, hook_pos_embed]': '(batch, seq_len, 2048)'}"
qwen-1.8b,Qwen/Qwen-1_8B,['qwen-1.8b'],qwen,Qwen-1_8B,944M,943718400,1.8b,943718400,24,16,2048,151936,silu,rotary,False,QWenLMHeadModel,RMS,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: Qwen-1_8B
n_heads: 16
d_mlp: 5504
act_fn: silu
d_vocab: 151936
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: QWenLMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: Qwen/Qwen-1_8B
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: 0.02
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 151936
parallel_attn_mlp: false
rotary_dim: 128
n_params: 943718400
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: true
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: true
rotary_adjacent_pairs: false
","embed:
  W_E: (151936, 2048)
blocks:
  '[0-23]':
    ln1:
      w: (2048,)
    ln2:
      w: (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (2048, 5504)
      b_in: (5504,)
      W_out: (5504, 2048)
      b_out: (2048,)
ln_final:
  w: (2048,)
unembed:
  W_U: (2048, 151936)
  b_U: (151936,)
","{'embed': {'W_E': '(151936, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'w': '(2048,)'}, 'ln2': {'w': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(2048, 5504)', 'b_in': '(5504,)', 'W_out': '(5504, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'w': '(2048,)'}, 'unembed': {'W_U': '(2048, 151936)', 'b_U': '(151936,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 5504)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 5504)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
qwen-7b,Qwen/Qwen-7B,['qwen-7b'],qwen,Qwen-7B,5.0B,5033164800,7b,5033164800,32,32,4096,151936,silu,rotary,False,QWenLMHeadModel,RMS,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: Qwen-7B
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 151936
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: QWenLMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: Qwen/Qwen-7B
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: 0.02
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 151936
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: true
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: true
rotary_adjacent_pairs: false
","embed:
  W_E: (151936, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 151936)
  b_U: (151936,)
","{'embed': {'W_E': '(151936, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 151936)', 'b_U': '(151936,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
qwen-14b,Qwen/Qwen-14B,['qwen-14b'],qwen,Qwen-14B,9.8B,9804185600,14b,9804185600,40,40,5120,152064,silu,rotary,False,QWenLMHeadModel,RMS,"n_layers: 40
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: Qwen-14B
n_heads: 40
d_mlp: 13696
act_fn: silu
d_vocab: 152064
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: QWenLMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: Qwen/Qwen-14B
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: 0.02
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 152064
parallel_attn_mlp: false
rotary_dim: 128
n_params: 9804185600
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: true
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: true
rotary_adjacent_pairs: false
","embed:
  W_E: (152064, 5120)
blocks:
  '[0-39]':
    ln1:
      w: (5120,)
    ln2:
      w: (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (5120, 13696)
      b_in: (13696,)
      W_out: (13696, 5120)
      b_out: (5120,)
ln_final:
  w: (5120,)
unembed:
  W_U: (5120, 152064)
  b_U: (152064,)
","{'embed': {'W_E': '(152064, 5120)'}, 'blocks': {'[0-39]': {'ln1': {'w': '(5120,)'}, 'ln2': {'w': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(5120, 13696)', 'b_in': '(13696,)', 'W_out': '(13696, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'w': '(5120,)'}, 'unembed': {'W_U': '(5120, 152064)', 'b_U': '(152064,)'}}","blocks:
  '[0-39]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13696)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-39]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 13696)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
qwen-1.8b-chat,Qwen/Qwen-1_8B-Chat,['qwen-1.8b-chat'],qwen,Qwen-1_8B-Chat,944M,943718400,1.8b,943718400,24,16,2048,151936,silu,rotary,False,QWenLMHeadModel,RMS,"n_layers: 24
d_model: 2048
n_ctx: 2048
d_head: 128
model_name: Qwen-1_8B-Chat
n_heads: 16
d_mlp: 5504
act_fn: silu
d_vocab: 151936
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: QWenLMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: Qwen/Qwen-1_8B-Chat
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: 0.02
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 151936
parallel_attn_mlp: false
rotary_dim: 128
n_params: 943718400
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: true
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: true
rotary_adjacent_pairs: false
","embed:
  W_E: (151936, 2048)
blocks:
  '[0-23]':
    ln1:
      w: (2048,)
    ln2:
      w: (2048,)
    attn:
      '[W_Q, W_K, W_V]': (16, 2048, 128)
      W_O: (16, 128, 2048)
      '[b_Q, b_K, b_V]': (16, 128)
      b_O: (2048,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (2048, 5504)
      b_in: (5504,)
      W_out: (5504, 2048)
      b_out: (2048,)
ln_final:
  w: (2048,)
unembed:
  W_U: (2048, 151936)
  b_U: (151936,)
","{'embed': {'W_E': '(151936, 2048)'}, 'blocks': {'[0-23]': {'ln1': {'w': '(2048,)'}, 'ln2': {'w': '(2048,)'}, 'attn': {'[W_Q, W_K, W_V]': '(16, 2048, 128)', 'W_O': '(16, 128, 2048)', '[b_Q, b_K, b_V]': '(16, 128)', 'b_O': '(2048,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(2048, 5504)', 'b_in': '(5504,)', 'W_out': '(5504, 2048)', 'b_out': '(2048,)'}}}, 'ln_final': {'w': '(2048,)'}, 'unembed': {'W_U': '(2048, 151936)', 'b_U': '(151936,)'}}","blocks:
  '[0-23]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        16, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 16, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 2048)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 5504)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 2048)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 2048)
hook_embed: (batch, seq_len, 2048)
","{'blocks': {'[0-23]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 16, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 16, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 5504)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 2048)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 2048)'}, 'hook_embed': '(batch, seq_len, 2048)'}"
qwen-7b-chat,Qwen/Qwen-7B-Chat,['qwen-7b-chat'],qwen,Qwen-7B-Chat,5.0B,5033164800,7b,5033164800,32,32,4096,151936,silu,rotary,False,QWenLMHeadModel,RMS,"n_layers: 32
d_model: 4096
n_ctx: 2048
d_head: 128
model_name: Qwen-7B-Chat
n_heads: 32
d_mlp: 11008
act_fn: silu
d_vocab: 151936
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: QWenLMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: Qwen/Qwen-7B-Chat
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: 0.02
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 151936
parallel_attn_mlp: false
rotary_dim: 128
n_params: 5033164800
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: true
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: true
rotary_adjacent_pairs: false
","embed:
  W_E: (151936, 4096)
blocks:
  '[0-31]':
    ln1:
      w: (4096,)
    ln2:
      w: (4096,)
    attn:
      '[W_Q, W_K, W_V]': (32, 4096, 128)
      W_O: (32, 128, 4096)
      '[b_Q, b_K, b_V]': (32, 128)
      b_O: (4096,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (4096, 11008)
      b_in: (11008,)
      W_out: (11008, 4096)
      b_out: (4096,)
ln_final:
  w: (4096,)
unembed:
  W_U: (4096, 151936)
  b_U: (151936,)
","{'embed': {'W_E': '(151936, 4096)'}, 'blocks': {'[0-31]': {'ln1': {'w': '(4096,)'}, 'ln2': {'w': '(4096,)'}, 'attn': {'[W_Q, W_K, W_V]': '(32, 4096, 128)', 'W_O': '(32, 128, 4096)', '[b_Q, b_K, b_V]': '(32, 128)', 'b_O': '(4096,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(4096, 11008)', 'b_in': '(11008,)', 'W_out': '(11008, 4096)', 'b_out': '(4096,)'}}}, 'ln_final': {'w': '(4096,)'}, 'unembed': {'W_U': '(4096, 151936)', 'b_U': '(151936,)'}}","blocks:
  '[0-31]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        32, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 32, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 4096)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 11008)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 4096)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 4096)
hook_embed: (batch, seq_len, 4096)
","{'blocks': {'[0-31]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 32, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 32, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 11008)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 4096)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 4096)'}, 'hook_embed': '(batch, seq_len, 4096)'}"
qwen-14b-chat,Qwen/Qwen-14B-Chat,['qwen-14b-chat'],qwen,Qwen-14B-Chat,9.8B,9804185600,14b,9804185600,40,40,5120,152064,silu,rotary,False,QWenLMHeadModel,RMS,"n_layers: 40
d_model: 5120
n_ctx: 2048
d_head: 128
model_name: Qwen-14B-Chat
n_heads: 40
d_mlp: 13696
act_fn: silu
d_vocab: 152064
eps: 1.0e-06
use_attn_result: false
use_attn_scale: true
use_split_qkv_input: false
use_hook_mlp_in: false
use_attn_in: false
use_local_attn: false
original_architecture: QWenLMHeadModel
from_checkpoint: false
checkpoint_index: null
checkpoint_label_type: null
checkpoint_value: null
tokenizer_name: Qwen/Qwen-14B-Chat
window_size: null
attn_types: null
init_mode: gpt2
normalization_type: RMS
device: cuda
n_devices: 1
attention_dir: causal
attn_only: false
seed: null
initializer_range: 0.02
init_weights: false
scale_attn_by_inverse_layer_idx: false
positional_embedding_type: rotary
final_rms: true
d_vocab_out: 152064
parallel_attn_mlp: false
rotary_dim: 128
n_params: 9804185600
use_hook_tokens: false
gated_mlp: true
default_prepend_bos: true
dtype: torch.float32
tokenizer_prepends_bos: true
n_key_value_heads: null
post_embedding_ln: false
rotary_base: 10000
trust_remote_code: true
rotary_adjacent_pairs: false
","embed:
  W_E: (152064, 5120)
blocks:
  '[0-39]':
    ln1:
      w: (5120,)
    ln2:
      w: (5120,)
    attn:
      '[W_Q, W_K, W_V]': (40, 5120, 128)
      W_O: (40, 128, 5120)
      '[b_Q, b_K, b_V]': (40, 128)
      b_O: (5120,)
      mask: (2048, 2048)
      IGNORE: ()
      '[rotary_sin, rotary_cos]': (2048, 128)
    mlp:
      '[W_in, W_gate]': (5120, 13696)
      b_in: (13696,)
      W_out: (13696, 5120)
      b_out: (5120,)
ln_final:
  w: (5120,)
unembed:
  W_U: (5120, 152064)
  b_U: (152064,)
","{'embed': {'W_E': '(152064, 5120)'}, 'blocks': {'[0-39]': {'ln1': {'w': '(5120,)'}, 'ln2': {'w': '(5120,)'}, 'attn': {'[W_Q, W_K, W_V]': '(40, 5120, 128)', 'W_O': '(40, 128, 5120)', '[b_Q, b_K, b_V]': '(40, 128)', 'b_O': '(5120,)', 'mask': '(2048, 2048)', 'IGNORE': '()', '[rotary_sin, rotary_cos]': '(2048, 128)'}, 'mlp': {'[W_in, W_gate]': '(5120, 13696)', 'b_in': '(13696,)', 'W_out': '(13696, 5120)', 'b_out': '(5120,)'}}}, 'ln_final': {'w': '(5120,)'}, 'unembed': {'W_U': '(5120, 152064)', 'b_U': '(152064,)'}}","blocks:
  '[0-39]':
    ln1:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    attn:
      '[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': (batch, seq_len,
        40, 128)
      '[hook_attn_scores, hook_pattern]': (batch, 40, seq_len, seq_len)
    ln2:
      hook_scale: (batch, seq_len, 1)
      hook_normalized: (batch, seq_len, 5120)
    mlp:
      '[hook_pre, hook_pre_linear, hook_post]': (batch, seq_len, 13696)
    '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': (batch,
      seq_len, 5120)
ln_final:
  hook_scale: (batch, seq_len, 1)
  hook_normalized: (batch, seq_len, 5120)
hook_embed: (batch, seq_len, 5120)
","{'blocks': {'[0-39]': {'ln1': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'attn': {'[hook_q, hook_k, hook_v, hook_rot_q, hook_rot_k, hook_z]': '(batch, seq_len, 40, 128)', '[hook_attn_scores, hook_pattern]': '(batch, 40, seq_len, seq_len)'}, 'ln2': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'mlp': {'[hook_pre, hook_pre_linear, hook_post]': '(batch, seq_len, 13696)'}, '[hook_resid_pre, hook_attn_out, hook_resid_mid, hook_mlp_out, hook_resid_post]': '(batch, seq_len, 5120)'}}, 'ln_final': {'hook_scale': '(batch, seq_len, 1)', 'hook_normalized': '(batch, seq_len, 5120)'}, 'hook_embed': '(batch, seq_len, 5120)'}"
