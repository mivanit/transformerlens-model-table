| name.default_alias                   | name.huggingface                                                                                                            | name.aliases                                                                                       | model_type   | name.from_cfg                          | n_params.as_str   |   n_params.as_int | n_params.from_name   |   cfg.n_params |   cfg.n_layers |   cfg.n_heads |   cfg.d_model |   cfg.d_vocab | cfg.act_fn   | cfg.positional_embedding_type   | cfg.parallel_attn_mlp   | cfg.original_architecture   | cfg.normalization_type   | tokenizer.name                              | tokenizer.vocab_size   | tokenizer.max_len               | tokenizer.class         | tokenizer.vocab_hash        |
|:-------------------------------------|:----------------------------------------------------------------------------------------------------------------------------|:---------------------------------------------------------------------------------------------------|:-------------|:---------------------------------------|:------------------|------------------:|:---------------------|---------------:|---------------:|--------------:|--------------:|--------------:|:-------------|:--------------------------------|:------------------------|:----------------------------|:-------------------------|:--------------------------------------------|:-----------------------|:--------------------------------|:------------------------|:----------------------------|
| gpt2-small                           | [gpt2](https://huggingface.co/gpt2)                                                                                         | gpt2-small                                                                                         | gpt2         | gpt2                                   | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | gpt2                                        | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| gpt2-medium                          | [gpt2-medium](https://huggingface.co/gpt2-medium)                                                                           |                                                                                                    | gpt2         | gpt2-medium                            | 302M              |         301989888 |                      |      301989888 |             24 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | gpt2-medium                                 | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| gpt2-large                           | [gpt2-large](https://huggingface.co/gpt2-large)                                                                             |                                                                                                    | gpt2         | gpt2-large                             | 708M              |         707788800 |                      |      707788800 |             36 |            20 |          1280 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | gpt2-large                                  | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| gpt2-xl                              | [gpt2-xl](https://huggingface.co/gpt2-xl)                                                                                   |                                                                                                    | gpt2         | gpt2-xl                                | 1.5B              |        1474560000 |                      |     1474560000 |             48 |            25 |          1600 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | gpt2-xl                                     | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| distillgpt2                          | [distilgpt2](https://huggingface.co/distilgpt2)                                                                             | distillgpt2, distill-gpt2, distil-gpt2, gpt2-xs                                                    | gpt2         | distilgpt2                             | 42M               |          42467328 |                      |       42467328 |              6 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | distilgpt2                                  | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| opt-125m                             | [facebook/opt-125m](https://huggingface.co/facebook/opt-125m)                                                               | opt-125m, opt-small, opt                                                                           | opt          | opt-125m                               | 85M               |          84934656 | 125m                 |       84934656 |             12 |            12 |           768 |         50272 | relu         | standard                        | False                   | OPTForCausalLM              | LN                       | facebook/opt-125m                           | 50265.0                | 1000000000000000019884624838656 | GPT2TokenizerFast       | f1FIzqnRiMYzke1CU0hp8TDxq7k |
| opt-1.3b                             | [facebook/opt-1.3b](https://huggingface.co/facebook/opt-1.3b)                                                               | opt-1.3b, opt-medium                                                                               | opt          | opt-1.3b                               | 1.2B              |        1207959552 | 1.3b                 |     1207959552 |             24 |            32 |          2048 |         50272 | relu         | standard                        | False                   | OPTForCausalLM              | LN                       | facebook/opt-1.3b                           | 50265.0                | 1000000000000000019884624838656 | GPT2TokenizerFast       | f1FIzqnRiMYzke1CU0hp8TDxq7k |
| opt-2.7b                             | [facebook/opt-2.7b](https://huggingface.co/facebook/opt-2.7b)                                                               | opt-2.7b, opt-large                                                                                | opt          | opt-2.7b                               | 2.5B              |        2516582400 | 2.7b                 |     2516582400 |             32 |            32 |          2560 |         50272 | relu         | standard                        | False                   | OPTForCausalLM              | LN                       | facebook/opt-2.7b                           | 50265.0                | 1000000000000000019884624838656 | GPT2TokenizerFast       | f1FIzqnRiMYzke1CU0hp8TDxq7k |
| opt-6.7b                             | [facebook/opt-6.7b](https://huggingface.co/facebook/opt-6.7b)                                                               | opt-6.7b, opt-xl                                                                                   | opt          | opt-6.7b                               | 6.4B              |        6442450944 | 6.7b                 |     6442450944 |             32 |            32 |          4096 |         50272 | relu         | standard                        | False                   | OPTForCausalLM              | LN                       | facebook/opt-6.7b                           | 50265.0                | 1000000000000000019884624838656 | GPT2TokenizerFast       | f1FIzqnRiMYzke1CU0hp8TDxq7k |
| opt-13b                              | [facebook/opt-13b](https://huggingface.co/facebook/opt-13b)                                                                 | opt-13b, opt-xxl                                                                                   | opt          | opt-13b                                | 13B               |       12582912000 | 13b                  |    12582912000 |             40 |            40 |          5120 |         50272 | relu         | standard                        | False                   | OPTForCausalLM              | LN                       | facebook/opt-13b                            | 50265.0                | 1000000000000000019884624838656 | GPT2TokenizerFast       | f1FIzqnRiMYzke1CU0hp8TDxq7k |
| opt-30b                              | [facebook/opt-30b](https://huggingface.co/facebook/opt-30b)                                                                 | opt-30b, opt-xxxl                                                                                  | opt          | opt-30b                                | 30B               |       29595009024 | 30b                  |    29595009024 |             48 |            56 |          7168 |         50272 | relu         | standard                        | False                   | OPTForCausalLM              | LN                       | facebook/opt-30b                            | 50265.0                | 1000000000000000019884624838656 | GPT2TokenizerFast       | f1FIzqnRiMYzke1CU0hp8TDxq7k |
| opt-66b                              | [facebook/opt-66b](https://huggingface.co/facebook/opt-66b)                                                                 | opt-66b, opt-xxxxl                                                                                 | opt          | opt-66b                                | 65B               |       65229815808 | 66b                  |    65229815808 |             64 |            72 |          9216 |         50272 | relu         | standard                        | False                   | OPTForCausalLM              | LN                       | facebook/opt-66b                            | 50265.0                | 1000000000000000019884624838656 | GPT2TokenizerFast       | f1FIzqnRiMYzke1CU0hp8TDxq7k |
| gpt-neo-125M                         |                                                                                                                             |                                                                                                    | gpt-neo      | gpt-neo-125M                           | 85M               |          84934656 | 125M                 |       84934656 |             12 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | EleutherAI/gpt-neo-125M                     | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| gpt-neo-1.3B                         |                                                                                                                             |                                                                                                    | gpt-neo      | gpt-neo-1.3B                           | 1.2B              |        1207959552 | 1.3B                 |     1207959552 |             24 |            16 |          2048 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | EleutherAI/gpt-neo-1.3B                     | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| gpt-neo-2.7B                         |                                                                                                                             |                                                                                                    | gpt-neo      | gpt-neo-2.7B                           | 2.5B              |        2516582400 | 2.7B                 |     2516582400 |             32 |            20 |          2560 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | EleutherAI/gpt-neo-2.7B                     | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| gpt-j-6B                             |                                                                                                                             |                                                                                                    | gpt-j        | gpt-j-6B                               | 5.6B              |        5637144576 | 6B                   |     5637144576 |             28 |            16 |          4096 |         50400 | gelu_new     | rotary                          | True                    | GPTJForCausalLM             | LN                       | EleutherAI/gpt-j-6B                         | 50257.0                | 2048                            | GPT2TokenizerFast       | aKfp-BCA9d3W27qknxFiS0DGC5s |
| gpt-neox-20b                         | [EleutherAI/gpt-neox-20b](https://huggingface.co/EleutherAI/gpt-neox-20b)                                                   | gpt-neox-20b, gpt-neox, neox                                                                       | gpt-neo      | gpt-neox-20b                           | 20B               |       19931332608 | 20b                  |    19931332608 |             44 |            64 |          6144 |         50432 | gelu_fast    | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| stanford-gpt2-small-a                | [stanford-crfm/alias-gpt2-small-x21](https://huggingface.co/stanford-crfm/alias-gpt2-small-x21)                             | stanford-gpt2-small-a, alias-gpt2-small-x21, gpt2-mistral-small-a, gpt2-stanford-small-a           | gpt2         | alias-gpt2-small-x21                   | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/alias-gpt2-small-x21          | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-small-b                | [stanford-crfm/battlestar-gpt2-small-x49](https://huggingface.co/stanford-crfm/battlestar-gpt2-small-x49)                   | stanford-gpt2-small-b, battlestar-gpt2-small-x49, gpt2-mistral-small-b, gpt2-mistral-small-b       | gpt2         | battlestar-gpt2-small-x49              | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/battlestar-gpt2-small-x49     | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-small-c                | [stanford-crfm/caprica-gpt2-small-x81](https://huggingface.co/stanford-crfm/caprica-gpt2-small-x81)                         | stanford-gpt2-small-c, caprica-gpt2-small-x81, gpt2-mistral-small-c, gpt2-stanford-small-c         | gpt2         | caprica-gpt2-small-x81                 | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/caprica-gpt2-small-x81        | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-small-d                | [stanford-crfm/darkmatter-gpt2-small-x343](https://huggingface.co/stanford-crfm/darkmatter-gpt2-small-x343)                 | stanford-gpt2-small-d, darkmatter-gpt2-small-x343, gpt2-mistral-small-d, gpt2-mistral-small-d      | gpt2         | darkmatter-gpt2-small-x343             | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/darkmatter-gpt2-small-x343    | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-small-e                | [stanford-crfm/expanse-gpt2-small-x777](https://huggingface.co/stanford-crfm/expanse-gpt2-small-x777)                       | stanford-gpt2-small-e, expanse-gpt2-small-x777, gpt2-mistral-small-e, gpt2-mistral-small-e         | gpt2         | expanse-gpt2-small-x777                | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/expanse-gpt2-small-x777       | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-medium-a               | [stanford-crfm/arwen-gpt2-medium-x21](https://huggingface.co/stanford-crfm/arwen-gpt2-medium-x21)                           | stanford-gpt2-medium-a, arwen-gpt2-medium-x21, gpt2-medium-small-a, gpt2-stanford-medium-a         | gpt2         | arwen-gpt2-medium-x21                  | 302M              |         301989888 |                      |      301989888 |             24 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/arwen-gpt2-medium-x21         | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-medium-b               | [stanford-crfm/beren-gpt2-medium-x49](https://huggingface.co/stanford-crfm/beren-gpt2-medium-x49)                           | stanford-gpt2-medium-b, beren-gpt2-medium-x49, gpt2-medium-small-b, gpt2-stanford-medium-b         | gpt2         | beren-gpt2-medium-x49                  | 302M              |         301989888 |                      |      301989888 |             24 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/beren-gpt2-medium-x49         | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-medium-c               | [stanford-crfm/celebrimbor-gpt2-medium-x81](https://huggingface.co/stanford-crfm/celebrimbor-gpt2-medium-x81)               | stanford-gpt2-medium-c, celebrimbor-gpt2-medium-x81, gpt2-medium-small-c, gpt2-medium-small-c      | gpt2         | celebrimbor-gpt2-medium-x81            | 302M              |         301989888 |                      |      301989888 |             24 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/celebrimbor-gpt2-medium-x81   | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-medium-d               | [stanford-crfm/durin-gpt2-medium-x343](https://huggingface.co/stanford-crfm/durin-gpt2-medium-x343)                         | stanford-gpt2-medium-d, durin-gpt2-medium-x343, gpt2-medium-small-d, gpt2-stanford-medium-d        | gpt2         | durin-gpt2-medium-x343                 | 302M              |         301989888 |                      |      301989888 |             24 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/durin-gpt2-medium-x343        | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stanford-gpt2-medium-e               | [stanford-crfm/eowyn-gpt2-medium-x777](https://huggingface.co/stanford-crfm/eowyn-gpt2-medium-x777)                         | stanford-gpt2-medium-e, eowyn-gpt2-medium-x777, gpt2-medium-small-e, gpt2-stanford-medium-e        | gpt2         | eowyn-gpt2-medium-x777                 | 302M              |         301989888 |                      |      301989888 |             24 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | stanford-crfm/eowyn-gpt2-medium-x777        | 50257.0                | 1024                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| pythia-14m                           | [EleutherAI/pythia-14m](https://huggingface.co/EleutherAI/pythia-14m)                                                       | pythia-14m                                                                                         | pythia       | pythia-14m                             | 1.2M              |           1179648 | 14m                  |        1179648 |              6 |             4 |           128 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-14m                       | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-31m                           | [EleutherAI/pythia-31m](https://huggingface.co/EleutherAI/pythia-31m)                                                       | pythia-31m                                                                                         | pythia       | pythia-31m                             | 4.7M              |           4718592 | 31m                  |        4718592 |              6 |             8 |           256 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-31m                       | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-70m                           | [EleutherAI/pythia-70m](https://huggingface.co/EleutherAI/pythia-70m)                                                       | pythia-70m, pythia, EleutherAI/pythia-19m, pythia-19m                                              | pythia       | pythia-70m                             | 19M               |          18874368 | 70m                  |       18874368 |              6 |             8 |           512 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-70m                       | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-160m                          | [EleutherAI/pythia-160m](https://huggingface.co/EleutherAI/pythia-160m)                                                     | pythia-160m, EleutherAI/pythia-125m, pythia-125m                                                   | pythia       | pythia-160m                            | 85M               |          84934656 | 160m                 |       84934656 |             12 |            12 |           768 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-160m                      | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-410m                          | [EleutherAI/pythia-410m](https://huggingface.co/EleutherAI/pythia-410m)                                                     | pythia-410m, EleutherAI/pythia-350m, pythia-350m                                                   | pythia       | pythia-410m                            | 302M              |         301989888 | 410m                 |      301989888 |             24 |            16 |          1024 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-410m                      | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1b                            | [EleutherAI/pythia-1b](https://huggingface.co/EleutherAI/pythia-1b)                                                         | pythia-1b, EleutherAI/pythia-800m, pythia-800m                                                     | pythia       | pythia-1b                              | 805M              |         805306368 | 1b                   |      805306368 |             16 |             8 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1b                        | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1.4b                          | [EleutherAI/pythia-1.4b](https://huggingface.co/EleutherAI/pythia-1.4b)                                                     | pythia-1.4b, EleutherAI/pythia-1.3b, pythia-1.3b                                                   | pythia       | pythia-1.4b                            | 1.2B              |        1207959552 | 1.4b                 |     1207959552 |             24 |            16 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1.4b                      | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-2.8b                          | [EleutherAI/pythia-2.8b](https://huggingface.co/EleutherAI/pythia-2.8b)                                                     | pythia-2.8b, EleutherAI/pythia-2.7b, pythia-2.7b                                                   | pythia       | pythia-2.8b                            | 2.5B              |        2516582400 | 2.8b                 |     2516582400 |             32 |            32 |          2560 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-2.8b                      | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-6.9b                          | [EleutherAI/pythia-6.9b](https://huggingface.co/EleutherAI/pythia-6.9b)                                                     | pythia-6.9b, EleutherAI/pythia-6.7b, pythia-6.7b                                                   | pythia       | pythia-6.9b                            | 6.4B              |        6442450944 | 6.9b                 |     6442450944 |             32 |            32 |          4096 |         50432 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-6.9b                      | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-12b                           | [EleutherAI/pythia-12b](https://huggingface.co/EleutherAI/pythia-12b)                                                       | pythia-12b, EleutherAI/pythia-13b, pythia-13b                                                      | pythia       | pythia-12b                             | 11B               |       11324620800 | 12b                  |    11324620800 |             36 |            40 |          5120 |         50688 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-12b                       | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-70m-deduped                   | [EleutherAI/pythia-70m-deduped](https://huggingface.co/EleutherAI/pythia-70m-deduped)                                       | pythia-70m-deduped, EleutherAI/pythia-19m-deduped, pythia-19m-deduped                              | pythia       | pythia-70m-deduped                     | 19M               |          18874368 | 70m                  |       18874368 |              6 |             8 |           512 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-70m-deduped               | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-160m-deduped                  | [EleutherAI/pythia-160m-deduped](https://huggingface.co/EleutherAI/pythia-160m-deduped)                                     | pythia-160m-deduped, EleutherAI/pythia-125m-deduped, pythia-125m-deduped                           | pythia       | pythia-160m-deduped                    | 85M               |          84934656 | 160m                 |       84934656 |             12 |            12 |           768 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-160m-deduped              | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-410m-deduped                  | [EleutherAI/pythia-410m-deduped](https://huggingface.co/EleutherAI/pythia-410m-deduped)                                     | pythia-410m-deduped, EleutherAI/pythia-350m-deduped, pythia-350m-deduped                           | pythia       | pythia-410m-deduped                    | 302M              |         301989888 | 410m                 |      301989888 |             24 |            16 |          1024 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-410m-deduped              | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1b-deduped                    | [EleutherAI/pythia-1b-deduped](https://huggingface.co/EleutherAI/pythia-1b-deduped)                                         | pythia-1b-deduped, EleutherAI/pythia-800m-deduped, pythia-800m-deduped                             | pythia       | pythia-1b-deduped                      | 805M              |         805306368 | 1b                   |      805306368 |             16 |             8 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1b-deduped                | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1.4b-deduped                  | [EleutherAI/pythia-1.4b-deduped](https://huggingface.co/EleutherAI/pythia-1.4b-deduped)                                     | pythia-1.4b-deduped, EleutherAI/pythia-1.3b-deduped, pythia-1.3b-deduped                           | pythia       | pythia-1.4b-deduped                    | 1.2B              |        1207959552 | 1.4b                 |     1207959552 |             24 |            16 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1.4b-deduped              | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-2.8b-deduped                  | [EleutherAI/pythia-2.8b-deduped](https://huggingface.co/EleutherAI/pythia-2.8b-deduped)                                     | pythia-2.8b-deduped, EleutherAI/pythia-2.7b-deduped, pythia-2.7b-deduped                           | pythia       | pythia-2.8b-deduped                    | 2.5B              |        2516582400 | 2.8b                 |     2516582400 |             32 |            32 |          2560 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-2.8b-deduped              | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-6.9b-deduped                  | [EleutherAI/pythia-6.9b-deduped](https://huggingface.co/EleutherAI/pythia-6.9b-deduped)                                     | pythia-6.9b-deduped, EleutherAI/pythia-6.7b-deduped, pythia-6.7b-deduped                           | pythia       | pythia-6.9b-deduped                    | 6.4B              |        6442450944 | 6.9b                 |     6442450944 |             32 |            32 |          4096 |         50432 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-6.9b-deduped              | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-12b-deduped                   | [EleutherAI/pythia-12b-deduped](https://huggingface.co/EleutherAI/pythia-12b-deduped)                                       | pythia-12b-deduped, EleutherAI/pythia-13b-deduped, pythia-13b-deduped                              | pythia       | pythia-12b-deduped                     | 11B               |       11324620800 | 12b                  |    11324620800 |             36 |            40 |          5120 |         50688 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-12b-deduped               | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-70m-v0                        | [EleutherAI/pythia-70m-v0](https://huggingface.co/EleutherAI/pythia-70m-v0)                                                 | pythia-70m-v0, pythia-v0, EleutherAI/pythia-19m-v0, pythia-19m-v0                                  | pythia       | pythia-70m-v0                          | 19M               |          18874368 | 70m                  |       18874368 |              6 |             8 |           512 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-70m-v0                    | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-160m-v0                       | [EleutherAI/pythia-160m-v0](https://huggingface.co/EleutherAI/pythia-160m-v0)                                               | pythia-160m-v0, EleutherAI/pythia-125m-v0, pythia-125m-v0                                          | pythia       | pythia-160m-v0                         | 85M               |          84934656 | 160m                 |       84934656 |             12 |            12 |           768 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-160m-v0                   | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-410m-v0                       | [EleutherAI/pythia-410m-v0](https://huggingface.co/EleutherAI/pythia-410m-v0)                                               | pythia-410m-v0, EleutherAI/pythia-350m-v0, pythia-350m-v0                                          | pythia       | pythia-410m-v0                         | 302M              |         301989888 | 410m                 |      301989888 |             24 |            16 |          1024 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-410m-v0                   | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1b-v0                         | [EleutherAI/pythia-1b-v0](https://huggingface.co/EleutherAI/pythia-1b-v0)                                                   | pythia-1b-v0, EleutherAI/pythia-800m-v0, pythia-800m-v0                                            | pythia       | pythia-1b-v0                           | 805M              |         805306368 | 1b                   |      805306368 |             16 |             8 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1b-v0                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1.4b-v0                       | [EleutherAI/pythia-1.4b-v0](https://huggingface.co/EleutherAI/pythia-1.4b-v0)                                               | pythia-1.4b-v0, EleutherAI/pythia-1.3b-v0, pythia-1.3b-v0                                          | pythia       | pythia-1.4b-v0                         | 1.2B              |        1207959552 | 1.4b                 |     1207959552 |             24 |            16 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1.4b-v0                   | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-2.8b-v0                       | [EleutherAI/pythia-2.8b-v0](https://huggingface.co/EleutherAI/pythia-2.8b-v0)                                               | pythia-2.8b-v0, EleutherAI/pythia-2.7b-v0, pythia-2.7b-v0                                          | pythia       | pythia-2.8b-v0                         | 2.5B              |        2516582400 | 2.8b                 |     2516582400 |             32 |            32 |          2560 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-2.8b-v0                   | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-6.9b-v0                       | [EleutherAI/pythia-6.9b-v0](https://huggingface.co/EleutherAI/pythia-6.9b-v0)                                               | pythia-6.9b-v0, EleutherAI/pythia-6.7b-v0, pythia-6.7b-v0                                          | pythia       | pythia-6.9b-v0                         | 6.4B              |        6442450944 | 6.9b                 |     6442450944 |             32 |            32 |          4096 |         50432 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-6.9b-v0                   | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-12b-v0                        | [EleutherAI/pythia-12b-v0](https://huggingface.co/EleutherAI/pythia-12b-v0)                                                 | pythia-12b-v0, EleutherAI/pythia-13b-v0, pythia-13b-v0                                             | pythia       | pythia-12b-v0                          | 11B               |       11324620800 | 12b                  |    11324620800 |             36 |            40 |          5120 |         50688 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-12b-v0                    | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-70m-deduped-v0                | [EleutherAI/pythia-70m-deduped-v0](https://huggingface.co/EleutherAI/pythia-70m-deduped-v0)                                 | pythia-70m-deduped-v0, EleutherAI/pythia-19m-deduped-v0, pythia-19m-deduped-v0                     | pythia       | pythia-70m-deduped-v0                  | 19M               |          18874368 | 70m                  |       18874368 |              6 |             8 |           512 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-70m-deduped-v0            | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-160m-deduped-v0               | [EleutherAI/pythia-160m-deduped-v0](https://huggingface.co/EleutherAI/pythia-160m-deduped-v0)                               | pythia-160m-deduped-v0, EleutherAI/pythia-125m-deduped-v0, pythia-125m-deduped-v0                  | pythia       | pythia-160m-deduped-v0                 | 85M               |          84934656 | 160m                 |       84934656 |             12 |            12 |           768 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-160m-deduped-v0           | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-410m-deduped-v0               | [EleutherAI/pythia-410m-deduped-v0](https://huggingface.co/EleutherAI/pythia-410m-deduped-v0)                               | pythia-410m-deduped-v0, EleutherAI/pythia-350m-deduped-v0, pythia-350m-deduped-v0                  | pythia       | pythia-410m-deduped-v0                 | 302M              |         301989888 | 410m                 |      301989888 |             24 |            16 |          1024 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-410m-deduped-v0           | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1b-deduped-v0                 | [EleutherAI/pythia-1b-deduped-v0](https://huggingface.co/EleutherAI/pythia-1b-deduped-v0)                                   | pythia-1b-deduped-v0, EleutherAI/pythia-800m-deduped-v0, pythia-800m-deduped-v0                    | pythia       | pythia-1b-deduped-v0                   | 805M              |         805306368 | 1b                   |      805306368 |             16 |             8 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1b-deduped-v0             | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-1.4b-deduped-v0               | [EleutherAI/pythia-1.4b-deduped-v0](https://huggingface.co/EleutherAI/pythia-1.4b-deduped-v0)                               | pythia-1.4b-deduped-v0, EleutherAI/pythia-1.3b-deduped-v0, pythia-1.3b-deduped-v0                  | pythia       | pythia-1.4b-deduped-v0                 | 1.2B              |        1207959552 | 1.4b                 |     1207959552 |             24 |            16 |          2048 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-1.4b-deduped-v0           | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-2.8b-deduped-v0               | [EleutherAI/pythia-2.8b-deduped-v0](https://huggingface.co/EleutherAI/pythia-2.8b-deduped-v0)                               | pythia-2.8b-deduped-v0, EleutherAI/pythia-2.7b-deduped-v0, pythia-2.7b-deduped-v0                  | pythia       | pythia-2.8b-deduped-v0                 | 2.5B              |        2516582400 | 2.8b                 |     2516582400 |             32 |            32 |          2560 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-2.8b-deduped-v0           | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-6.9b-deduped-v0               | [EleutherAI/pythia-6.9b-deduped-v0](https://huggingface.co/EleutherAI/pythia-6.9b-deduped-v0)                               | pythia-6.9b-deduped-v0, EleutherAI/pythia-6.7b-deduped-v0, pythia-6.7b-deduped-v0                  | pythia       | pythia-6.9b-deduped-v0                 | 6.4B              |        6442450944 | 6.9b                 |     6442450944 |             32 |            32 |          4096 |         50432 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-6.9b-deduped-v0           | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-12b-deduped-v0                | [EleutherAI/pythia-12b-deduped-v0](https://huggingface.co/EleutherAI/pythia-12b-deduped-v0)                                 | pythia-12b-deduped-v0, EleutherAI/pythia-13b-deduped-v0, pythia-13b-deduped-v0                     | pythia       | pythia-12b-deduped-v0                  | 11B               |       11324620800 | 12b                  |    11324620800 |             36 |            40 |          5120 |         50688 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-12b-deduped-v0            | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-160m-seed1                    | [EleutherAI/pythia-160m-seed1](https://huggingface.co/EleutherAI/pythia-160m-seed1)                                         | pythia-160m-seed1, EleutherAI/pythia-125m-seed1, pythia-125m-seed1                                 | pythia       | pythia-160m-seed1                      | 85M               |          84934656 | 160m                 |       84934656 |             12 |            12 |           768 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-160m-seed1                | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-160m-seed2                    | [EleutherAI/pythia-160m-seed2](https://huggingface.co/EleutherAI/pythia-160m-seed2)                                         | pythia-160m-seed2, EleutherAI/pythia-125m-seed2, pythia-125m-seed2                                 | pythia       | pythia-160m-seed2                      | 85M               |          84934656 | 160m                 |       84934656 |             12 |            12 |           768 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-160m-seed2                | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| pythia-160m-seed3                    | [EleutherAI/pythia-160m-seed3](https://huggingface.co/EleutherAI/pythia-160m-seed3)                                         | pythia-160m-seed3, EleutherAI/pythia-125m-seed3, pythia-125m-seed3                                 | pythia       | pythia-160m-seed3                      | 85M               |          84934656 | 160m                 |       84934656 |             12 |            12 |           768 |         50304 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | EleutherAI/pythia-160m-seed3                | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-1l-pile                         | [NeelNanda/SoLU_1L_v9_old](https://huggingface.co/NeelNanda/SoLU_1L_v9_old)                                                 | solu-1l-pile, solu-1l-old                                                                          | solu         | SoLU_1L_v9_old                         | 13M               |          12582912 |                      |       12582912 |              1 |            16 |          1024 |         50278 | solu_ln      | standard                        | False                   | neel-solu-old               | LN                       | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-2l-pile                         | [NeelNanda/SoLU_2L_v10_old](https://huggingface.co/NeelNanda/SoLU_2L_v10_old)                                               | solu-2l-pile, solu-2l-old                                                                          | solu         | SoLU_2L_v10_old                        | 13M               |          12812288 |                      |       12812288 |              2 |            11 |           736 |         50278 | solu_ln      | standard                        | False                   | neel-solu-old               | LNPre                    | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-4l-pile                         | [NeelNanda/SoLU_4L_v11_old](https://huggingface.co/NeelNanda/SoLU_4L_v11_old)                                               | solu-4l-pile, solu-4l-old                                                                          | solu         | SoLU_4L_v11_old                        | 13M               |          12582912 |                      |       12582912 |              4 |             8 |           512 |         50278 | solu_ln      | standard                        | False                   | neel-solu-old               | LNPre                    | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-6l-pile                         | [NeelNanda/SoLU_6L_v13_old](https://huggingface.co/NeelNanda/SoLU_6L_v13_old)                                               | solu-6l-pile, solu-6l-old                                                                          | solu         | SoLU_6L_v13_old                        | 42M               |          42467328 |                      |       42467328 |              6 |            12 |           768 |         50278 | solu_ln      | standard                        | False                   | neel-solu-old               | LNPre                    | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-8l-pile                         | [NeelNanda/SoLU_8L_v21_old](https://huggingface.co/NeelNanda/SoLU_8L_v21_old)                                               | solu-8l-pile, solu-8l-old                                                                          | solu         | SoLU_8L_v21_old                        | 101M              |         100663296 |                      |      100663296 |              8 |            16 |          1024 |         50278 | solu_ln      | standard                        | False                   | neel-solu-old               | LNPre                    | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-10l-pile                        | [NeelNanda/SoLU_10L_v22_old](https://huggingface.co/NeelNanda/SoLU_10L_v22_old)                                             | solu-10l-pile, solu-10l-old                                                                        | solu         | SoLU_10L_v22_old                       | 197M              |         196608000 |                      |      196608000 |             10 |            20 |          1280 |         50278 | solu_ln      | standard                        | False                   | neel-solu-old               | LNPre                    | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-12l-pile                        | [NeelNanda/SoLU_12L_v23_old](https://huggingface.co/NeelNanda/SoLU_12L_v23_old)                                             | solu-12l-pile, solu-12l-old                                                                        | solu         | SoLU_12L_v23_old                       | 340M              |         339738624 |                      |      339738624 |             12 |            24 |          1536 |         50278 | solu_ln      | standard                        | False                   | neel-solu-old               | LN                       | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-1l                              | [NeelNanda/SoLU_1L512W_C4_Code](https://huggingface.co/NeelNanda/SoLU_1L512W_C4_Code)                                       | solu-1l, solu-1l-new, solu-1l-c4-code                                                              | solu         | SoLU_1L512W_C4_Code                    | 3.1M              |           3145728 |                      |        3145728 |              1 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-2l                              | [NeelNanda/SoLU_2L512W_C4_Code](https://huggingface.co/NeelNanda/SoLU_2L512W_C4_Code)                                       | solu-2l, solu-2l-new, solu-2l-c4-code                                                              | solu         | SoLU_2L512W_C4_Code                    | 6.3M              |           6291456 |                      |        6291456 |              2 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-3l                              | [NeelNanda/SoLU_3L512W_C4_Code](https://huggingface.co/NeelNanda/SoLU_3L512W_C4_Code)                                       | solu-3l, solu-3l-new, solu-3l-c4-code                                                              | solu         | SoLU_3L512W_C4_Code                    | 9.4M              |           9437184 |                      |        9437184 |              3 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-4l                              | [NeelNanda/SoLU_4L512W_C4_Code](https://huggingface.co/NeelNanda/SoLU_4L512W_C4_Code)                                       | solu-4l, solu-4l-new, solu-4l-c4-code                                                              | solu         | SoLU_4L512W_C4_Code                    | 13M               |          12582912 |                      |       12582912 |              4 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-6l                              | [NeelNanda/SoLU_6L768W_C4_Code](https://huggingface.co/NeelNanda/SoLU_6L768W_C4_Code)                                       | solu-6l, solu-6l-new, solu-6l-c4-code                                                              | solu         | SoLU_6L768W_C4_Code                    | 42M               |          42467328 |                      |       42467328 |              6 |            12 |           768 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-8l                              | [NeelNanda/SoLU_8L1024W_C4_Code](https://huggingface.co/NeelNanda/SoLU_8L1024W_C4_Code)                                     | solu-8l, solu-8l-new, solu-8l-c4-code                                                              | solu         | SoLU_8L1024W_C4_Code                   | 101M              |         100663296 |                      |      100663296 |              8 |            16 |          1024 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-10l                             | [NeelNanda/SoLU_10L1280W_C4_Code](https://huggingface.co/NeelNanda/SoLU_10L1280W_C4_Code)                                   | solu-10l, solu-10l-new, solu-10l-c4-code                                                           | solu         | SoLU_10L1280W_C4_Code                  | 197M              |         196608000 |                      |      196608000 |             10 |            20 |          1280 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-12l                             | [NeelNanda/SoLU_12L1536W_C4_Code](https://huggingface.co/NeelNanda/SoLU_12L1536W_C4_Code)                                   | solu-12l, solu-12l-new, solu-12l-c4-code                                                           | solu         | SoLU_12L1536W_C4_Code                  | 340M              |         339738624 |                      |      339738624 |             12 |            24 |          1536 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| gelu-1l                              | [NeelNanda/GELU_1L512W_C4_Code](https://huggingface.co/NeelNanda/GELU_1L512W_C4_Code)                                       | gelu-1l, gelu-1l-new, gelu-1l-c4-code                                                              | gelu         | GELU_1L512W_C4_Code                    | 3.1M              |           3145728 |                      |        3145728 |              1 |             8 |           512 |         48262 | gelu         | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| gelu-2l                              | [NeelNanda/GELU_2L512W_C4_Code](https://huggingface.co/NeelNanda/GELU_2L512W_C4_Code)                                       | gelu-2l, gelu-2l-new, gelu-2l-c4-code                                                              | gelu         | GELU_2L512W_C4_Code                    | 6.3M              |           6291456 |                      |        6291456 |              2 |             8 |           512 |         48262 | gelu         | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| gelu-3l                              | [NeelNanda/GELU_3L512W_C4_Code](https://huggingface.co/NeelNanda/GELU_3L512W_C4_Code)                                       | gelu-3l, gelu-3l-new, gelu-3l-c4-code                                                              | gelu         | GELU_3L512W_C4_Code                    | 9.4M              |           9437184 |                      |        9437184 |              3 |             8 |           512 |         48262 | gelu         | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| gelu-4l                              | [NeelNanda/GELU_4L512W_C4_Code](https://huggingface.co/NeelNanda/GELU_4L512W_C4_Code)                                       | gelu-4l, gelu-4l-new, gelu-4l-c4-code                                                              | gelu         | GELU_4L512W_C4_Code                    | 13M               |          12582912 |                      |       12582912 |              4 |             8 |           512 |         48262 | gelu         | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| attn-only-1l                         | [NeelNanda/Attn_Only_1L512W_C4_Code](https://huggingface.co/NeelNanda/Attn_Only_1L512W_C4_Code)                             | attn-only-1l, attn-only-1l-new, attn-only-1l-c4-code                                               | attn-only    | Attn_Only_1L512W_C4_Code               | 1.0M              |           1048576 |                      |        1048576 |              1 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| attn-only-2l                         | [NeelNanda/Attn_Only_2L512W_C4_Code](https://huggingface.co/NeelNanda/Attn_Only_2L512W_C4_Code)                             | attn-only-2l, attn-only-2l-new, attn-only-2l-c4-code                                               | attn-only    | Attn_Only_2L512W_C4_Code               | 2.1M              |           2097152 |                      |        2097152 |              2 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| attn-only-3l                         | [NeelNanda/Attn_Only_3L512W_C4_Code](https://huggingface.co/NeelNanda/Attn_Only_3L512W_C4_Code)                             | attn-only-3l, attn-only-3l-new, attn-only-3l-c4-code                                               | attn-only    | Attn_Only_3L512W_C4_Code               | 3.1M              |           3145728 |                      |        3145728 |              3 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| attn-only-4l                         | [NeelNanda/Attn_Only_4L512W_C4_Code](https://huggingface.co/NeelNanda/Attn_Only_4L512W_C4_Code)                             | attn-only-4l, attn-only-4l-new, attn-only-4l-c4-code                                               | attn-only    | Attn_Only_4L512W_C4_Code               | 4.2M              |           4194304 |                      |        4194304 |              4 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| attn-only-2l-demo                    | [NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr](https://huggingface.co/NeelNanda/Attn-Only-2L512W-Shortformer-6B-big-lr) | attn-only-2l-demo, attn-only-2l-shortformer-6b-big-lr, attn-only-2l-induction-demo, attn-only-demo | attn-only    | Attn-Only-2L512W-Shortformer-6B-big-lr | 2.1M              |           2097152 |                      |        2097152 |              2 |             8 |           512 |         50277 | solu_ln      | shortformer                     | False                   | neel                        |                          | EleutherAI/gpt-neox-20b                     | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| solu-1l-wiki                         | [NeelNanda/SoLU_1L512W_Wiki_Finetune](https://huggingface.co/NeelNanda/SoLU_1L512W_Wiki_Finetune)                           | solu-1l-wiki, solu-1l-wiki-finetune, solu-1l-finetune                                              | solu         | SoLU_1L512W_Wiki_Finetune              | 3.1M              |           3145728 |                      |        3145728 |              1 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| solu-4l-wiki                         | [NeelNanda/SoLU_4L512W_Wiki_Finetune](https://huggingface.co/NeelNanda/SoLU_4L512W_Wiki_Finetune)                           | solu-4l-wiki, solu-4l-wiki-finetune, solu-4l-finetune                                              | solu         | SoLU_4L512W_Wiki_Finetune              | 13M               |          12582912 |                      |       12582912 |              4 |             8 |           512 |         48262 | solu_ln      | standard                        | False                   | neel                        | LN                       | NeelNanda/gpt-neox-tokenizer-digits         | 48262.0                | 1000000000000000019884624838656 | PreTrainedTokenizerFast | AsGo9tS8Sq4-rlVHM2o3-GyDkJU |
| redwood_attn_2l                      | [ArthurConmy/redwood_attn_2l](https://huggingface.co/ArthurConmy/redwood_attn_2l)                                           | redwood_attn_2l                                                                                    |              | redwood_attn_2l                        | 524K              |            524288 |                      |         524288 |              2 |             8 |           256 |         50259 | gelu_new     | shortformer                     | False                   | neel                        | LN                       | ArthurConmy/redwood_tokenizer               | 50257.0                | 1024                            | GPT2TokenizerFast       | J8auoAiqFanHN7mOtkTrFA9voRk |
| llama-7b                             | [llama-7b-hf](https://huggingface.co/llama-7b-hf)                                                                           | llama-7b                                                                                           | llama        | llama-7b-hf                            | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| llama-13b                            | [llama-13b-hf](https://huggingface.co/llama-13b-hf)                                                                         | llama-13b                                                                                          | llama        | llama-13b-hf                           | 13B               |       12687769600 | 13b                  |    12687769600 |             40 |            40 |          5120 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| llama-30b                            | [llama-30b-hf](https://huggingface.co/llama-30b-hf)                                                                         | llama-30b                                                                                          | llama        | llama-30b-hf                           | 32B               |       32102154240 | 30b                  |    32102154240 |             60 |            52 |          6656 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| llama-65b                            | [llama-65b-hf](https://huggingface.co/llama-65b-hf)                                                                         | llama-65b                                                                                          | llama        | llama-65b-hf                           | 65B               |       64760053760 | 65b                  |    64760053760 |             80 |            64 |          8192 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| Llama-2-7b                           |                                                                                                                             |                                                                                                    | Llama-2      | Llama-2-7b-hf                          | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| Llama-2-7b-chat                      |                                                                                                                             |                                                                                                    | Llama-2      | Llama-2-7b-chat-hf                     | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| Llama-2-13b                          |                                                                                                                             |                                                                                                    | Llama-2      | Llama-2-13b-hf                         | 13B               |       12687769600 | 13b                  |    12687769600 |             40 |            40 |          5120 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| Llama-2-13b-chat                     |                                                                                                                             |                                                                                                    | Llama-2      | Llama-2-13b-chat-hf                    | 13B               |       12687769600 | 13b                  |    12687769600 |             40 |            40 |          5120 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| Llama-2-70b-chat                     |                                                                                                                             |                                                                                                    | Llama-2      | Llama-2-70b-chat-hf                    | 78B               |       77846282240 | 70b                  |    77846282240 |             80 |            64 |          8192 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| CodeLlamallama-2-7b                  |                                                                                                                             |                                                                                                    | llama        | CodeLlama-7b-hf                        | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |         32016 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| CodeLlama-7b-python                  |                                                                                                                             |                                                                                                    | CodeLlama    | CodeLlama-7b-Python-hf                 | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| CodeLlama-7b-instruct                |                                                                                                                             |                                                                                                    | CodeLlama    | CodeLlama-7b-Instruct-hf               | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |         32016 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| meta-llama/Meta-Llama-3-8B           |                                                                                                                             |                                                                                                    | llama        | Meta-Llama-3-8B                        | 7.8B              |        7784628224 | 8B                   |     7784628224 |             32 |            32 |          4096 |        128256 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| meta-llama/Meta-Llama-3-8B-Instruct  |                                                                                                                             |                                                                                                    | llama        | Meta-Llama-3-8B-Instruct               | 7.8B              |        7784628224 | 8B                   |     7784628224 |             32 |            32 |          4096 |        128256 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| meta-llama/Meta-Llama-3-70B          |                                                                                                                             |                                                                                                    | llama        | Meta-Llama-3-70B                       | 78B               |       77846282240 | 70B                  |    77846282240 |             80 |            64 |          8192 |        128256 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| meta-llama/Meta-Llama-3-70B-Instruct |                                                                                                                             |                                                                                                    | llama        | Meta-Llama-3-70B-Instruct              | 78B               |       77846282240 | 70B                  |    77846282240 |             80 |            64 |          8192 |        128256 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      |                                             |                        |                                 |                         |                             |
| othello-gpt                          | [Baidicoot/Othello-GPT-Transformer-Lens](https://huggingface.co/Baidicoot/Othello-GPT-Transformer-Lens)                     | othello-gpt                                                                                        |              | Othello-GPT-Transformer-Lens           | 25M               |          25165824 |                      |       25165824 |              8 |             8 |           512 |            61 | gelu         | standard                        | False                   | mingpt                      | LN                       |                                             |                        |                                 |                         |                             |
| bert-base-cased                      | [bert-base-cased](https://huggingface.co/bert-base-cased)                                                                   |                                                                                                    | bert         | bert-base-cased                        | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         28996 | gelu         | standard                        | False                   | BertForMaskedLM             | LN                       | bert-base-cased                             | 28996.0                | 512                             | BertTokenizerFast       | SSKvHuFYtPbvgwMSLSIhfFE_kF8 |
| tiny-stories-1M                      |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-1M                         | 393K              |            393216 | 1M                   |         393216 |              8 |            16 |            64 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-1M                   | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-3M                      |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-3M                         | 1.6M              |           1572864 | 3M                   |        1572864 |              8 |            16 |           128 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-3M                   | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-8M                      |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-8M                         | 6.3M              |           6291456 | 8M                   |        6291456 |              8 |            16 |           256 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-8M                   | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-28M                     |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-28M                        | 25M               |          25165824 | 28M                  |       25165824 |              8 |            16 |           512 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-28M                  | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-33M                     |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-33M                        | 28M               |          28311552 | 33M                  |       28311552 |              4 |            16 |           768 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-33M                  | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-instruct-1M             |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-Instruct-1M                | 393K              |            393216 | 1M                   |         393216 |              8 |            16 |            64 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-Instruct-1M          | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-instruct-3M             |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-Instruct-3M                | 1.6M              |           1572864 | 3M                   |        1572864 |              8 |            16 |           128 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-Instruct-3M          | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-instruct-8M             |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-Instruct-8M                | 6.3M              |           6291456 | 8M                   |        6291456 |              8 |            16 |           256 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-Instruct-8M          | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-instruct-28M            |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-Instruct-28M               | 25M               |          25165824 | 28M                  |       25165824 |              8 |            16 |           512 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-Instruct-28M         | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-instruct-33M            |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-Instruct-33M               | 28M               |          28311552 | 33M                  |       28311552 |              4 |            16 |           768 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-Instruct-33M         | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-1L-21M                  |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-1Layer-21M                 | 13M               |          12582912 | 21M                  |       12582912 |              1 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-1Layer-21M           | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-2L-33M                  |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-2Layers-33M                | 25M               |          25165824 | 33M                  |       25165824 |              2 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-2Layers-33M          | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-instruct-1L-21M         |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-Instuct-1Layer-21M         | 13M               |          12582912 | 21M                  |       12582912 |              1 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-Instuct-1Layer-21M   | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| tiny-stories-instruct-2L-33M         |                                                                                                                             |                                                                                                    | tiny-stories | TinyStories-Instruct-2Layers-33M       | 25M               |          25165824 | 33M                  |       25165824 |              2 |            16 |          1024 |         50257 | gelu_new     | standard                        | False                   | GPTNeoForCausalLM           | LN                       | roneneldan/TinyStories-Instruct-2Layers-33M | 50257.0                | 2048                            | GPT2TokenizerFast       | v8xfIj5kwZX5RwgLU66lZNZUlE4 |
| stablelm-base-alpha-3b               | [stabilityai/stablelm-base-alpha-3b](https://huggingface.co/stabilityai/stablelm-base-alpha-3b)                             | stablelm-base-alpha-3b, stablelm-base-3b                                                           | stablelm     | stablelm-base-alpha-3b                 | 3.2B              |        3221225472 | 3b                   |     3221225472 |             16 |            32 |          4096 |         50688 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | stabilityai/stablelm-base-alpha-3b          | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| stablelm-base-alpha-7b               | [stabilityai/stablelm-base-alpha-7b](https://huggingface.co/stabilityai/stablelm-base-alpha-7b)                             | stablelm-base-alpha-7b, stablelm-base-7b                                                           | stablelm     | stablelm-base-alpha-7b                 | 7.2B              |        7247757312 | 7b                   |     7247757312 |             16 |            48 |          6144 |         50432 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | stabilityai/stablelm-base-alpha-7b          | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | 96EawM8Lij99W7OBTk0KW2ELUrQ |
| stablelm-tuned-alpha-3b              | [stabilityai/stablelm-tuned-alpha-3b](https://huggingface.co/stabilityai/stablelm-tuned-alpha-3b)                           | stablelm-tuned-alpha-3b, stablelm-tuned-3b                                                         | stablelm     | stablelm-tuned-alpha-3b                | 3.2B              |        3221225472 | 3b                   |     3221225472 |             16 |            32 |          4096 |         50688 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | stabilityai/stablelm-tuned-alpha-3b         | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | RD3vcWSd_TiTpqo5dHyICzaXtGQ |
| stablelm-tuned-alpha-7b              | [stabilityai/stablelm-tuned-alpha-7b](https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b)                           | stablelm-tuned-alpha-7b, stablelm-tuned-7b                                                         | stablelm     | stablelm-tuned-alpha-7b                | 7.2B              |        7247757312 | 7b                   |     7247757312 |             16 |            48 |          6144 |         50432 | gelu         | rotary                          | True                    | GPTNeoXForCausalLM          | LN                       | stabilityai/stablelm-tuned-alpha-7b         | 50254.0                | 1000000000000000019884624838656 | GPTNeoXTokenizerFast    | RD3vcWSd_TiTpqo5dHyICzaXtGQ |
| mistral-7b                           | [mistralai/Mistral-7B-v0.1](https://huggingface.co/mistralai/Mistral-7B-v0.1)                                               | mistral-7b                                                                                         | mistral      | Mistral-7B-v0.1                        | 7.8B              |        7784628224 | 7b                   |     7784628224 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | MistralForCausalLM          | RMS                      | mistralai/Mistral-7B-v0.1                   | 32000.0                | 1000000000000000019884624838656 | LlamaTokenizerFast      | kkCQxUk-PF9Ay_ZKDdKCh02YaGQ |
| mistral-7b-instruct                  | [mistralai/Mistral-7B-Instruct-v0.1](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1)                             | mistral-7b-instruct                                                                                | mistral      | Mistral-7B-Instruct-v0.1               | 7.8B              |        7784628224 | 7b                   |     7784628224 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | MistralForCausalLM          | RMS                      | mistralai/Mistral-7B-Instruct-v0.1          | 32000.0                | 1000000000000000019884624838656 | LlamaTokenizerFast      | kkCQxUk-PF9Ay_ZKDdKCh02YaGQ |
| mixtral                              | [mistralai/Mixtral-8x7B-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)                                           | mixtral, mixtral-8x7b                                                                              |              | Mixtral-8x7B-v0.1                      | 47B               |       47245688832 |                      |    47245688832 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | MixtralForCausalLM          | RMS                      | mistralai/Mixtral-8x7B-v0.1                 | 32000.0                | 1000000000000000019884624838656 | LlamaTokenizerFast      | kkCQxUk-PF9Ay_ZKDdKCh02YaGQ |
| mixtral-instruct                     | [mistralai/Mixtral-8x7B-Instruct-v0.1](https://huggingface.co/mistralai/Mixtral-8x7B-Instruct-v0.1)                         | mixtral-instruct, mixtral-8x7b-instruct                                                            |              | Mixtral-8x7B-Instruct-v0.1             | 47B               |       47245688832 |                      |    47245688832 |             32 |            32 |          4096 |         32000 | silu         | rotary                          | False                   | MixtralForCausalLM          | RMS                      | mistralai/Mixtral-8x7B-Instruct-v0.1        | 32000.0                | 1000000000000000019884624838656 | LlamaTokenizerFast      | kkCQxUk-PF9Ay_ZKDdKCh02YaGQ |
| bloom-560m                           | [bigscience/bloom-560m](https://huggingface.co/bigscience/bloom-560m)                                                       | bloom-560m                                                                                         | bloom        | bloom-560m                             | 302M              |         301989888 | 560m                 |      301989888 |             24 |            16 |          1024 |        250880 | gelu_fast    | alibi                           | False                   | BloomForCausalLM            | LN                       | bigscience/bloom-560m                       | 250680.0               | 1000000000000000019884624838656 | BloomTokenizerFast      | OO9NZoesMCpWsijo1O2DAbq9GqI |
| bloom-1b1                            | [bigscience/bloom-1b1](https://huggingface.co/bigscience/bloom-1b1)                                                         | bloom-1b1                                                                                          | bloom        | bloom-1b1                              | 679M              |         679477248 |                      |      679477248 |             24 |            16 |          1536 |        250880 | gelu_fast    | alibi                           | False                   | BloomForCausalLM            | LN                       | bigscience/bloom-1b1                        | 250680.0               | 1000000000000000019884624838656 | BloomTokenizerFast      | OO9NZoesMCpWsijo1O2DAbq9GqI |
| bloom-1b7                            | [bigscience/bloom-1b7](https://huggingface.co/bigscience/bloom-1b7)                                                         | bloom-1b7                                                                                          | bloom        | bloom-1b7                              | 1.2B              |        1207959552 |                      |     1207959552 |             24 |            16 |          2048 |        250880 | gelu_fast    | alibi                           | False                   | BloomForCausalLM            | LN                       | bigscience/bloom-1b7                        | 250680.0               | 1000000000000000019884624838656 | BloomTokenizerFast      | OO9NZoesMCpWsijo1O2DAbq9GqI |
| bloom-3b                             | [bigscience/bloom-3b](https://huggingface.co/bigscience/bloom-3b)                                                           | bloom-3b                                                                                           | bloom        | bloom-3b                               | 2.4B              |        2359296000 | 3b                   |     2359296000 |             30 |            32 |          2560 |        250880 | gelu_fast    | alibi                           | False                   | BloomForCausalLM            | LN                       | bigscience/bloom-3b                         | 250680.0               | 1000000000000000019884624838656 | BloomTokenizerFast      | OO9NZoesMCpWsijo1O2DAbq9GqI |
| bloom-7b1                            | [bigscience/bloom-7b1](https://huggingface.co/bigscience/bloom-7b1)                                                         | bloom-7b1                                                                                          | bloom        | bloom-7b1                              | 6.0B              |        6039797760 |                      |     6039797760 |             30 |            32 |          4096 |        250880 | gelu_fast    | alibi                           | False                   | BloomForCausalLM            | LN                       | bigscience/bloom-7b1                        | 250680.0               | 1000000000000000019884624838656 | BloomTokenizerFast      | OO9NZoesMCpWsijo1O2DAbq9GqI |
| santacoder                           | [bigcode/santacoder](https://huggingface.co/bigcode/santacoder)                                                             | santacoder                                                                                         |              | santacoder                             | 1.2B              |        1207959552 |                      |     1207959552 |             24 |            16 |          2048 |         49280 | gelu_fast    | standard                        | False                   | GPT2LMHeadCustomModel       | LN                       | bigcode/santacoder                          | 49152.0                | 2048                            | GPT2TokenizerFast       | GiKC-dU7fpR4sGNkpwn7JKK6qys |
| qwen-1.8b                            | [Qwen/Qwen-1_8B](https://huggingface.co/Qwen/Qwen-1_8B)                                                                     | qwen-1.8b                                                                                          | qwen         | Qwen-1_8B                              | 1.2B              |        1214251008 | 1.8b                 |     1214251008 |             24 |            16 |          2048 |        151936 | silu         | rotary                          | False                   | QWenLMHeadModel             | RMS                      |                                             |                        |                                 |                         |                             |
| qwen-7b                              | [Qwen/Qwen-7B](https://huggingface.co/Qwen/Qwen-7B)                                                                         | qwen-7b                                                                                            | qwen         | Qwen-7B                                | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |        151936 | silu         | rotary                          | False                   | QWenLMHeadModel             | RMS                      |                                             |                        |                                 |                         |                             |
| qwen-14b                             | [Qwen/Qwen-14B](https://huggingface.co/Qwen/Qwen-14B)                                                                       | qwen-14b                                                                                           | qwen         | Qwen-14B                               | 13B               |       12609126400 | 14b                  |    12609126400 |             40 |            40 |          5120 |        152064 | silu         | rotary                          | False                   | QWenLMHeadModel             | RMS                      |                                             |                        |                                 |                         |                             |
| qwen-1.8b-chat                       | [Qwen/Qwen-1_8B-Chat](https://huggingface.co/Qwen/Qwen-1_8B-Chat)                                                           | qwen-1.8b-chat                                                                                     | qwen         | Qwen-1_8B-Chat                         | 1.2B              |        1214251008 | 1.8b                 |     1214251008 |             24 |            16 |          2048 |        151936 | silu         | rotary                          | False                   | QWenLMHeadModel             | RMS                      |                                             |                        |                                 |                         |                             |
| qwen-7b-chat                         | [Qwen/Qwen-7B-Chat](https://huggingface.co/Qwen/Qwen-7B-Chat)                                                               | qwen-7b-chat                                                                                       | qwen         | Qwen-7B-Chat                           | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |        151936 | silu         | rotary                          | False                   | QWenLMHeadModel             | RMS                      |                                             |                        |                                 |                         |                             |
| qwen-14b-chat                        | [Qwen/Qwen-14B-Chat](https://huggingface.co/Qwen/Qwen-14B-Chat)                                                             | qwen-14b-chat                                                                                      | qwen         | Qwen-14B-Chat                          | 13B               |       12609126400 | 14b                  |    12609126400 |             40 |            40 |          5120 |        152064 | silu         | rotary                          | False                   | QWenLMHeadModel             | RMS                      |                                             |                        |                                 |                         |                             |
| qwen1.5-0.5b                         | [Qwen/Qwen1.5-0.5B](https://huggingface.co/Qwen/Qwen1.5-0.5B)                                                               | qwen1.5-0.5b                                                                                       | qwen         | Qwen1.5-0.5B                           | 308M              |         308281344 | 0.5b                 |      308281344 |             24 |            16 |          1024 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-0.5B                           | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-0.5b-chat                    | [Qwen/Qwen1.5-0.5B-Chat](https://huggingface.co/Qwen/Qwen1.5-0.5B-Chat)                                                     | qwen1.5-0.5b-chat                                                                                  | qwen         | Qwen1.5-0.5B-Chat                      | 308M              |         308281344 | 0.5b                 |      308281344 |             24 |            16 |          1024 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-0.5B-Chat                      | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-1.8b                         | [Qwen/Qwen1.5-1.8B](https://huggingface.co/Qwen/Qwen1.5-1.8B)                                                               | qwen1.5-1.8b                                                                                       | qwen         | Qwen1.5-1.8B                           | 1.2B              |        1214251008 | 1.8b                 |     1214251008 |             24 |            16 |          2048 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-1.8B                           | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-1.8b-chat                    | [Qwen/Qwen1.5-1.8B-Chat](https://huggingface.co/Qwen/Qwen1.5-1.8B-Chat)                                                     | qwen1.5-1.8b-chat                                                                                  | qwen         | Qwen1.5-1.8B-Chat                      | 1.2B              |        1214251008 | 1.8b                 |     1214251008 |             24 |            16 |          2048 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-1.8B-Chat                      | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-4b                           | [Qwen/Qwen1.5-4B](https://huggingface.co/Qwen/Qwen1.5-4B)                                                                   | qwen1.5-4b                                                                                         | qwen         | Qwen1.5-4B                             | 3.2B              |        3171942400 | 4b                   |     3171942400 |             40 |            20 |          2560 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-4B                             | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-4b-chat                      | [Qwen/Qwen1.5-4B-Chat](https://huggingface.co/Qwen/Qwen1.5-4B-Chat)                                                         | qwen1.5-4b-chat                                                                                    | qwen         | Qwen1.5-4B-Chat                        | 3.2B              |        3171942400 | 4b                   |     3171942400 |             40 |            20 |          2560 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-4B-Chat                        | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-7b                           | [Qwen/Qwen1.5-7B](https://huggingface.co/Qwen/Qwen1.5-7B)                                                                   | qwen1.5-7b                                                                                         | qwen         | Qwen1.5-7B                             | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-7B                             | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-7b-chat                      | [Qwen/Qwen1.5-7B-Chat](https://huggingface.co/Qwen/Qwen1.5-7B-Chat)                                                         | qwen1.5-7b-chat                                                                                    | qwen         | Qwen1.5-7B-Chat                        | 6.5B              |        6476005376 | 7b                   |     6476005376 |             32 |            32 |          4096 |        151936 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-7B-Chat                        | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-14b                          | [Qwen/Qwen1.5-14B](https://huggingface.co/Qwen/Qwen1.5-14B)                                                                 | qwen1.5-14b                                                                                        | qwen         | Qwen1.5-14B                            | 13B               |       12609126400 | 14b                  |    12609126400 |             40 |            40 |          5120 |        152064 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-14B                            | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| qwen1.5-14b-chat                     | [Qwen/Qwen1.5-14B-Chat](https://huggingface.co/Qwen/Qwen1.5-14B-Chat)                                                       | qwen1.5-14b-chat                                                                                   | qwen         | Qwen1.5-14B-Chat                       | 13B               |       12609126400 | 14b                  |    12609126400 |             40 |            40 |          5120 |        152064 | silu         | rotary                          | False                   | Qwen2ForCausalLM            | RMS                      | Qwen/Qwen1.5-14B-Chat                       | 151643.0               | 32768                           | Qwen2TokenizerFast      | vakQOjPaHpZ23kxcqX0tTXi2EzQ |
| phi-1                                | [microsoft/phi-1](https://huggingface.co/microsoft/phi-1)                                                                   | phi-1                                                                                              | phi          | phi-1                                  | 1.2B              |        1207959552 |                      |     1207959552 |             24 |            32 |          2048 |         51200 | gelu_new     | rotary                          | True                    | PhiForCausalLM              | LN                       |                                             |                        |                                 |                         |                             |
| phi-1_5                              | [microsoft/phi-1_5](https://huggingface.co/microsoft/phi-1_5)                                                               | phi-1_5                                                                                            | phi          | phi-1_5                                | 1.2B              |        1207959552 |                      |     1207959552 |             24 |            32 |          2048 |         51200 | gelu_new     | rotary                          | True                    | PhiForCausalLM              | LN                       |                                             |                        |                                 |                         |                             |
| phi-2                                | [microsoft/phi-2](https://huggingface.co/microsoft/phi-2)                                                                   | phi-2                                                                                              | phi          | phi-2                                  | 2.5B              |        2516582400 |                      |     2516582400 |             32 |            32 |          2560 |         51200 | gelu_new     | rotary                          | True                    | PhiForCausalLM              | LN                       |                                             |                        |                                 |                         |                             |
| phi-3                                | [microsoft/Phi-3-mini-4k-instruct](https://huggingface.co/microsoft/Phi-3-mini-4k-instruct)                                 | phi-3                                                                                              | phi          | Phi-3-mini-4k-instruct                 | 3.6B              |        3623878656 |                      |     3623878656 |             32 |            32 |          3072 |         32064 | silu         | rotary                          | False                   | Phi3ForCausalLM             | RMS                      |                                             |                        |                                 |                         |                             |
| gemma-2b                             | [google/gemma-2b](https://huggingface.co/google/gemma-2b)                                                                   | gemma-2b                                                                                           | gemma        | gemma-2b                               | 2.1B              |        2113929216 | 2b                   |     2113929216 |             18 |             8 |          2048 |        256000 | gelu_new     | rotary                          | False                   | GemmaForCausalLM            | RMS                      | google/gemma-2b                             | 256000.0               | 1000000000000000019884624838656 | GemmaTokenizerFast      | 9GdVxqTUEa0Qt2kBBEHf3ebUisw |
| gemma-7b                             | [google/gemma-7b](https://huggingface.co/google/gemma-7b)                                                                   | gemma-7b                                                                                           | gemma        | gemma-7b                               | 7.8B              |        7751073792 | 7b                   |     7751073792 |             28 |            16 |          3072 |        256000 | gelu_new     | rotary                          | False                   | GemmaForCausalLM            | RMS                      | google/gemma-7b                             | 256000.0               | 1000000000000000019884624838656 | GemmaTokenizerFast      | 9GdVxqTUEa0Qt2kBBEHf3ebUisw |
| gemma-2b-it                          | [google/gemma-2b-it](https://huggingface.co/google/gemma-2b-it)                                                             | gemma-2b-it                                                                                        | gemma        | gemma-2b-it                            | 2.1B              |        2113929216 | 2b                   |     2113929216 |             18 |             8 |          2048 |        256000 | gelu_new     | rotary                          | False                   | GemmaForCausalLM            | RMS                      | google/gemma-2b-it                          | 256000.0               | 1000000000000000019884624838656 | GemmaTokenizerFast      | 9GdVxqTUEa0Qt2kBBEHf3ebUisw |
| gemma-7b-it                          | [google/gemma-7b-it](https://huggingface.co/google/gemma-7b-it)                                                             | gemma-7b-it                                                                                        | gemma        | gemma-7b-it                            | 7.8B              |        7751073792 | 7b                   |     7751073792 |             28 |            16 |          3072 |        256000 | gelu_new     | rotary                          | False                   | GemmaForCausalLM            | RMS                      | google/gemma-7b-it                          | 256000.0               | 1000000000000000019884624838656 | GemmaTokenizerFast      | 9GdVxqTUEa0Qt2kBBEHf3ebUisw |
| yi-6b                                | [01-ai/Yi-6B](https://huggingface.co/01-ai/Yi-6B)                                                                           | yi-6b, Yi-6B                                                                                       | yi           | Yi-6B                                  | 6.5B              |        6476005376 | 6b                   |     6476005376 |             32 |            32 |          4096 |         64000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      | 01-ai/Yi-6B                                 | 63992.0                | 4096                            | LlamaTokenizerFast      | VGXAFrTzytwGdUlX6AWH0NacncM |
| yi-34b                               | [01-ai/Yi-34B](https://huggingface.co/01-ai/Yi-34B)                                                                         | yi-34b, Yi-34B                                                                                     | yi           | Yi-34B                                 | 39B               |       38755368960 | 34b                  |    38755368960 |             60 |            56 |          7168 |         64000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      | 01-ai/Yi-34B                                | 64000.0                | 4096                            | LlamaTokenizerFast      | VBBPi7l7j0Xrv93YNq1tizlalWw |
| yi-6b-chat                           | [01-ai/Yi-6B-Chat](https://huggingface.co/01-ai/Yi-6B-Chat)                                                                 | yi-6b-chat, Yi-6B-Chat                                                                             | yi           | Yi-6B-Chat                             | 6.5B              |        6476005376 | 6b                   |     6476005376 |             32 |            32 |          4096 |         64000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      | 01-ai/Yi-6B-Chat                            | 63992.0                | 4096                            | LlamaTokenizerFast      | VGXAFrTzytwGdUlX6AWH0NacncM |
| yi-34b-chat                          | [01-ai/Yi-34B-Chat](https://huggingface.co/01-ai/Yi-34B-Chat)                                                               | yi-34b-chat, Yi-34B-Chat                                                                           | yi           | Yi-34B-Chat                            | 39B               |       38755368960 | 34b                  |    38755368960 |             60 |            56 |          7168 |         64000 | silu         | rotary                          | False                   | LlamaForCausalLM            | RMS                      | 01-ai/Yi-34B-Chat                           | 63992.0                | 4096                            | LlamaTokenizerFast      | VGXAFrTzytwGdUlX6AWH0NacncM |
| t5-small                             | [google-t5/t5-small](https://huggingface.co/google-t5/t5-small)                                                             | t5-small                                                                                           | t5           | t5-small                               | 19M               |          18874368 |                      |       18874368 |              6 |             8 |           512 |         32128 | relu         | relative_positional_bias        | False                   | T5ForConditionalGeneration  | LN                       | google-t5/t5-small                          | 32100.0                | 512                             | T5TokenizerFast         | jQeywCyCMVL_vza2wKfpuwjNVys |
| t5-base                              | [google-t5/t5-base](https://huggingface.co/google-t5/t5-base)                                                               | t5-base                                                                                            | t5           | t5-base                                | 85M               |          84934656 |                      |       84934656 |             12 |            12 |           768 |         32128 | relu         | relative_positional_bias        | False                   | T5ForConditionalGeneration  | LN                       | google-t5/t5-base                           | 32100.0                | 1000000000000000019884624838656 | T5TokenizerFast         | jQeywCyCMVL_vza2wKfpuwjNVys |
| t5-large                             | [google-t5/t5-large](https://huggingface.co/google-t5/t5-large)                                                             | t5-large                                                                                           | t5           | t5-large                               | 302M              |         301989888 |                      |      301989888 |             24 |            16 |          1024 |         32128 | relu         | relative_positional_bias        | False                   | T5ForConditionalGeneration  | LN                       | google-t5/t5-large                          | 32100.0                | 1000000000000000019884624838656 | T5TokenizerFast         | jQeywCyCMVL_vza2wKfpuwjNVys |
| mGPT                                 |                                                                                                                             |                                                                                                    |              | mGPT                                   | 1.2B              |        1207959552 |                      |     1207959552 |             24 |            16 |          2048 |        100000 | gelu_new     | standard                        | False                   | GPT2LMHeadModel             | LN                       | ai-forever/mGPT                             | 100000.0               | 2048                            | GPT2TokenizerFast       | 8j6CU_p3zgyeEBZ1Z3lu358tiy0 |