{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformer_lens\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(names) = 136,\tlen(names_from_dict) = 132,\tlen(aliases) = 136,\tlen(aliases_from_dict) = 309\n",
      "{'bert-base-cased', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
      "{'gpt2-medium-small-e', 'meta-llama/Llama-2-13b-chat-hf', 'gpt2-stanford-medium-e', 'bert-base-cased', 'EleutherAI/pythia-2.7b-deduped', 'pythia-125m-seed3', 'gpt-neo-medium', 'pythia-6.7b-v0', 'attn-only-4l-c4-code', 'pythia-350m', 'pythia-350m-deduped-v0', 'solu-2l-c4-code', 'gpt2-mistral-small-e', 'EleutherAI/pythia-125m-deduped', 'meta-llama/Llama-2-7b-hf', 'codellama/CodeLlama-7b-hf', 'EleutherAI/pythia-6.7b', 'distil-gpt2', 'gptj', 'gpt2-medium-small-a', 'gpt2-medium-small-d', 'pythia-2.7b-v0', 'opt-large', 'pythia-800m-deduped-v0', 'meta-llama/Llama-2-13b-hf', 'neo-small', 'solu-6l-new', 'attn-only-2l-induction-demo', 'gpt2-stanford-small-a', 'pythia-19m-v0', 'pythia-125m-v0', 'EleutherAI/pythia-350m-deduped-v0', 'gpt2-stanford-medium-a', 'solu-4l-finetune', 'pythia-125m', 'attn-only-2l-c4-code', 'gelu-3l-c4-code', 'EleutherAI/pythia-350m-deduped', 'solu-6l-c4-code', 'attn-only-3l-c4-code', 'EleutherAI/pythia-125m', 'stablelm-base-3b', 'EleutherAI/pythia-1.3b-v0', 'neox', 'opt-xxxxl', 'EleutherAI/pythia-800m-deduped', 'EleutherAI/pythia-1.3b-deduped', 'stablelm-tuned-7b', 'EleutherAI/pythia-125m-seed1', 'attn-only-1l-c4-code', 'pythia-13b-deduped', 'gpt2-large', 'gpt2-mistral-small-b', 'gelu-2l-new', 'attn-only-2l-shortformer-6b-big-lr', 'alias-gpt2-small-x21', 'expanse-gpt2-small-x777', 'EleutherAI/pythia-19m-deduped', 'attn-only-1l-new', 'attn-only-2l-new', 'gpt2-xs', 'solu-2l-old', 'gpt2-medium-small-b', 'gelu-4l-c4-code', 'gpt2-mistral-small-d', 'solu-12l-old', 'opt-medium', 'solu-1l-old', 'pythia-13b-deduped-v0', 'neo', 'pythia-1.3b', 'gpt2-xl', 'EleutherAI/pythia-2.7b-v0', 'opt', 'pythia-2.7b', 'pythia-19m', 'distill-gpt2', 'gpt-neox', 'solu-10l-new', 'pythia-6.7b-deduped-v0', 'pythia-2.7b-deduped', 'gpt2-stanford-medium-b', 'beren-gpt2-medium-x49', 'pythia-13b-v0', 'solu-4l-c4-code', 'pythia-2.7b-deduped-v0', 'EleutherAI/pythia-19m', 'pythia-125m-deduped', 'pythia-350m-v0', 'codellama/CodeLlama-7b-Instruct-hf', 'solu-4l-old', 'gpt-neo-small', 'solu-1l-finetune', 'pythia-800m-deduped', 'EleutherAI/pythia-13b-deduped-v0', 'opt-xxxl', 'EleutherAI/pythia-125m-seed2', 'gelu-4l-new', 'pythia-19m-deduped', 'pythia-125m-seed1', 'pythia-350m-deduped', 'darkmatter-gpt2-small-x343', 'solu-8l-c4-code', 'gpt2-medium', 'opt-small', 'neo-large', 'solu-12l-new', 'stablelm-tuned-3b', 'celebrimbor-gpt2-medium-x81', 'solu-4l-wiki-finetune', 'pythia-1.3b-deduped', 'solu-8l-new', 'solu-2l-new', 'gpt2-stanford-small-c', 'opt-xl', 'gpt-neo-large', 'EleutherAI/pythia-13b-deduped', 'EleutherAI/pythia-13b-v0', 'pythia-13b', 'gelu-1l-new', 'EleutherAI/pythia-19m-deduped-v0', 'EleutherAI/pythia-6.7b-deduped-v0', 'caprica-gpt2-small-x81', 'attn-only-3l-new', 'gpt2-mistral-small-a', 'gelu-3l-new', 'EleutherAI/pythia-6.7b-deduped', 'EleutherAI/pythia-19m-v0', 'EleutherAI/pythia-2.7b', 'gpt2-stanford-medium-d', 'attn-only-demo', 'EleutherAI/pythia-125m-deduped-v0', 'battlestar-gpt2-small-x49', 'pythia-6.7b-deduped', 'EleutherAI/pythia-1.3b', 'solu-10l-old', 'pythia-125m-seed2', 'neo-medium', 'EleutherAI/pythia-800m', 'pythia-800m-v0', 'pythia-125m-deduped-v0', 'EleutherAI/pythia-800m-deduped-v0', 'EleutherAI/pythia-350m', 'opt-xxl', 'meta-llama/Llama-2-7b-chat-hf', 'EleutherAI/pythia-800m-v0', 'EleutherAI/pythia-2.7b-deduped-v0', 'pythia-v0', 'codellama/CodeLlama-7b-Python-hf', 'gelu-2l-c4-code', 'attn-only-4l-new', 'pythia-800m', 'pythia', 'solu-10l-c4-code', 'gelu-1l-c4-code', 'pythia-19m-deduped-v0', 'EleutherAI/pythia-350m-v0', 'arwen-gpt2-medium-x21', 'solu-3l-new', 'solu-8l-old', 'solu-6l-old', 'EleutherAI/pythia-125m-seed3', 'solu-4l-new', 'gpt-j', 'EleutherAI/pythia-125m-v0', 'stablelm-base-7b', 'EleutherAI/pythia-13b', 'pythia-1.3b-v0', 'solu-1l-c4-code', 'EleutherAI/pythia-1.3b-deduped-v0', 'gpt2-mistral-small-c', 'solu-3l-c4-code', 'durin-gpt2-medium-x343', 'pythia-6.7b', 'EleutherAI/pythia-6.7b-v0', 'solu-1l-new', 'solu-12l-c4-code', 'solu-1l-wiki-finetune', 'pythia-1.3b-deduped-v0', 'gpt2-medium-small-c', 'eowyn-gpt2-medium-x777'}\n"
     ]
    }
   ],
   "source": [
    "names = set(transformer_lens.loading.OFFICIAL_MODEL_NAMES)\n",
    "names_from_dict = set(transformer_lens.loading.MODEL_ALIASES.keys())\n",
    "aliases = set(transformer_lens.loading.DEFAULT_MODEL_ALIASES)\n",
    "aliases_from_dict = set(list(chain(*transformer_lens.loading.MODEL_ALIASES.values())))\n",
    "print(f\"{len(names) = },\\t{len(names_from_dict) = },\\t{len(aliases) = },\\t{len(aliases_from_dict) = }\")\n",
    "print(names.symmetric_difference(names_from_dict))\n",
    "print(aliases.symmetric_difference(aliases_from_dict))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['AutoConfig',\n",
       " 'AutoModelForCausalLM',\n",
       " 'BertForPreTraining',\n",
       " 'Config',\n",
       " 'DEFAULT_MODEL_ALIASES',\n",
       " 'Dict',\n",
       " 'HfApi',\n",
       " 'HookedTransformerConfig',\n",
       " 'MODEL_ALIASES',\n",
       " 'NEED_REMOTE_CODE_MODELS',\n",
       " 'NON_HF_HOSTED_MODEL_NAMES',\n",
       " 'OFFICIAL_MODEL_NAMES',\n",
       " 'Optional',\n",
       " 'PYTHIA_CHECKPOINTS',\n",
       " 'PYTHIA_V0_CHECKPOINTS',\n",
       " 'STANFORD_CRFM_CHECKPOINTS',\n",
       " '__builtins__',\n",
       " '__cached__',\n",
       " '__doc__',\n",
       " '__file__',\n",
       " '__loader__',\n",
       " '__name__',\n",
       " '__package__',\n",
       " '__spec__',\n",
       " 'convert_bert_weights',\n",
       " 'convert_bloom_weights',\n",
       " 'convert_coder_weights',\n",
       " 'convert_gpt2_weights',\n",
       " 'convert_gptj_weights',\n",
       " 'convert_hf_model_config',\n",
       " 'convert_llama_weights',\n",
       " 'convert_mingpt_weights',\n",
       " 'convert_mistral_weights',\n",
       " 'convert_nanogpt_weights',\n",
       " 'convert_neel_model_config',\n",
       " 'convert_neel_solu_old_weights',\n",
       " 'convert_neo_weights',\n",
       " 'convert_neox_weights',\n",
       " 'convert_opt_weights',\n",
       " 'convert_qwen_weights',\n",
       " 'dataclasses',\n",
       " 'einops',\n",
       " 'fill_missing_keys',\n",
       " 'get_basic_config',\n",
       " 'get_checkpoint_labels',\n",
       " 'get_num_params_of_pretrained',\n",
       " 'get_official_model_name',\n",
       " 'get_pretrained_model_config',\n",
       " 'get_pretrained_state_dict',\n",
       " 'logging',\n",
       " 'make_model_alias_map',\n",
       " 're',\n",
       " 'torch',\n",
       " 'utils']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(transformer_lens.loading)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from get_model_table import get_model_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading model info:  94%|█████████▍| 128/136 [00:12<00:00, 12.86it/s]WARNING:root:Loading model bigcode/santacoder requires setting trust_remote_code=True\n",
      "Loading model info:  96%|█████████▌| 130/136 [00:13<00:00,  9.41it/s]WARNING:root:Loading model Qwen/Qwen-1_8B requires setting trust_remote_code=True\n",
      "WARNING:root:Loading model Qwen/Qwen-7B requires setting trust_remote_code=True\n",
      "Loading model info:  97%|█████████▋| 132/136 [00:13<00:00,  6.66it/s]WARNING:root:Loading model Qwen/Qwen-14B requires setting trust_remote_code=True\n",
      "Loading model info:  98%|█████████▊| 133/136 [00:14<00:00,  5.78it/s]WARNING:root:Loading model Qwen/Qwen-1_8B-Chat requires setting trust_remote_code=True\n",
      "Loading model info:  99%|█████████▊| 134/136 [00:14<00:00,  5.03it/s]WARNING:root:Loading model Qwen/Qwen-7B-Chat requires setting trust_remote_code=True\n",
      "Loading model info:  99%|█████████▉| 135/136 [00:14<00:00,  4.58it/s]WARNING:root:Loading model Qwen/Qwen-14B-Chat requires setting trust_remote_code=True\n",
      "Loading model info: 100%|██████████| 136/136 [00:15<00:00,  9.04it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>default_alias</th>\n",
       "      <th>official_name</th>\n",
       "      <th>model_size_info</th>\n",
       "      <th>model_type</th>\n",
       "      <th>cfg_model_name</th>\n",
       "      <th>n_params_str</th>\n",
       "      <th>n_params</th>\n",
       "      <th>n_layers</th>\n",
       "      <th>n_heads</th>\n",
       "      <th>d_model</th>\n",
       "      <th>d_vocab</th>\n",
       "      <th>act_fn</th>\n",
       "      <th>positional_embedding_type</th>\n",
       "      <th>parallel_attn_mlp</th>\n",
       "      <th>original_architecture</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>gpt2-small</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>85M</td>\n",
       "      <td>84934656</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-medium</td>\n",
       "      <td>302M</td>\n",
       "      <td>301989888</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>1024</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-large</td>\n",
       "      <td>708M</td>\n",
       "      <td>707788800</td>\n",
       "      <td>36</td>\n",
       "      <td>20</td>\n",
       "      <td>1280</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>gpt2-xl</td>\n",
       "      <td>1.5B</td>\n",
       "      <td>1474560000</td>\n",
       "      <td>48</td>\n",
       "      <td>25</td>\n",
       "      <td>1600</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>distillgpt2</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>None</td>\n",
       "      <td>gpt2</td>\n",
       "      <td>distilgpt2</td>\n",
       "      <td>42M</td>\n",
       "      <td>42467328</td>\n",
       "      <td>6</td>\n",
       "      <td>12</td>\n",
       "      <td>768</td>\n",
       "      <td>50257</td>\n",
       "      <td>gelu_new</td>\n",
       "      <td>standard</td>\n",
       "      <td>False</td>\n",
       "      <td>GPT2LMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>qwen-7b</td>\n",
       "      <td>Qwen/Qwen-7B</td>\n",
       "      <td>7b</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-7B</td>\n",
       "      <td>5.0B</td>\n",
       "      <td>5033164800</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4096</td>\n",
       "      <td>151936</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>qwen-14b</td>\n",
       "      <td>Qwen/Qwen-14B</td>\n",
       "      <td>14b</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-14B</td>\n",
       "      <td>9.8B</td>\n",
       "      <td>9804185600</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>5120</td>\n",
       "      <td>152064</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>qwen-1.8b-chat</td>\n",
       "      <td>Qwen/Qwen-1_8B-Chat</td>\n",
       "      <td>1.8b</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-1_8B-Chat</td>\n",
       "      <td>944M</td>\n",
       "      <td>943718400</td>\n",
       "      <td>24</td>\n",
       "      <td>16</td>\n",
       "      <td>2048</td>\n",
       "      <td>151936</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>qwen-7b-chat</td>\n",
       "      <td>Qwen/Qwen-7B-Chat</td>\n",
       "      <td>7b</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-7B-Chat</td>\n",
       "      <td>5.0B</td>\n",
       "      <td>5033164800</td>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "      <td>4096</td>\n",
       "      <td>151936</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>qwen-14b-chat</td>\n",
       "      <td>Qwen/Qwen-14B-Chat</td>\n",
       "      <td>14b</td>\n",
       "      <td>None</td>\n",
       "      <td>Qwen-14B-Chat</td>\n",
       "      <td>9.8B</td>\n",
       "      <td>9804185600</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>5120</td>\n",
       "      <td>152064</td>\n",
       "      <td>silu</td>\n",
       "      <td>rotary</td>\n",
       "      <td>False</td>\n",
       "      <td>QWenLMHeadModel</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>136 rows × 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      default_alias        official_name model_size_info model_type  \\\n",
       "0        gpt2-small                 gpt2            None       gpt2   \n",
       "1       gpt2-medium          gpt2-medium            None       gpt2   \n",
       "2        gpt2-large           gpt2-large            None       gpt2   \n",
       "3           gpt2-xl              gpt2-xl            None       gpt2   \n",
       "4       distillgpt2           distilgpt2            None       gpt2   \n",
       "..              ...                  ...             ...        ...   \n",
       "131         qwen-7b         Qwen/Qwen-7B              7b       None   \n",
       "132        qwen-14b        Qwen/Qwen-14B             14b       None   \n",
       "133  qwen-1.8b-chat  Qwen/Qwen-1_8B-Chat            1.8b       None   \n",
       "134    qwen-7b-chat    Qwen/Qwen-7B-Chat              7b       None   \n",
       "135   qwen-14b-chat   Qwen/Qwen-14B-Chat             14b       None   \n",
       "\n",
       "     cfg_model_name n_params_str    n_params  n_layers  n_heads  d_model  \\\n",
       "0              gpt2          85M    84934656        12       12      768   \n",
       "1       gpt2-medium         302M   301989888        24       16     1024   \n",
       "2        gpt2-large         708M   707788800        36       20     1280   \n",
       "3           gpt2-xl         1.5B  1474560000        48       25     1600   \n",
       "4        distilgpt2          42M    42467328         6       12      768   \n",
       "..              ...          ...         ...       ...      ...      ...   \n",
       "131         Qwen-7B         5.0B  5033164800        32       32     4096   \n",
       "132        Qwen-14B         9.8B  9804185600        40       40     5120   \n",
       "133  Qwen-1_8B-Chat         944M   943718400        24       16     2048   \n",
       "134    Qwen-7B-Chat         5.0B  5033164800        32       32     4096   \n",
       "135   Qwen-14B-Chat         9.8B  9804185600        40       40     5120   \n",
       "\n",
       "     d_vocab    act_fn positional_embedding_type  parallel_attn_mlp  \\\n",
       "0      50257  gelu_new                  standard              False   \n",
       "1      50257  gelu_new                  standard              False   \n",
       "2      50257  gelu_new                  standard              False   \n",
       "3      50257  gelu_new                  standard              False   \n",
       "4      50257  gelu_new                  standard              False   \n",
       "..       ...       ...                       ...                ...   \n",
       "131   151936      silu                    rotary              False   \n",
       "132   152064      silu                    rotary              False   \n",
       "133   151936      silu                    rotary              False   \n",
       "134   151936      silu                    rotary              False   \n",
       "135   152064      silu                    rotary              False   \n",
       "\n",
       "    original_architecture  \n",
       "0         GPT2LMHeadModel  \n",
       "1         GPT2LMHeadModel  \n",
       "2         GPT2LMHeadModel  \n",
       "3         GPT2LMHeadModel  \n",
       "4         GPT2LMHeadModel  \n",
       "..                    ...  \n",
       "131       QWenLMHeadModel  \n",
       "132       QWenLMHeadModel  \n",
       "133       QWenLMHeadModel  \n",
       "134       QWenLMHeadModel  \n",
       "135       QWenLMHeadModel  \n",
       "\n",
       "[136 rows x 15 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(get_model_table(force_reload=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = transformer_lens.loading.get_pretrained_model_config(\"gpt2-small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cfg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
